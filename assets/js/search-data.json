{
  
    
        "post0": {
            "title": "A Simple Autoencoder",
            "content": "import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # convert data to torch.FloatTensor transform = transforms.ToTensor() # load the training and test datasets train_data = datasets.MNIST(root=&#39;data&#39;, train=True, download=True, transform=transform) test_data = datasets.MNIST(root=&#39;data&#39;, train=False, download=True, transform=transform) . # Create training and test dataloaders # number of subprocesses to use for data loading num_workers = 0 # how many samples per batch to load batch_size = 20 # prepare data loaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) . Visualize the Data . import matplotlib.pyplot as plt %matplotlib inline # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # get one image from the batch img = np.squeeze(images[0]) fig = plt.figure(figsize = (5,5)) ax = fig.add_subplot(111) ax.imshow(img, cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x11bedad30&gt; . . Linear Autoencoder . We&#39;ll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let&#39;s start by building a simple autoencoder. The encoder and decoder should be made of one linear layer. The units that connect the encoder and decoder will be the compressed representation. . Since the images are normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range. . . TODO: Build the graph for the autoencoder in the cell below. . The input images will be flattened into 784 length vectors. The targets are the same as the inputs. The encoder and decoder will be made of two linear layers, each. The depth dimensions should change as follows:784 inputs &gt; encoding_dim &gt; 784 outputs.&gt; All layers will have ReLu activations applied except for the final output layer, which has a sigmoid activation. . The compressed representation should be a vector with dimension encoding_dim=32. . import torch.nn as nn import torch.nn.functional as F # define the NN architecture class Autoencoder(nn.Module): def __init__(self, encoding_dim): super(Autoencoder, self).__init__() ## encoder ## # linear layer (784 -&gt; encoding_dim) self.fc1 = nn.Linear(28 * 28, encoding_dim) ## decoder ## # linear layer (encoding_dim -&gt; input size) self.fc2 = nn.Linear(encoding_dim, 28*28) def forward(self, x): # add layer, with relu activation function x = F.relu(self.fc1(x)) # output layer (sigmoid for scaling from 0 to 1) x = F.sigmoid(self.fc2(x)) return x # initialize the NN encoding_dim = 32 model = Autoencoder(encoding_dim) print(model) . Autoencoder( (fc1): Linear(in_features=784, out_features=32, bias=True) (fc2): Linear(in_features=32, out_features=784, bias=True) ) . . Training . Here I&#39;ll write a bit of code to train the network. I&#39;m not too interested in validation here, so I&#39;ll just monitor the training loss and the test loss afterwards. . We are not concerned with labels in this case, just images, which we can get from the train_loader. Because we&#39;re comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I&#39;ll use MSELoss. And compare output images and input images as follows: . loss = criterion(outputs, images) . Otherwise, this is pretty straightfoward training with PyTorch. We flatten our images, pass them into the autoencoder, and record the training loss as we go. . # specify loss function criterion = nn.MSELoss() # specify loss function optimizer = torch.optim.Adam(model.parameters(), lr=0.001) . # number of epochs to train the model n_epochs = 20 for epoch in range(1, n_epochs+1): # monitor training loss train_loss = 0.0 ################### # train the model # ################### for data in train_loader: # _ stands in for labels, here images, _ = data # flatten images images = images.view(images.size(0), -1) # clear the gradients of all optimized variables optimizer.zero_grad() # forward pass: compute predicted outputs by passing inputs to the model outputs = model(images) # calculate the loss loss = criterion(outputs, images) # backward pass: compute gradient of the loss with respect to model parameters loss.backward() # perform a single optimization step (parameter update) optimizer.step() # update running training loss train_loss += loss.item()*images.size(0) # print avg training statistics train_loss = train_loss/len(train_loader) print(&#39;Epoch: {} tTraining Loss: {:.6f}&#39;.format( epoch, train_loss )) . Epoch: 1 Training Loss: 0.636266 Epoch: 2 Training Loss: 0.300271 Epoch: 3 Training Loss: 0.258592 Epoch: 4 Training Loss: 0.249542 Epoch: 5 Training Loss: 0.245499 Epoch: 6 Training Loss: 0.243327 Epoch: 7 Training Loss: 0.241832 Epoch: 8 Training Loss: 0.240754 Epoch: 9 Training Loss: 0.239949 Epoch: 10 Training Loss: 0.239308 Epoch: 11 Training Loss: 0.238760 Epoch: 12 Training Loss: 0.238279 Epoch: 13 Training Loss: 0.237813 Epoch: 14 Training Loss: 0.237417 Epoch: 15 Training Loss: 0.237069 Epoch: 16 Training Loss: 0.236747 Epoch: 17 Training Loss: 0.236448 Epoch: 18 Training Loss: 0.236169 Epoch: 19 Training Loss: 0.235894 Epoch: 20 Training Loss: 0.235611 . Checking out the results . Below I&#39;ve plotted some of the test images along with their reconstructions. For the most part these look pretty good except for some blurriness in some parts. . # obtain one batch of test images dataiter = iter(test_loader) images, labels = dataiter.next() images_flatten = images.view(images.size(0), -1) # get sample outputs output = model(images_flatten) # prep images for display images = images.numpy() # output is resized into a batch of images output = output.view(batch_size, 1, 28, 28) # use detach when it&#39;s an output that requires_grad output = output.detach().numpy() # plot the first ten input images and then reconstructed images fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4)) # input images on top row, reconstructions on bottom for images, row in zip([images, output], axes): for img, ax in zip(images, row): ax.imshow(np.squeeze(img), cmap=&#39;gray&#39;) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) . Up Next . We&#39;re dealing with images here, so we can (usually) get better performance using convolution layers. So, next we&#39;ll build a better autoencoder with convolutional layers. .",
            "url": "https://manisaiprasad.github.io/notes/2020/06/11/Simple_Linear_Autoencoder.html",
            "relUrl": "/2020/06/11/Simple_Linear_Autoencoder.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating a Sentiment Analysis Web App",
            "content": "Step 1: Downloading the data . As in the XGBoost in SageMaker notebook, we will be using the IMDb dataset . Maas, Andrew L., et al. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011. . %mkdir ../data !wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz !tar -zxf ../data/aclImdb_v1.tar.gz -C ../data . mkdir: cannot create directory ‘../data’: File exists --2020-06-04 12:40:34-- http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10 Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 84125825 (80M) [application/x-gzip] Saving to: ‘../data/aclImdb_v1.tar.gz’ ../data/aclImdb_v1. 100%[===================&gt;] 80.23M 22.6MB/s in 4.8s 2020-06-04 12:40:39 (16.8 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825] . Step 2: Preparing and Processing the data . Also, as in the XGBoost notebook, we will be doing some initial data processing. The first few steps are the same as in the XGBoost example. To begin with, we will read in each of the reviews and combine them into a single input structure. Then, we will split the dataset into a training set and a testing set. . import os import glob def read_imdb_data(data_dir=&#39;../data/aclImdb&#39;): data = {} labels = {} for data_type in [&#39;train&#39;, &#39;test&#39;]: data[data_type] = {} labels[data_type] = {} for sentiment in [&#39;pos&#39;, &#39;neg&#39;]: data[data_type][sentiment] = [] labels[data_type][sentiment] = [] path = os.path.join(data_dir, data_type, sentiment, &#39;*.txt&#39;) files = glob.glob(path) for f in files: with open(f) as review: data[data_type][sentiment].append(review.read()) # Here we represent a positive review by &#39;1&#39; and a negative review by &#39;0&#39; labels[data_type][sentiment].append(1 if sentiment == &#39;pos&#39; else 0) assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), &quot;{}/{} data size does not match labels size&quot;.format(data_type, sentiment) return data, labels . data, labels = read_imdb_data() print(&quot;IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg&quot;.format( len(data[&#39;train&#39;][&#39;pos&#39;]), len(data[&#39;train&#39;][&#39;neg&#39;]), len(data[&#39;test&#39;][&#39;pos&#39;]), len(data[&#39;test&#39;][&#39;neg&#39;]))) . IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg . Now that we&#39;ve read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records. . from sklearn.utils import shuffle def prepare_imdb_data(data, labels): &quot;&quot;&quot;Prepare training and test sets from IMDb movie reviews.&quot;&quot;&quot; #Combine positive and negative reviews and labels data_train = data[&#39;train&#39;][&#39;pos&#39;] + data[&#39;train&#39;][&#39;neg&#39;] data_test = data[&#39;test&#39;][&#39;pos&#39;] + data[&#39;test&#39;][&#39;neg&#39;] labels_train = labels[&#39;train&#39;][&#39;pos&#39;] + labels[&#39;train&#39;][&#39;neg&#39;] labels_test = labels[&#39;test&#39;][&#39;pos&#39;] + labels[&#39;test&#39;][&#39;neg&#39;] #Shuffle reviews and corresponding labels within training and test sets data_train, labels_train = shuffle(data_train, labels_train) data_test, labels_test = shuffle(data_test, labels_test) # Return a unified training data, test data, training labels, test labets return data_train, data_test, labels_train, labels_test . train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels) print(&quot;IMDb reviews (combined): train = {}, test = {}&quot;.format(len(train_X), len(test_X))) . IMDb reviews (combined): train = 25000, test = 25000 . Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly. . print(train_X[100]) print(train_y[100]) . Who would think Andy Griffith&#39;s &#34;Helen Crump&#34; (Aneta Corsaut) had a Steve McQueen movie in her past? But that is only one of several weird and wonderful things about the ultimate 1950s teenagers-battle-creatures movie, which might best be described as Rebel Without A Cause meets God Knows What From Outer Space. The Rebel is Steven McQueen (who would shortly decide that &#34;Steve&#34; sounded less prissy), a good boy with just enough wild to be interesting; the very wholesome yet understanding girlfriend is the aforementioned Aneta Corsaut. It was bad enough when their date was disrupted by teenage hot-rodders, but they are considerably more nonplussed when they encounter a gelatinous, man-eating What Is It that rides down to earth on its own hotrod meteor--and begins gobbling up townfolk right and left. But will the grown ups believe them? Of course not, what do they know, they&#39;re just kids!&lt;br /&gt;&lt;br /&gt;The movie is teeny bopper at its teeny bopping best. The actors take the rather pretentious script very seriously, with many a soulful look into each other eyes, and the &#34;adult&#34; supporting cast probably says &#34;Kids!&#34; very third sentence or so. But the real pleasure of the film its creature, which is well imagined, well-executed, and often manages to generate a surprising degree of suspense. And although clearly on the cheap side (check out those miniature sets, guys!), THE BLOB is actually a fairly well-made film--and there&#39;s that catchy little theme song thrown in for good measure. The 40-plus crowd (myself included) will enjoy the movie as nostalgia, but that won&#39;t prevent them from hooting right along with the younger set at its whole-milk-and-white-bread 1950s sensibility, and the film would be a great choice for either family-movie night or a more sophisticated &#34;grown ups only&#34; get together. Make plenty of Jello cubes for movie snacking! Gary F. Taylor, aka GFT, Amazon Reviewer 1 . The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as entertained and entertaining are considered the same with regard to sentiment analysis. . import nltk from nltk.corpus import stopwords from nltk.stem.porter import * import re from bs4 import BeautifulSoup def review_to_words(review): nltk.download(&quot;stopwords&quot;, quiet=True) stemmer = PorterStemmer() text = BeautifulSoup(review, &quot;html.parser&quot;).get_text() # Remove HTML tags text = re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, text.lower()) # Convert to lower case words = text.split() # Split string into words words = [w for w in words if w not in stopwords.words(&quot;english&quot;)] # Remove stopwords words = [PorterStemmer().stem(w) for w in words] # stem return words . The review_to_words method defined above uses BeautifulSoup to remove any html tags that appear and uses the nltk package to tokenize the reviews. As a check to ensure we know how everything is working, try applying review_to_words to one of the reviews in the training set. . # TODO: Apply review_to_words to a review (train_X[100] or any other review) review_to_words(train_X[150]) . [&#39;actress&#39;, &#39;make&#39;, &#39;movi&#39;, &#39;africa&#39;, &#39;kidnap&#39;, &#39;taken&#39;, &#39;jungl&#39;, &#39;held&#39;, &#39;ransom&#39;, &#39;produc&#39;, &#39;hire&#39;, &#39;one&#39;, &#39;go&#39;, &#39;bring&#39;, &#39;back&#39;, &#39;complic&#39;, &#39;everyth&#39;, &#39;cannib&#39;, &#39;jungl&#39;, &#39;worship&#39;, &#39;realli&#39;, &#39;ugli&#39;, &#39;look&#39;, &#39;god&#39;, &#39;like&#39;, &#39;eat&#39;, &#39;nake&#39;, &#39;women&#39;, &#39;gori&#39;, &#39;sleazi&#39;, &#39;movi&#39;, &#39;copiou&#39;, &#39;amount&#39;, &#39;nuditi&#39;, &#39;violenc&#39;, &#39;mention&#39;, &#39;violenc&#39;, &#39;nude&#39;, &#39;peopl&#39;, &#39;exploit&#39;, &#39;film&#39;, &#39;design&#39;, &#39;appeal&#39;, &#39;deepest&#39;, &#39;darkest&#39;, &#39;part&#39;, &#39;movi&#39;, &#39;bore&#39;, &#39;film&#39;, &#39;would&#39;, &#39;classic&#39;, &#39;let&#39;, &#39;face&#39;, &#39;despit&#39;, &#39;gore&#39;, &#39;nasti&#39;, &#39;sex&#39;, &#39;abus&#39;, &#39;ugli&#39;, &#39;monster&#39;, &#39;movi&#39;, &#39;snoozer&#39;, &#39;pace&#39;, &#39;kilter&#39;, &#39;put&#39;, &#39;multipl&#39;, &#39;plot&#39;, &#39;line&#39;, &#39;seem&#39;, &#39;happen&#39;, &#39;separ&#39;, &#39;even&#39;, &#39;though&#39;, &#39;ultim&#39;, &#39;one&#39;, &#39;stori&#39;, &#39;worst&#39;, &#39;almost&#39;, &#39;one&#39;, &#39;say&#39;, &#39;anyth&#39;, &#39;minim&#39;, &#39;dialog&#39;, &#39;concern&#39;, &#39;cruelti&#39;, &#39;one&#39;, &#39;charact&#39;, &#39;protest&#39;, &#39;want&#39;, &#39;quiet&#39;, &#39;dull&#39;, &#39;movi&#39;, &#39;frequent&#39;, &#39;scream&#39;, &#39;victim&#39;, &#39;recommend&#39;, &#39;sleep&#39;, &#39;aid&#39;, &#39;movi&#39;, &#39;avoid&#39;, &#39;unless&#39;, &#39;need&#39;, &#39;sleep&#39;, &#39;unless&#39;, &#39;need&#39;, &#39;see&#39;, &#39;everi&#39;, &#39;euro&#39;, &#39;cannib&#39;, &#39;movi&#39;, &#39;asid&#39;, &#39;videoasia&#39;, &#39;releas&#39;, &#39;part&#39;, &#39;terror&#39;, &#39;tale&#39;, &#39;seri&#39;, &#39;print&#39;, &#39;oddli&#39;, &#39;letter&#39;, &#39;box&#39;, &#39;look&#39;, &#39;result&#39;, &#39;take&#39;, &#39;print&#39;, &#39;japanes&#39;, &#39;sourc&#39;, &#39;fog&#39;, &#39;crop&#39;, &#39;remov&#39;, &#39;subtitl&#39;, &#39;print&#39;, &#39;also&#39;, &#39;open&#39;, &#39;titl&#39;] . Question: Above we mentioned that review_to_words method removes html formatting and allows us to tokenize the words found in a review, for example, converting entertained and entertaining into entertain so that they are treated as though they are the same word. What else, if anything, does this method do to the input? . Answer: Ths method &#39;review_to_words&#39; removes punctuation marks and also some last letters of some of the words. It also removes stop words. . The method below applies the review_to_words method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time. . import pickle cache_dir = os.path.join(&quot;../cache&quot;, &quot;sentiment_analysis&quot;) # where to store cache files os.makedirs(cache_dir, exist_ok=True) # ensure cache directory exists def preprocess_data(data_train, data_test, labels_train, labels_test, cache_dir=cache_dir, cache_file=&quot;preprocessed_data.pkl&quot;): &quot;&quot;&quot;Convert each review to words; read from cache if available.&quot;&quot;&quot; # If cache_file is not None, try to read from it first cache_data = None if cache_file is not None: try: with open(os.path.join(cache_dir, cache_file), &quot;rb&quot;) as f: cache_data = pickle.load(f) print(&quot;Read preprocessed data from cache file:&quot;, cache_file) except: pass # unable to read from cache, but that&#39;s okay # If cache is missing, then do the heavy lifting if cache_data is None: # Preprocess training and test data to obtain words for each review #words_train = list(map(review_to_words, data_train)) #words_test = list(map(review_to_words, data_test)) words_train = [review_to_words(review) for review in data_train] words_test = [review_to_words(review) for review in data_test] # Write to cache file for future runs if cache_file is not None: cache_data = dict(words_train=words_train, words_test=words_test, labels_train=labels_train, labels_test=labels_test) with open(os.path.join(cache_dir, cache_file), &quot;wb&quot;) as f: pickle.dump(cache_data, f) print(&quot;Wrote preprocessed data to cache file:&quot;, cache_file) else: # Unpack data loaded from cache file words_train, words_test, labels_train, labels_test = (cache_data[&#39;words_train&#39;], cache_data[&#39;words_test&#39;], cache_data[&#39;labels_train&#39;], cache_data[&#39;labels_test&#39;]) return words_train, words_test, labels_train, labels_test . # Preprocess data train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y) . Read preprocessed data from cache file: preprocessed_data.pkl . Transform the data . In the XGBoost notebook we transformed the data from its word representation to a bag-of-words feature representation. For the model we are going to construct in this notebook we will construct a feature representation which is very similar. To start, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don&#39;t contain much information for the purposes of sentiment analysis. The way we will deal with this problem is that we will fix the size of our working vocabulary and we will only include the words that appear most frequently. We will then combine all of the infrequent words into a single category and, in our case, we will label it as 1. . Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category &#39;no word&#39; (which we will label 0) and truncate long reviews. . (TODO) Create a word dictionary . To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the &#39;no word&#39; and &#39;infrequent&#39; categories) to be 5000 but you may wish to change this to see how it affects the model. . TODO: Complete the implementation for the build_dict() method below. Note that even though the vocab_size is set to 5000, we only want to construct a mapping for the most frequently appearing 4998 words. This is because we want to reserve the special labels 0 for &#39;no word&#39; and 1 for &#39;infrequent word&#39;. . import numpy as np def build_dict(data, vocab_size = 5000): &quot;&quot;&quot;Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.&quot;&quot;&quot; # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a # sentence is a list of words. word_count = {} # A dict storing the words that appear in the reviews along with how often they occur for review in data: for word in review: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and # sorted_words[-1] is the least frequently appearing word. sorted_words = [item[0] for item in sorted(word_count.items(), key=lambda x: x[1], reverse=True)] word_dict = {} # This is what we are building, a dictionary that translates words into integers for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the &#39;no word&#39; word_dict[word] = idx + 2 # &#39;infrequent&#39; labels return word_dict . word_dict = build_dict(train_X) . Question: What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set? . Answer: movi, film, one, like and time are the words commonly used in movie review, however the word one being in the list is quite interesting . # TODO: Use this space to determine the five most frequently appearing words in the training set. keys = list(word_dict.keys()) keys[:5] . [&#39;movi&#39;, &#39;film&#39;, &#39;one&#39;, &#39;like&#39;, &#39;time&#39;] . Save word_dict . Later on when we construct an endpoint which processes a submitted review we will need to make use of the word_dict which we have created. As such, we will save it to a file now for future use. . data_dir = &#39;../data/pytorch&#39; # The folder we will use for storing data if not os.path.exists(data_dir): # Make sure that the folder exists os.makedirs(data_dir) . with open(os.path.join(data_dir, &#39;word_dict.pkl&#39;), &quot;wb&quot;) as f: pickle.dump(word_dict, f) . Transform the reviews . Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is 500. . def convert_and_pad(word_dict, sentence, pad=500): NOWORD = 0 # We will use 0 to represent the &#39;no word&#39; category INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict working_sentence = [NOWORD] * pad for word_index, word in enumerate(sentence[:pad]): if word in word_dict: working_sentence[word_index] = word_dict[word] else: working_sentence[word_index] = INFREQ return working_sentence, min(len(sentence), pad) def convert_and_pad_data(word_dict, data, pad=500): result = [] lengths = [] for sentence in data: converted, leng = convert_and_pad(word_dict, sentence, pad) result.append(converted) lengths.append(leng) return np.array(result), np.array(lengths) . train_X, train_X_len = convert_and_pad_data(word_dict, train_X) test_X, test_X_len = convert_and_pad_data(word_dict, test_X) . As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set? . # Use this cell to examine one of the processed reviews to make sure everything is working as intended. train_X[150] . array([ 28, 1, 238, 4148, 2727, 1, 3487, 1, 1, 494, 255, 185, 1508, 1, 1, 4, 1, 3856, 42, 136, 141, 22, 3, 1, 2214, 1, 4413, 740, 613, 4512, 68, 4413, 646, 95, 1, 365, 11, 1, 1, 397, 1508, 1, 340, 1508, 4413, 116, 259, 115, 59, 4, 416, 381, 371, 65, 670, 1152, 4413, 485, 815, 1, 745, 228, 16, 24, 40, 1314, 2512, 1984, 30, 1841, 2524, 671, 3, 798, 531, 353, 710, 1984, 83, 3272, 834, 1061, 1018, 229, 83, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . Question: In the cells above we use the preprocess_data and convert_and_pad_data methods to process both the training and testing set. Why or why not might this be a problem? . Answer: It gives up the information of words not appreaing in word_dict. All these input vectors are now the same size. . Step 3: Upload the data to S3 . As in the XGBoost notebook, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on. . Save the processed training dataset locally . It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form label, length, review[500] where review[500] is a sequence of 500 integers representing the words in the review. . import pandas as pd pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) .to_csv(os.path.join(data_dir, &#39;train.csv&#39;), header=False, index=False) . Uploading the training data . Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model. . import sagemaker sagemaker_session = sagemaker.Session() bucket = sagemaker_session.default_bucket() prefix = &#39;sagemaker/sentiment_rnn&#39; role = sagemaker.get_execution_role() . input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix) . NOTE: The cell above uploads the entire contents of our data directory. This includes the word_dict.pkl file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory. . Step 4: Build and Train the PyTorch Model . In the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects . Model Artifacts, | Training Code, and | Inference Code, | . each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code. . We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the model.py file, inside of the train folder. You can see the provided implementation by running the cell below. . !pygmentize train/model.py . import torch.nn as nn class LSTMClassifier(nn.Module): &#34;&#34;&#34; This is the simple RNN model we will be using to perform Sentiment Analysis. &#34;&#34;&#34; def __init__(self, embedding_dim, hidden_dim, vocab_size): &#34;&#34;&#34; Initialize the model by settingg up the various layers. &#34;&#34;&#34; super(LSTMClassifier, self).__init__() self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) self.lstm = nn.LSTM(embedding_dim, hidden_dim) self.dense = nn.Linear(in_features=hidden_dim, out_features=1) self.sig = nn.Sigmoid() self.word_dict = None def forward(self, x): &#34;&#34;&#34; Perform a forward pass of our model on some input. &#34;&#34;&#34; x = x.t() lengths = x[0,:] reviews = x[1:,:] embeds = self.embedding(reviews) lstm_out, _ = self.lstm(embeds) out = self.dense(lstm_out) out = out[lengths - 1, range(len(lengths))] return self.sig(out.squeeze()) . The important takeaway from the implementation provided is that there are three parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. We will likely want to make these parameters configurable in the training script so that if we wish to modify them we do not need to modify the script itself. We will see how to do this later on. To start we will write some of the training code in the notebook so that we can more easily diagnose any issues that arise. . First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving. . import torch import torch.utils.data # Read in only the first 250 rows train_sample = pd.read_csv(os.path.join(data_dir, &#39;train.csv&#39;), header=None, names=None, nrows=250) # Turn the input pandas dataframe into tensors train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze() train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long() # Build the dataset train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y) # Build the dataloader train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50) . (TODO) Writing the training method . Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later. . def train(model, train_loader, epochs, optimizer, loss_fn, device): for epoch in range(1, epochs + 1): model.train() total_loss = 0 for batch in train_loader: batch_X, batch_y = batch batch_X = batch_X.to(device) batch_y = batch_y.to(device) # TODO: Complete this train method to train the model provided. optimizer.zero_grad() out = model.forward(batch_X) loss = loss_fn(out, batch_y) loss.backward() optimizer.step() total_loss += loss.data.item() print(&quot;Epoch: {}, BCELoss: {}&quot;.format(epoch, total_loss / len(train_loader))) . Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose. . import torch.optim as optim from train.model import LSTMClassifier device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = LSTMClassifier(32, 100, 5000).to(device) optimizer = optim.Adam(model.parameters()) loss_fn = torch.nn.BCELoss() train(model, train_sample_dl, 5, optimizer, loss_fn, device) . Epoch: 1, BCELoss: 0.6924273371696472 Epoch: 2, BCELoss: 0.6801935195922851 Epoch: 3, BCELoss: 0.6707218647003174 Epoch: 4, BCELoss: 0.6613067507743835 Epoch: 5, BCELoss: 0.6509132981300354 . In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a requirements.txt file and install any required Python libraries, after which the training script will be run. . (TODO) Training the model . When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the train directory is a file called train.py which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the train() method which you wrote earlier in this notebook. . TODO: Copy the train() method written above and paste it into the train/train.py file where required. . The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided train/train.py file. . from sagemaker.pytorch import PyTorch estimator = PyTorch(entry_point=&quot;train.py&quot;, source_dir=&quot;train&quot;, role=role, framework_version=&#39;0.4.0&#39;, train_instance_count=1, train_instance_type=&#39;ml.m4.xlarge&#39;, hyperparameters={ &#39;epochs&#39;: 10, &#39;hidden_dim&#39;: 200, }) . estimator.fit({&#39;training&#39;: input_data}) . 2020-06-04 11:22:24 Starting - Starting the training job... 2020-06-04 11:22:26 Starting - Launching requested ML instances...... 2020-06-04 11:23:32 Starting - Preparing the instances for training...... 2020-06-04 11:24:52 Downloading - Downloading input data... 2020-06-04 11:24:59 Training - Downloading the training image bash: cannot set terminal process group (-1): Inappropriate ioctl for device bash: no job control in this shell 2020-06-04 11:25:14,061 sagemaker-containers INFO Imported framework sagemaker_pytorch_container.training 2020-06-04 11:25:14,063 sagemaker-containers INFO No GPUs detected (normal if no gpus installed) 2020-06-04 11:25:14,076 sagemaker_pytorch_container.training INFO Block until all host DNS lookups succeed. 2020-06-04 11:25:14,296 sagemaker_pytorch_container.training INFO Invoking user training script. 2020-06-04 11:25:14,509 sagemaker-containers INFO Module train does not provide a setup.py. Generating setup.py 2020-06-04 11:25:14,509 sagemaker-containers INFO Generating setup.cfg 2020-06-04 11:25:14,509 sagemaker-containers INFO Generating MANIFEST.in 2020-06-04 11:25:14,509 sagemaker-containers INFO Installing module with the following comman: /usr/bin/python -m pip install -U . -r requirements.txt Processing /opt/ml/code Collecting pandas (from -r requirements.txt (line 1)) Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB) Collecting numpy (from -r requirements.txt (line 2)) Downloading https://files.pythonhosted.org/packages/bb/ef/d5a21cbc094d3f4d5b5336494dbcc9550b70c766a8345513c7c24ed18418/numpy-1.16.4-cp35-cp35m-manylinux1_x86_64.whl (17.2MB) Collecting nltk (from -r requirements.txt (line 3)) Downloading https://files.pythonhosted.org/packages/8d/5d/825889810b85c303c8559a3fd74d451d80cf3585a851f2103e69576bf583/nltk-3.4.3.zip (1.4MB) Collecting beautifulsoup4 (from -r requirements.txt (line 4)) Downloading https://files.pythonhosted.org/packages/1d/5d/3260694a59df0ec52f8b4883f5d23b130bc237602a1411fa670eae12351e/beautifulsoup4-4.7.1-py3-none-any.whl (94kB) Collecting html5lib (from -r requirements.txt (line 5)) Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas-&gt;-r requirements.txt (line 1)) (2.7.5) Collecting pytz&gt;=2011k (from pandas-&gt;-r requirements.txt (line 1)) Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk-&gt;-r requirements.txt (line 3)) (1.11.0) Collecting soupsieve&gt;=1.2 (from beautifulsoup4-&gt;-r requirements.txt (line 4)) Downloading https://files.pythonhosted.org/packages/b9/a5/7ea40d0f8676bde6e464a6435a48bc5db09b1a8f4f06d41dd997b8f3c616/soupsieve-1.9.1-py2.py3-none-any.whl Collecting webencodings (from html5lib-&gt;-r requirements.txt (line 5)) Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl Building wheels for collected packages: nltk, train Running setup.py bdist_wheel for nltk: started Running setup.py bdist_wheel for nltk: finished with status &#39;done&#39; Stored in directory: /root/.cache/pip/wheels/54/40/b7/c56ad418e6cd4d9e1e594b5e138d1ca6eec11a6ee3d464e5bb Running setup.py bdist_wheel for train: started Running setup.py bdist_wheel for train: finished with status &#39;done&#39; Stored in directory: /tmp/pip-ephem-wheel-cache-8oifb2qr/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3 Successfully built nltk train Installing collected packages: pytz, numpy, pandas, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train Found existing installation: numpy 1.15.4 Uninstalling numpy-1.15.4: Successfully uninstalled numpy-1.15.4 Successfully installed beautifulsoup4-4.7.1 html5lib-1.0.1 nltk-3.4.3 numpy-1.16.4 pandas-0.24.2 pytz-2019.1 soupsieve-1.9.1 train-1.0.0 webencodings-0.5.1 You are using pip version 18.1, however version 19.1.1 is available. You should consider upgrading via the &#39;pip install --upgrade pip&#39; command. 2020-06-04 11:25:26,074 sagemaker-containers INFO No GPUs detected (normal if no gpus installed) 2020-06-04 11:25:26,088 sagemaker-containers INFO Invoking user script Training Env: { &#34;network_interface_name&#34;: &#34;eth0&#34;, &#34;input_config_dir&#34;: &#34;/opt/ml/input/config&#34;, &#34;num_cpus&#34;: 4, &#34;output_intermediate_dir&#34;: &#34;/opt/ml/output/intermediate&#34;, &#34;output_data_dir&#34;: &#34;/opt/ml/output/data&#34;, &#34;output_dir&#34;: &#34;/opt/ml/output&#34;, &#34;job_name&#34;: &#34;sagemaker-pytorch-2020-06-04-11-22-24-190&#34;, &#34;channel_input_dirs&#34;: { &#34;training&#34;: &#34;/opt/ml/input/data/training&#34; }, &#34;module_name&#34;: &#34;train&#34;, &#34;module_dir&#34;: &#34;s3://sagemaker-us-east-2-660633772842/sagemaker-pytorch-2020-06-04-11-22-24-190/source/sourcedir.tar.gz&#34;, &#34;user_entry_point&#34;: &#34;train.py&#34;, &#34;log_level&#34;: 20, &#34;hyperparameters&#34;: { &#34;hidden_dim&#34;: 200, &#34;epochs&#34;: 10 }, &#34;input_dir&#34;: &#34;/opt/ml/input&#34;, &#34;model_dir&#34;: &#34;/opt/ml/model&#34;, &#34;framework_module&#34;: &#34;sagemaker_pytorch_container.training:main&#34;, &#34;hosts&#34;: [ &#34;algo-1&#34; ], &#34;current_host&#34;: &#34;algo-1&#34;, &#34;additional_framework_parameters&#34;: {}, &#34;resource_config&#34;: { &#34;hosts&#34;: [ &#34;algo-1&#34; ], &#34;current_host&#34;: &#34;algo-1&#34;, &#34;network_interface_name&#34;: &#34;eth0&#34; }, &#34;input_data_config&#34;: { &#34;training&#34;: { &#34;TrainingInputMode&#34;: &#34;File&#34;, &#34;S3DistributionType&#34;: &#34;FullyReplicated&#34;, &#34;RecordWrapperType&#34;: &#34;None&#34; } }, &#34;num_gpus&#34;: 0 } Environment variables: SM_CURRENT_HOST=algo-1 SM_INPUT_DATA_CONFIG={&#34;training&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;}} SM_USER_ENTRY_POINT=train.py SM_OUTPUT_DATA_DIR=/opt/ml/output/data SM_HPS={&#34;epochs&#34;:10,&#34;hidden_dim&#34;:200} SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate SM_HP_EPOCHS=10 SM_HP_HIDDEN_DIM=200 SM_CHANNELS=[&#34;training&#34;] SM_MODEL_DIR=/opt/ml/model SM_NUM_GPUS=0 SM_OUTPUT_DIR=/opt/ml/output SM_INPUT_DIR=/opt/ml/input SM_RESOURCE_CONFIG={&#34;current_host&#34;:&#34;algo-1&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;network_interface_name&#34;:&#34;eth0&#34;} SM_LOG_LEVEL=20 SM_TRAINING_ENV={&#34;additional_framework_parameters&#34;:{},&#34;channel_input_dirs&#34;:{&#34;training&#34;:&#34;/opt/ml/input/data/training&#34;},&#34;current_host&#34;:&#34;algo-1&#34;,&#34;framework_module&#34;:&#34;sagemaker_pytorch_container.training:main&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;hyperparameters&#34;:{&#34;epochs&#34;:10,&#34;hidden_dim&#34;:200},&#34;input_config_dir&#34;:&#34;/opt/ml/input/config&#34;,&#34;input_data_config&#34;:{&#34;training&#34;:{&#34;RecordWrapperType&#34;:&#34;None&#34;,&#34;S3DistributionType&#34;:&#34;FullyReplicated&#34;,&#34;TrainingInputMode&#34;:&#34;File&#34;}},&#34;input_dir&#34;:&#34;/opt/ml/input&#34;,&#34;job_name&#34;:&#34;sagemaker-pytorch-2020-06-04-11-22-24-190&#34;,&#34;log_level&#34;:20,&#34;model_dir&#34;:&#34;/opt/ml/model&#34;,&#34;module_dir&#34;:&#34;s3://sagemaker-us-east-2-660633772842/sagemaker-pytorch-2020-06-04-11-22-24-190/source/sourcedir.tar.gz&#34;,&#34;module_name&#34;:&#34;train&#34;,&#34;network_interface_name&#34;:&#34;eth0&#34;,&#34;num_cpus&#34;:4,&#34;num_gpus&#34;:0,&#34;output_data_dir&#34;:&#34;/opt/ml/output/data&#34;,&#34;output_dir&#34;:&#34;/opt/ml/output&#34;,&#34;output_intermediate_dir&#34;:&#34;/opt/ml/output/intermediate&#34;,&#34;resource_config&#34;:{&#34;current_host&#34;:&#34;algo-1&#34;,&#34;hosts&#34;:[&#34;algo-1&#34;],&#34;network_interface_name&#34;:&#34;eth0&#34;},&#34;user_entry_point&#34;:&#34;train.py&#34;} SM_FRAMEWORK_PARAMS={} SM_INPUT_CONFIG_DIR=/opt/ml/input/config SM_HOSTS=[&#34;algo-1&#34;] SM_MODULE_DIR=s3://sagemaker-us-east-2-660633772842/sagemaker-pytorch-2020-06-04-11-22-24-190/source/sourcedir.tar.gz SM_MODULE_NAME=train SM_USER_ARGS=[&#34;--epochs&#34;,&#34;10&#34;,&#34;--hidden_dim&#34;,&#34;200&#34;] PYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main SM_NETWORK_INTERFACE_NAME=eth0 SM_NUM_CPUS=4 SM_CHANNEL_TRAINING=/opt/ml/input/data/training Invoking script with the following command: /usr/bin/python -m train --epochs 10 --hidden_dim 200 Using device cpu. Get train data loader. Model loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000. 2020-06-04:26:02 Training - Training image download completed. Training in progress.Epoch: 1, BCELoss: 0.6771698350809059 Epoch: 2, BCELoss: 0.6191755812995288 Epoch: 3, BCELoss: 0.5250254960692659 Epoch: 4, BCELoss: 0.4307900624615805 Epoch: 5, BCELoss: 0.3844200512584375 Epoch: 6, BCELoss: 0.3376043636579903 Epoch: 7, BCELoss: 0.3183594282184328 Epoch: 8, BCELoss: 0.3473552097471393 Epoch: 9, BCELoss: 0.39758459128895585 2020-06-04:55 Uploading - Uploading generated training model 2020-06-04 13:05:55 Completed - Training job completed Epoch: 10, BCELoss: 0.2802868272577013 2020-06-04 13:05:48,848 sagemaker-containers INFO Reporting training SUCCESS Billable seconds: 6063 . Step 5: Testing the model . As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly. . Step 6: Deploy the model for testing . Now that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form review_length, review[500] where review[500] is a sequence of 500 integers which describe the words present in the review, encoded using word_dict. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this. . There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called model_fn() and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made. . NOTE: When the built-in inference code is run it must import the model_fn() method from the train.py file. This is why the training code is wrapped in a main guard ( ie, if __name__ == &#39;__main__&#39;: ) . Since we don&#39;t need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is. . NOTE: When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until you shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for. . In other words If you are no longer using a deployed endpoint, shut it down! . TODO: Deploy the trained model. . # TODO: Deploy the trained model predictor = estimator.deploy(initial_instance_count=1, instance_type=&#39;ml.m4.xlarge&#39;) . ! . Step 7 - Use the model for testing . Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is. . test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1) . # We split the data into chunks and send each chunk seperately, accumulating the results. def predict(data, rows=512): split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1)) predictions = np.array([]) for array in split_array: predictions = np.append(predictions, predictor.predict(array)) return predictions . predictions = predict(test_X.values) predictions = [round(num) for num in predictions] . from sklearn.metrics import accuracy_score accuracy_score(test_y, predictions) . 0.8554 . Question: How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do you think is better for sentiment analysis? . Answer: The difference between the XGBoost model is infinitesimal. The xgb was slightly better but by a small margin. . (TODO) More testing . We now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model. . test_review = &#39;The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.&#39; . The question we now need to answer is, how do we send this review to our model? . Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews. . Removed any html tags and stemmed the input | Encoded the review as a sequence of integers using word_dict | . In order process the review we will need to repeat these two steps. . TODO: Using the review_to_words and convert_and_pad methods from section one, convert test_review into a numpy array test_data suitable to send to our model. Remember that our model expects input of the form review_length, review[500]. . # TODO: Convert test_review into a form usable by the model and save the results in test_data test_data_review_to_words = review_to_words(test_review) test_data = [np.array(convert_and_pad(word_dict, test_data_review_to_words)[0])] . Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review. . predictor.predict(test_data) . array(0.5820169, dtype=float32) . Since the return value of our model is close to 1, we can be certain that the review we submitted is positive. . Delete the endpoint . Of course, just like in the XGBoost notebook, once we&#39;ve deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it. . estimator.delete_endpoint() . Step 6 (again) - Deploy the model for the web app . Now that we know that our model is working, it&#39;s time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review. . As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code. . We will store the code that we write in the serve directory. Provided in this directory is the model.py file that we used to construct our model, a utils.py file which contains the review_to_words and convert_and_pad pre-processing functions which we used during the initial data processing, and predict.py, the file which will contain our custom inference code. Note also that requirements.txt is present which will tell SageMaker what Python libraries are required by our custom inference code. . When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use. . model_fn: This function is the same function that we used in the training script and it tells SageMaker how to load our model. | input_fn: This function receives the raw serialized input that has been sent to the model&#39;s endpoint and its job is to de-serialize and make the input available for the inference code. | output_fn: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model&#39;s endpoint. | predict_fn: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete. | . For the simple website that we are constructing during this project, the input_fn and output_fn methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize. . (TODO) Writing inference code . Before writing our custom inference code, we will begin by taking a look at the code which has been provided. . !pygmentize serve/predict.py . import argparse import json import os import pickle import sys import sagemaker_containers import pandas as pd import numpy as np import torch import torch.nn as nn import torch.optim as optim import torch.utils.data from model import LSTMClassifier from utils import review_to_words, convert_and_pad def model_fn(model_dir): &#34;&#34;&#34;Load the PyTorch model from the `model_dir` directory.&#34;&#34;&#34; print(&#34;Loading model.&#34;) # First, load the parameters used to create the model. model_info = {} model_info_path = os.path.join(model_dir, &#39;model_info.pth&#39;) with open(model_info_path, &#39;rb&#39;) as f: model_info = torch.load(f) print(&#34;model_info: {}&#34;.format(model_info)) # Determine the device and construct the model. device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) model = LSTMClassifier(model_info[&#39;embedding_dim&#39;], model_info[&#39;hidden_dim&#39;], model_info[&#39;vocab_size&#39;]) # Load the store model parameters. model_path = os.path.join(model_dir, &#39;model.pth&#39;) with open(model_path, &#39;rb&#39;) as f: model.load_state_dict(torch.load(f)) # Load the saved word_dict. word_dict_path = os.path.join(model_dir, &#39;word_dict.pkl&#39;) with open(word_dict_path, &#39;rb&#39;) as f: model.word_dict = pickle.load(f) model.to(device).eval() print(&#34;Done loading model.&#34;) return model def input_fn(serialized_input_data, content_type): print(&#39;Deserializing the input data.&#39;) if content_type == &#39;text/plain&#39;: data = serialized_input_data.decode(&#39;utf-8&#39;) return data raise Exception(&#39;Requested unsupported ContentType in content_type: &#39; + content_type) def output_fn(prediction_output, accept): print(&#39;Serializing the generated output.&#39;) return str(prediction_output) def predict_fn(input_data, model): print(&#39;Inferring sentiment of input data.&#39;) device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) if model.word_dict is None: raise Exception(&#39;Model has not been loaded properly, no word_dict.&#39;) # TODO: Process input_data so that it is ready to be sent to our model. # You should produce two variables: # data_X - A sequence of length 500 which represents the converted review # data_len - The length of the review words = review_to_words(input_data) data_X, data_len = convert_and_pad(model.word_dict, words) # Using data_X and data_len we construct an appropriate input tensor. Remember # that our model expects input data of the form &#39;len, review[500]&#39;. data_pack = np.hstack((data_len, data_X)) data_pack = data_pack.reshape(1, -1) data = torch.from_numpy(data_pack) data = data.to(device) # Make sure to put the model into evaluation mode model.eval() # TODO: Compute the result of applying the model to the input data. The variable `result` should # be a numpy array which contains a single integer which is either 1 or 0 with torch.no_grad(): output = model.forward(data) result = np.round(output.numpy()) return result . As mentioned earlier, the model_fn method is the same as the one provided in the training code and the input_fn and output_fn methods are very simple and your task will be to complete the predict_fn method. Make sure that you save the completed file as predict.py in the serve directory. . TODO: Complete the predict_fn() method in the serve/predict.py file. . Deploying the model . Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container. . NOTE: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a numpy array. In our case we want to send a string so we need to construct a simple wrapper around the RealTimePredictor class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data. . from sagemaker.predictor import RealTimePredictor from sagemaker.pytorch import PyTorchModel class StringPredictor(RealTimePredictor): def __init__(self, endpoint_name, sagemaker_session): super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type=&#39;text/plain&#39;) model = PyTorchModel(model_data=estimator.model_data, role = role, framework_version=&#39;0.4.0&#39;, entry_point=&#39;predict.py&#39;, source_dir=&#39;serve&#39;, predictor_cls=StringPredictor) predictor = model.deploy(initial_instance_count=1, instance_type=&#39;ml.m4.xlarge&#39;) . ! . Testing the model . Now that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first 250 positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive. . import glob def test_reviews(data_dir=&#39;../data/aclImdb&#39;, stop=250): results = [] ground = [] # We make sure to test both positive and negative reviews for sentiment in [&#39;pos&#39;, &#39;neg&#39;]: path = os.path.join(data_dir, &#39;test&#39;, sentiment, &#39;*.txt&#39;) files = glob.glob(path) files_read = 0 print(&#39;Starting &#39;, sentiment, &#39; files&#39;) # Iterate through the files and send them to the predictor for f in files: with open(f) as review: # First, we store the ground truth (was the review positive or negative) if sentiment == &#39;pos&#39;: ground.append(1) else: ground.append(0) # Read in the review and convert to &#39;utf-8&#39; for transmission via HTTP review_input = review.read().encode(&#39;utf-8&#39;) # Send the review to the predictor and store the results results.append(int(predictor.predict(review_input))) # Sending reviews to our endpoint one at a time takes a while so we # only send a small number of reviews files_read += 1 if files_read == stop: break return ground, results . ground, results = test_reviews() . Starting pos files Starting neg files . from sklearn.metrics import accuracy_score accuracy_score(ground, results) . 0.866 . As an additional test, we can try sending the test_review that we looked at earlier. . predictor.predict(test_review) . b&#39;1&#39; . Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don&#39;t have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back. . Step 7 (again): Use the model for the web app . TODO: This entire section and the next contain tasks for you to complete, mostly using the AWS console. So far we have been accessing our model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services. . . The diagram above gives an overview of how the various services will work together. On the far right is the model which we trained above and which is deployed using SageMaker. On the far left is our web app that collects a user&#39;s movie review, sends it off and expects a positive or negative sentiment in return. . In the middle is where some of the magic happens. We will construct a Lambda function, which you can think of as a straightforward Python function that can be executed whenever a specified event occurs. We will give this function permission to send and recieve data from a SageMaker endpoint. . Lastly, the method we will use to execute the Lambda function is a new endpoint that we will create using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets our web app communicate with the Lambda function. . Setting up a Lambda function . The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we&#39;ve created and then return the result. . Part A: Create an IAM Role for the Lambda function . Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function. . Using the AWS Console, navigate to the IAM page and click on Roles. Then, click on Create role. Make sure that the AWS service is the type of trusted entity selected and choose Lambda as the service that will use this role, then click Next: Permissions. . In the search box type sagemaker and select the check box next to the AmazonSageMakerFullAccess policy. Then, click on Next: Review. . Lastly, give this role a name. Make sure you use a name that you will remember later on, for example LambdaSageMakerRole. Then, click on Create role. . Part B: Create a Lambda function . Now it is time to actually create the Lambda function. . Using the AWS Console, navigate to the AWS Lambda page and click on Create a function. When you get to the next page, make sure that Author from scratch is selected. Now, name your Lambda function, using a name that you will remember later on, for example sentiment_analysis_func. Make sure that the Python 3.6 runtime is selected and then choose the role that you created in the previous part. Then, click on Create Function. . On the next page you will see some information about the Lambda function you&#39;ve just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below. . # We need to use the low-level library to interact with SageMaker since the SageMaker API # is not available natively through Lambda. import boto3 def lambda_handler(event, context): # The SageMaker runtime is what allows us to invoke the endpoint that we&#39;ve created. runtime = boto3.Session().client(&#39;sagemaker-runtime&#39;) # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given response = runtime.invoke_endpoint(EndpointName = &#39;**ENDPOINT NAME HERE**&#39;, # The name of the endpoint we created ContentType = &#39;text/plain&#39;, # The data format that is expected Body = event[&#39;body&#39;]) # The actual review # The response is an HTTP response whose body contains the result of our inference result = response[&#39;Body&#39;].read().decode(&#39;utf-8&#39;) return { &#39;statusCode&#39; : 200, &#39;headers&#39; : { &#39;Content-Type&#39; : &#39;text/plain&#39;, &#39;Access-Control-Allow-Origin&#39; : &#39;*&#39; }, &#39;body&#39; : result } . Once you have copy and pasted the code above into the Lambda code editor, replace the **ENDPOINT NAME HERE** portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below. . predictor.endpoint . &#39;sagemaker-pytorch-2020-06-04-14-11-55-240&#39; . Once you have added the endpoint name to the Lambda function, click on Save. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function. . Setting up API Gateway . Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created. . Using AWS Console, navigate to Amazon API Gateway and then click on Get started. . On the next page, make sure that New API is selected and give the new api a name, for example, sentiment_analysis_api. Then, click on Create API. . Now we have created an API, however it doesn&#39;t currently do anything. What we want it to do is to trigger the Lambda function that we created earlier. . Select the Actions dropdown menu and click Create Method. A new blank method will be created, select its dropdown menu and select POST, then click on the check mark beside it. . For the integration point, make sure that Lambda Function is selected and click on the Use Lambda Proxy integration. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway. . Type the name of the Lambda function you created earlier into the Lambda Function text entry box and then click on Save. Click on OK in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created. . The last step in creating the API Gateway is to select the Actions dropdown and click on Deploy API. You will need to create a new Deployment stage and name it anything you like, for example prod. . You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text Invoke URL. . Step 4: Deploying our web app . Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier. . In the website folder there should be a file called index.html. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **REPLACE WITH PUBLIC API URL**. Replace this string with the url that you wrote down in the last step and then save the file. . Now, if you open index.html on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model. . If you&#39;d like to go further, you can host this html file anywhere you&#39;d like, for example using github or hosting a static site on Amazon&#39;s S3. Once you have done this you can share the link with anyone you&#39;d like and have them play with it too! . Important Note In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don&#39;t need it, otherwise you will end up with a surprisingly large AWS bill. . TODO: Make sure that you include the edited index.html file in your project submission. . Now that your web app is working, trying playing around with it and see how well it works. . Question: Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review? . Answer: I looked at the reviews of Men in black, one positive and the other negative. Here are the results. . Delete the endpoint . Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill. . predictor.delete_endpoint() .",
            "url": "https://manisaiprasad.github.io/notes/2020/06/04/SageMaker-Project.html",
            "relUrl": "/2020/06/04/SageMaker-Project.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Face Generation",
            "content": "# can comment out after executing !unzip processed_celeba_small.zip . Archive: processed_celeba_small.zip replace processed_celeba_small/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C . data_dir = &#39;processed_celeba_small/&#39; &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL &quot;&quot;&quot; import pickle as pkl import matplotlib.pyplot as plt import numpy as np import problem_unittests as tests #import helper %matplotlib inline . Visualize the CelebA Data . The CelebA dataset contains over 200,000 celebrity images with annotations. Since you&#39;re going to be generating faces, you won&#39;t need the annotations, you&#39;ll only need the images. Note that these are color images with 3 color channels (RGB)#RGB_Images) each. . Pre-process and Load the Data . Since the project&#39;s main focus is on building the GANs, we&#39;ve done some of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don&#39;t include a face, then resized down to 64x64x3 NumPy images. This pre-processed dataset is a smaller subset of the very large CelebA data. . There are a few other steps that you&#39;ll need to transform this data and create a DataLoader. . Exercise:Complete the following get_dataloader function, such that it satisfies these requirements: . Your images should be square, Tensor images of size image_size x image_size in the x and y dimension. | Your function should return a DataLoader that shuffles and batches these Tensor images. | . ImageFolder . To create a dataset given a directory of images, it&#39;s recommended that you use PyTorch&#39;s ImageFolder wrapper, with a root directory processed_celeba_small/ and data transformation passed in. . # necessary imports import torch from torchvision import datasets from torchvision import transforms . def get_dataloader(batch_size, image_size, data_dir=&#39;processed_celeba_small/&#39;): &quot;&quot;&quot; Batch the neural network data using DataLoader :param batch_size: The size of each batch; the number of images in a batch :param img_size: The square size of the image data (x, y) :param data_dir: Directory where image data is located :return: DataLoader with batched data &quot;&quot;&quot; # TODO: Implement function and return a dataloader transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()]) dataset = datasets.ImageFolder(data_dir, transform) return torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True) . Create a DataLoader . Exercise: Create a DataLoader celeba_train_loader with appropriate hyperparameters. . Call the above function and create a dataloader to view images. . You can decide on any reasonable batch_size parameter | Your image_size must be 32. Resizing the data to a smaller size will make for faster training, while still creating convincing images of faces! | . # Define function hyperparameters batch_size = 40 img_size = 32 &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; # Call your function and get a dataloader celeba_train_loader = get_dataloader(batch_size, img_size) . Next, you can view some images! You should seen square images of somewhat-centered faces. . Note: You&#39;ll need to convert the Tensor images into a NumPy type and transpose the dimensions to correctly display an image, suggested imshow code is below, but it may not be perfect. . # helper display function def imshow(img): npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; # obtain one batch of training images dataiter = iter(celeba_train_loader) images, _ = dataiter.next() # _ for no labels # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(20, 4)) plot_size=20 for idx in np.arange(plot_size): ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[]) imshow(images[idx]) . Exercise: Pre-process your image data and scale it to a pixel range of -1 to 1 . You need to do a bit of pre-processing; you know that the output of a tanh activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.) . # TODO: Complete the scale function def scale(x, feature_range=(-1, 1)): &#39;&#39;&#39; Scale takes in an image x and returns that image, scaled with a feature_range of pixel values from -1 to 1. This function assumes that the input x is already scaled from 0-1.&#39;&#39;&#39; # assume x is scaled to (0, 1) # scale to feature_range and return scaled x min, max = feature_range return x * (max - min) + min . &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; # check scaled range # should be close to -1 to 1 img = images[0] scaled_img = scale(img) print(&#39;Min: &#39;, scaled_img.min()) print(&#39;Max: &#39;, scaled_img.max()) . Min: tensor(-0.8902) Max: tensor(0.5451) . . Define the Model . A GAN is comprised of two adversarial networks, a discriminator and a generator. . Discriminator . Your first task will be to define the discriminator. This is a convolutional classifier like you&#39;ve built before, only without any maxpooling layers. To deal with this complex data, it&#39;s suggested you use a deep network with normalization. You are also allowed to create any helper functions that may be useful. . Exercise: Complete the Discriminator class . The inputs to the discriminator are 32x32x3 tensor images | The output should be a single value that will indicate whether a given image is real or fake | . import torch.nn as nn import torch.nn.functional as F #conv function def conv(in_channels, out_channels, kernel_size, stride=2, padding = 1, batch_norm= True): layers = [] conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,kernel_size = kernel_size, stride = stride, padding = padding, bias = False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) . class Discriminator(nn.Module): def __init__(self, conv_dim): &quot;&quot;&quot; Initialize the Discriminator Module :param conv_dim: The depth of the first convolutional layer &quot;&quot;&quot; super(Discriminator, self).__init__() # complete init function self.conv_dim = conv_dim self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # x, y = 64 depth = 3 self.conv2 = conv(conv_dim, conv_dim * 2, 4) # x, y = 32 depth = 64 self.conv3 = conv(conv_dim * 2, conv_dim * 4, 4) # x, y = 16 depth = 128 self.fc = nn.Linear(conv_dim*4*4*4, 1) self.out = nn.Sigmoid() self.dropout = nn.Dropout(0.3) def forward(self, x): &quot;&quot;&quot; Forward propagation of the neural network :param x: The input to the neural network :return: Discriminator logits; the output of the neural network &quot;&quot;&quot; # define feedforward behavior x = F.leaky_relu(self.conv1(x), 0.2) x = F.leaky_relu(self.conv2(x), 0.2) x = F.leaky_relu(self.conv3(x), 0.2) x = x.view(-1, self.conv_dim*4*4*4) x = self.fc(x) x = self.dropout(x) return x &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; tests.test_discriminator(Discriminator) . Tests Passed . Generator . The generator should upsample an input and generate a new image of the same size as our training data 32x32x3. This should be mostly transpose convolutional layers with normalization applied to the outputs. . Exercise: Complete the Generator class . The inputs to the generator are vectors of some length z_size | The output should be a image of shape 32x32x3 | . def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Generator(nn.Module): def __init__(self, z_size, conv_dim): &quot;&quot;&quot; Initialize the Generator Module :param z_size: The length of the input latent vector, z :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer &quot;&quot;&quot; super(Generator, self).__init__() # complete init function self.conv_dim = conv_dim self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4 ) self.t_conv2 = deconv(conv_dim*2, conv_dim, 4) self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False) self.fc = nn.Linear(z_size, conv_dim*4*4*4) self.dropout = nn.Dropout(0.3) def forward(self, x): &quot;&quot;&quot; Forward propagation of the neural network :param x: The input to the neural network :return: A 32x32x3 Tensor image as output &quot;&quot;&quot; # define feedforward behavior x = self.fc(x) x = self.dropout(x) x = x.view(-1, self.conv_dim*4, 4, 4) x = F.relu(self.t_conv1(x)) x = F.relu(self.t_conv2(x)) x = F.tanh(self.t_conv3(x)) return x &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; tests.test_generator(Generator) . Tests Passed . Initialize the weights of your networks . To help your models converge, you should initialize the weights of the convolutional and linear layers in your model. From reading the original DCGAN paper, they say: . All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. . So, your next task will be to define a weight initialization function that does just this! . You can refer back to the lesson on weight initialization or even consult existing model code, such as that from the networks.py file in CycleGAN Github repository to help you complete this function. . Exercise: Complete the weight initialization function . This should initialize only convolutional and linear layers | Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02. | The bias terms, if they exist, may be left alone or set to 0. | . def weights_init_normal(m): &quot;&quot;&quot; Applies initial weights to certain layers in a model . The weights are taken from a normal distribution with mean = 0, std dev = 0.02. :param m: A module or layer in a network &quot;&quot;&quot; # classname will be something like: # `Conv`, `BatchNorm2d`, `Linear`, etc. classname = m.__class__.__name__ # TODO: Apply initial weights to convolutional and linear layers if hasattr(m, &#39;weight&#39;) and (classname.find(&#39;Conv&#39;) != -1 or classname.find(&#39;Linear&#39;) != -1): nn.init.normal_(m.weight.data, 0.0, 0.02) if hasattr(m.bias, &#39;data&#39;): nn.init.constant_(m.bias.data, 0.0) . Build complete network . Define your models&#39; hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you&#39;ve passed in the correct input arguments. . &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; def build_network(d_conv_dim, g_conv_dim, z_size): # define discriminator and generator D = Discriminator(d_conv_dim) G = Generator(z_size=z_size, conv_dim=g_conv_dim) # initialize model weights D.apply(weights_init_normal) G.apply(weights_init_normal) print(D) print() print(G) return D, G . Exercise: Define model hyperparameters . # Define model hyperparams d_conv_dim = 32 g_conv_dim = 32 z_size = 100 &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE &quot;&quot;&quot; D, G = build_network(d_conv_dim, g_conv_dim, z_size) . Discriminator( (conv1): Sequential( (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) (conv2): Sequential( (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv3): Sequential( (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (fc): Linear(in_features=2048, out_features=1, bias=True) (out): Sigmoid() (dropout): Dropout(p=0.3) ) Generator( (t_conv1): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv2): Sequential( (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv3): Sequential( (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) (fc): Linear(in_features=100, out_features=2048, bias=True) (dropout): Dropout(p=0.3) ) . Training on GPU . Check if you can train on GPU. Here, we&#39;ll set this as a boolean variable train_on_gpu. Later, you&#39;ll be responsible for making sure that . Models, | Model inputs, and | Loss function arguments | . Are moved to GPU, where appropriate. . &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL &quot;&quot;&quot; import torch # Check for a GPU train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;No GPU found. Please use a GPU to train your neural network.&#39;) else: print(&#39;Training on GPU!&#39;) . Training on GPU! . . Discriminator and Generator Losses . Now we need to calculate the losses for both types of adversarial networks. . Discriminator Losses . For the discriminator, the total loss is the sum of the losses for real and fake images, d_loss = d_real_loss + d_fake_loss. | Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that. | . Generator Loss . The generator loss will look similar only with flipped labels. The generator&#39;s goal is to get the discriminator to think its generated images are real. . Exercise:Complete real and fake loss functions . You may choose to use either cross entropy or a least squares error loss to complete the following real_loss and fake_loss functions. . def real_loss(D_out): &#39;&#39;&#39;Calculates how close discriminator outputs are to being real. param, D_out: discriminator logits return: real loss&#39;&#39;&#39; batch_size = D_out.size(0) labels = torch.ones(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss def fake_loss(D_out): &#39;&#39;&#39;Calculates how close discriminator outputs are to being fake. param, D_out: discriminator logits return: fake loss&#39;&#39;&#39; batch_size = D_out.size(0) labels = torch.zeros(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss . Optimizers . Exercise: Define optimizers for your Discriminator (D) and Generator (G) . Define optimizers for your models with appropriate hyperparameters. . import torch.optim as optim # Create optimizers for the discriminator D and generator G d_optimizer = optim.Adam(D.parameters(), lr=0.0005, betas=(0.5, 0.999)) g_optimizer = optim.Adam(G.parameters(), lr=0.0005, betas=(0.5, 0.999)) . . Training . Training will involve alternating between training the discriminator and the generator. You&#39;ll use your functions real_loss and fake_loss to help you calculate the discriminator losses. . You should train the discriminator by alternating on real and fake images | Then the generator, which tries to trick the discriminator and should have an opposing loss function | . Saving Samples . You&#39;ve been given some code to print out some loss statistics and save some generated &quot;fake&quot; samples. . Exercise: Complete the training function . Keep in mind that, if you&#39;ve moved your models to GPU, you&#39;ll also have to move any model inputs to GPU. . def train(D, G, n_epochs, print_every=50): &#39;&#39;&#39;Trains adversarial networks for some number of epochs param, D: the discriminator network param, G: the generator network param, n_epochs: number of epochs to train for param, print_every: when to print and record the models&#39; losses return: D and G losses&#39;&#39;&#39; # move models to GPU if train_on_gpu: D.cuda() G.cuda() # keep track of loss and generated, &quot;fake&quot; samples samples = [] losses = [] # Get some fixed data for sampling. These are images that are held # constant throughout training, and allow us to inspect the model&#39;s performance sample_size=16 fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) fixed_z = torch.from_numpy(fixed_z).float() # move z to GPU if available if train_on_gpu: fixed_z = fixed_z.cuda() # epoch training loop for epoch in range(n_epochs): # batch training loop for batch_i, (real_images, _) in enumerate(celeba_train_loader): batch_size = real_images.size(0) real_images = scale(real_images) # =============================================== # YOUR CODE HERE: TRAIN THE NETWORKS # =============================================== # 1. Train the discriminator on real and fake images if train_on_gpu: real_images = real_images.cuda() d_optimizer.zero_grad() D_real = D(real_images) d_real_loss = real_loss(D_real) z_flex = np.random.uniform(-1, 1, size=(batch_size, z_size)) z_flex = torch.from_numpy(z_flex).float() if train_on_gpu: z_flex = z_flex.cuda() fake_images = G(z_flex) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() # 2. Train the generator with an adversarial loss g_optimizer.zero_grad() z_flex = np.random.uniform(-1, 1, size=(batch_size, z_size)) z_flex = torch.from_numpy(z_flex).float() if train_on_gpu: z_flex = z_flex.cuda() fake_images = G(z_flex) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_loss.backward() g_optimizer.step() # =============================================== # END OF YOUR CODE # =============================================== # Print some loss stats if batch_i % print_every == 0: # append discriminator loss and generator loss losses.append((d_loss.item(), g_loss.item())) # print discriminator and generator loss print(&#39;Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}&#39;.format( epoch+1, n_epochs, d_loss.item(), g_loss.item())) ## AFTER EACH EPOCH## # this code assumes your generator is named G, feel free to change the name # generate and save sample, fake images G.eval() # for generating samples samples_z = G(fixed_z) samples.append(samples_z) G.train() # back to training mode # Save training generator samples with open(&#39;train_samples.pkl&#39;, &#39;wb&#39;) as f: pkl.dump(samples, f) # finally return losses return losses . Set your number of training epochs and train your GAN! . # set number of epochs n_epochs = 4 &quot;&quot;&quot; DON&#39;T MODIFY ANYTHING IN THIS CELL &quot;&quot;&quot; # call training function losses = train(D, G, n_epochs=n_epochs) . Epoch [ 1/ 4] | d_loss: 0.9543 | g_loss: 1.5350 Epoch [ 1/ 4] | d_loss: 0.5929 | g_loss: 3.2429 Epoch [ 1/ 4] | d_loss: 0.6430 | g_loss: 2.0835 Epoch [ 1/ 4] | d_loss: 0.6437 | g_loss: 1.9817 Epoch [ 1/ 4] | d_loss: 0.6648 | g_loss: 2.7845 Epoch [ 1/ 4] | d_loss: 0.7008 | g_loss: 1.9217 Epoch [ 1/ 4] | d_loss: 0.6131 | g_loss: 1.6540 Epoch [ 1/ 4] | d_loss: 0.4068 | g_loss: 3.4382 Epoch [ 1/ 4] | d_loss: 0.7518 | g_loss: 3.2530 Epoch [ 1/ 4] | d_loss: 0.6950 | g_loss: 2.3203 Epoch [ 1/ 4] | d_loss: 0.6637 | g_loss: 2.9369 Epoch [ 1/ 4] | d_loss: 0.5528 | g_loss: 2.3518 Epoch [ 1/ 4] | d_loss: 1.0430 | g_loss: 2.4640 Epoch [ 1/ 4] | d_loss: 0.5910 | g_loss: 2.4881 Epoch [ 1/ 4] | d_loss: 0.7038 | g_loss: 1.4659 Epoch [ 1/ 4] | d_loss: 0.7219 | g_loss: 1.4134 Epoch [ 1/ 4] | d_loss: 0.5838 | g_loss: 3.3896 Epoch [ 1/ 4] | d_loss: 0.5991 | g_loss: 2.9405 Epoch [ 1/ 4] | d_loss: 0.6622 | g_loss: 2.8994 Epoch [ 1/ 4] | d_loss: 0.7982 | g_loss: 2.2967 Epoch [ 1/ 4] | d_loss: 0.8091 | g_loss: 4.5961 Epoch [ 1/ 4] | d_loss: 0.9745 | g_loss: 2.0797 Epoch [ 1/ 4] | d_loss: 0.5561 | g_loss: 2.8210 Epoch [ 1/ 4] | d_loss: 1.3526 | g_loss: 1.9864 Epoch [ 1/ 4] | d_loss: 0.7917 | g_loss: 3.1085 Epoch [ 1/ 4] | d_loss: 0.6917 | g_loss: 1.7029 Epoch [ 1/ 4] | d_loss: 0.5557 | g_loss: 3.7095 Epoch [ 1/ 4] | d_loss: 0.5642 | g_loss: 2.7531 Epoch [ 1/ 4] | d_loss: 0.5263 | g_loss: 2.8755 Epoch [ 1/ 4] | d_loss: 0.6113 | g_loss: 2.3179 Epoch [ 1/ 4] | d_loss: 0.5461 | g_loss: 2.6818 Epoch [ 1/ 4] | d_loss: 0.7528 | g_loss: 2.1431 Epoch [ 1/ 4] | d_loss: 0.7829 | g_loss: 2.3606 Epoch [ 1/ 4] | d_loss: 0.4917 | g_loss: 3.0870 Epoch [ 1/ 4] | d_loss: 0.8348 | g_loss: 1.6196 Epoch [ 1/ 4] | d_loss: 0.4584 | g_loss: 4.6504 Epoch [ 1/ 4] | d_loss: 0.5668 | g_loss: 1.6971 Epoch [ 1/ 4] | d_loss: 0.6959 | g_loss: 2.6015 Epoch [ 1/ 4] | d_loss: 0.8386 | g_loss: 3.0574 Epoch [ 1/ 4] | d_loss: 0.8319 | g_loss: 2.1612 Epoch [ 1/ 4] | d_loss: 0.8606 | g_loss: 1.9073 Epoch [ 1/ 4] | d_loss: 0.7202 | g_loss: 3.1903 Epoch [ 1/ 4] | d_loss: 0.8003 | g_loss: 2.5057 Epoch [ 1/ 4] | d_loss: 0.7087 | g_loss: 1.5643 Epoch [ 1/ 4] | d_loss: 0.5708 | g_loss: 3.1067 Epoch [ 2/ 4] | d_loss: 0.6891 | g_loss: 2.3912 Epoch [ 2/ 4] | d_loss: 0.6651 | g_loss: 3.4993 Epoch [ 2/ 4] | d_loss: 0.6724 | g_loss: 3.8782 Epoch [ 2/ 4] | d_loss: 0.6112 | g_loss: 2.7862 Epoch [ 2/ 4] | d_loss: 0.6992 | g_loss: 2.2919 Epoch [ 2/ 4] | d_loss: 0.5953 | g_loss: 3.5657 Epoch [ 2/ 4] | d_loss: 0.9395 | g_loss: 1.3864 Epoch [ 2/ 4] | d_loss: 0.6467 | g_loss: 2.6107 Epoch [ 2/ 4] | d_loss: 0.5340 | g_loss: 2.6843 Epoch [ 2/ 4] | d_loss: 0.6760 | g_loss: 1.9171 Epoch [ 2/ 4] | d_loss: 0.7779 | g_loss: 1.9786 Epoch [ 2/ 4] | d_loss: 0.6068 | g_loss: 2.8820 Epoch [ 2/ 4] | d_loss: 0.5188 | g_loss: 3.2137 Epoch [ 2/ 4] | d_loss: 0.6281 | g_loss: 2.0057 Epoch [ 2/ 4] | d_loss: 0.6045 | g_loss: 3.0687 Epoch [ 2/ 4] | d_loss: 0.7373 | g_loss: 3.0363 Epoch [ 2/ 4] | d_loss: 0.7076 | g_loss: 2.6595 Epoch [ 2/ 4] | d_loss: 0.5447 | g_loss: 1.9639 Epoch [ 2/ 4] | d_loss: 0.6645 | g_loss: 1.8272 Epoch [ 2/ 4] | d_loss: 0.5412 | g_loss: 2.1932 Epoch [ 2/ 4] | d_loss: 0.5731 | g_loss: 2.6391 Epoch [ 2/ 4] | d_loss: 0.6522 | g_loss: 3.2824 Epoch [ 2/ 4] | d_loss: 0.6336 | g_loss: 3.0962 Epoch [ 2/ 4] | d_loss: 0.5573 | g_loss: 1.9972 Epoch [ 2/ 4] | d_loss: 0.9897 | g_loss: 2.0034 Epoch [ 2/ 4] | d_loss: 0.4868 | g_loss: 3.6994 Epoch [ 2/ 4] | d_loss: 0.6374 | g_loss: 2.6883 Epoch [ 2/ 4] | d_loss: 0.5325 | g_loss: 3.0683 Epoch [ 2/ 4] | d_loss: 0.5658 | g_loss: 2.6274 Epoch [ 2/ 4] | d_loss: 0.8498 | g_loss: 1.6561 Epoch [ 2/ 4] | d_loss: 0.7226 | g_loss: 2.9423 Epoch [ 2/ 4] | d_loss: 0.4388 | g_loss: 3.7304 Epoch [ 2/ 4] | d_loss: 0.9574 | g_loss: 1.5787 Epoch [ 2/ 4] | d_loss: 0.7695 | g_loss: 2.5766 Epoch [ 2/ 4] | d_loss: 0.6120 | g_loss: 2.8754 Epoch [ 2/ 4] | d_loss: 0.6594 | g_loss: 2.6452 Epoch [ 2/ 4] | d_loss: 0.5776 | g_loss: 2.6482 Epoch [ 2/ 4] | d_loss: 0.8700 | g_loss: 3.0829 Epoch [ 2/ 4] | d_loss: 1.1771 | g_loss: 0.3470 Epoch [ 2/ 4] | d_loss: 0.9872 | g_loss: 1.9041 Epoch [ 2/ 4] | d_loss: 0.4907 | g_loss: 2.8731 Epoch [ 2/ 4] | d_loss: 0.6997 | g_loss: 3.1965 Epoch [ 2/ 4] | d_loss: 0.4731 | g_loss: 2.5641 Epoch [ 2/ 4] | d_loss: 0.4805 | g_loss: 2.4071 Epoch [ 2/ 4] | d_loss: 0.6692 | g_loss: 2.5874 Epoch [ 3/ 4] | d_loss: 0.8139 | g_loss: 2.6130 Epoch [ 3/ 4] | d_loss: 0.5827 | g_loss: 1.8789 Epoch [ 3/ 4] | d_loss: 0.5186 | g_loss: 2.6960 Epoch [ 3/ 4] | d_loss: 0.4982 | g_loss: 2.3930 Epoch [ 3/ 4] | d_loss: 0.5548 | g_loss: 2.8024 Epoch [ 3/ 4] | d_loss: 0.5767 | g_loss: 2.8569 Epoch [ 3/ 4] | d_loss: 0.6634 | g_loss: 3.0444 Epoch [ 3/ 4] | d_loss: 0.6918 | g_loss: 3.1840 Epoch [ 3/ 4] | d_loss: 0.8059 | g_loss: 2.6495 Epoch [ 3/ 4] | d_loss: 0.7115 | g_loss: 2.4581 Epoch [ 3/ 4] | d_loss: 0.7276 | g_loss: 1.9877 Epoch [ 3/ 4] | d_loss: 1.6601 | g_loss: 4.7520 Epoch [ 3/ 4] | d_loss: 0.8051 | g_loss: 3.0616 Epoch [ 3/ 4] | d_loss: 0.8769 | g_loss: 4.9627 Epoch [ 3/ 4] | d_loss: 0.7601 | g_loss: 1.6775 Epoch [ 3/ 4] | d_loss: 0.6340 | g_loss: 2.1903 Epoch [ 3/ 4] | d_loss: 0.5626 | g_loss: 3.2477 Epoch [ 3/ 4] | d_loss: 0.6305 | g_loss: 2.9768 Epoch [ 3/ 4] | d_loss: 0.4766 | g_loss: 3.5111 Epoch [ 3/ 4] | d_loss: 0.6240 | g_loss: 2.7864 Epoch [ 3/ 4] | d_loss: 0.5936 | g_loss: 3.0884 Epoch [ 3/ 4] | d_loss: 0.6003 | g_loss: 3.2051 Epoch [ 3/ 4] | d_loss: 0.4966 | g_loss: 1.3002 Epoch [ 3/ 4] | d_loss: 0.6618 | g_loss: 2.7635 Epoch [ 3/ 4] | d_loss: 0.6841 | g_loss: 3.6588 Epoch [ 3/ 4] | d_loss: 0.6752 | g_loss: 1.7802 Epoch [ 3/ 4] | d_loss: 0.5633 | g_loss: 2.0170 Epoch [ 3/ 4] | d_loss: 0.6335 | g_loss: 3.3707 Epoch [ 3/ 4] | d_loss: 0.5484 | g_loss: 2.2442 Epoch [ 3/ 4] | d_loss: 0.6060 | g_loss: 2.1775 Epoch [ 3/ 4] | d_loss: 0.6508 | g_loss: 3.7853 Epoch [ 3/ 4] | d_loss: 0.6711 | g_loss: 2.3415 Epoch [ 3/ 4] | d_loss: 0.7224 | g_loss: 1.3784 Epoch [ 3/ 4] | d_loss: 0.9187 | g_loss: 3.2816 Epoch [ 3/ 4] | d_loss: 0.6265 | g_loss: 2.0665 Epoch [ 3/ 4] | d_loss: 0.4166 | g_loss: 2.6612 Epoch [ 3/ 4] | d_loss: 0.5769 | g_loss: 3.6045 Epoch [ 3/ 4] | d_loss: 0.5875 | g_loss: 2.2835 Epoch [ 3/ 4] | d_loss: 0.9718 | g_loss: 1.8930 Epoch [ 3/ 4] | d_loss: 0.6753 | g_loss: 2.7544 Epoch [ 3/ 4] | d_loss: 0.6786 | g_loss: 2.3325 Epoch [ 3/ 4] | d_loss: 0.5801 | g_loss: 2.6541 Epoch [ 3/ 4] | d_loss: 0.7057 | g_loss: 3.6706 Epoch [ 3/ 4] | d_loss: 0.5865 | g_loss: 2.4676 Epoch [ 3/ 4] | d_loss: 0.4876 | g_loss: 3.2982 Epoch [ 4/ 4] | d_loss: 0.7296 | g_loss: 2.6619 Epoch [ 4/ 4] | d_loss: 0.7047 | g_loss: 3.0570 Epoch [ 4/ 4] | d_loss: 0.5510 | g_loss: 3.4973 Epoch [ 4/ 4] | d_loss: 0.7429 | g_loss: 2.6766 Epoch [ 4/ 4] | d_loss: 0.6061 | g_loss: 3.5634 Epoch [ 4/ 4] | d_loss: 0.5285 | g_loss: 2.2621 Epoch [ 4/ 4] | d_loss: 0.5936 | g_loss: 1.7881 Epoch [ 4/ 4] | d_loss: 0.6385 | g_loss: 2.0635 Epoch [ 4/ 4] | d_loss: 0.4948 | g_loss: 3.1917 Epoch [ 4/ 4] | d_loss: 0.8046 | g_loss: 2.4466 Epoch [ 4/ 4] | d_loss: 0.6530 | g_loss: 1.7120 Epoch [ 4/ 4] | d_loss: 0.8158 | g_loss: 3.3746 Epoch [ 4/ 4] | d_loss: 0.8810 | g_loss: 3.0753 Epoch [ 4/ 4] | d_loss: 0.6491 | g_loss: 2.2416 Epoch [ 4/ 4] | d_loss: 0.5951 | g_loss: 3.4942 Epoch [ 4/ 4] | d_loss: 0.6886 | g_loss: 3.4805 Epoch [ 4/ 4] | d_loss: 0.6784 | g_loss: 1.9638 Epoch [ 4/ 4] | d_loss: 0.6050 | g_loss: 2.8853 Epoch [ 4/ 4] | d_loss: 0.6489 | g_loss: 2.2782 Epoch [ 4/ 4] | d_loss: 0.8394 | g_loss: 2.9367 Epoch [ 4/ 4] | d_loss: 0.4878 | g_loss: 2.9896 Epoch [ 4/ 4] | d_loss: 0.7408 | g_loss: 3.6535 Epoch [ 4/ 4] | d_loss: 0.7079 | g_loss: 2.2508 Epoch [ 4/ 4] | d_loss: 0.6588 | g_loss: 2.0329 Epoch [ 4/ 4] | d_loss: 0.4121 | g_loss: 2.2812 Epoch [ 4/ 4] | d_loss: 0.5546 | g_loss: 2.4180 Epoch [ 4/ 4] | d_loss: 0.6394 | g_loss: 3.0900 Epoch [ 4/ 4] | d_loss: 0.9115 | g_loss: 3.4219 Epoch [ 4/ 4] | d_loss: 0.8249 | g_loss: 2.5315 Epoch [ 4/ 4] | d_loss: 0.6281 | g_loss: 3.1169 Epoch [ 4/ 4] | d_loss: 0.7264 | g_loss: 3.1522 Epoch [ 4/ 4] | d_loss: 0.6780 | g_loss: 2.9495 Epoch [ 4/ 4] | d_loss: 0.5666 | g_loss: 2.9044 Epoch [ 4/ 4] | d_loss: 0.5735 | g_loss: 2.9984 Epoch [ 4/ 4] | d_loss: 7.0042 | g_loss: 1.7333 Epoch [ 4/ 4] | d_loss: 0.6597 | g_loss: 2.9417 Epoch [ 4/ 4] | d_loss: 0.4735 | g_loss: 2.9011 Epoch [ 4/ 4] | d_loss: 0.6370 | g_loss: 3.5389 Epoch [ 4/ 4] | d_loss: 0.4648 | g_loss: 2.7975 Epoch [ 4/ 4] | d_loss: 0.5486 | g_loss: 2.5830 Epoch [ 4/ 4] | d_loss: 0.6145 | g_loss: 2.6738 Epoch [ 4/ 4] | d_loss: 0.5349 | g_loss: 5.3156 Epoch [ 4/ 4] | d_loss: 0.6774 | g_loss: 2.9797 Epoch [ 4/ 4] | d_loss: 0.5850 | g_loss: 2.2509 Epoch [ 4/ 4] | d_loss: 0.4623 | g_loss: 2.7988 . Training loss . Plot the training losses for the generator and discriminator, recorded after each epoch. . fig, ax = plt.subplots() losses = np.array(losses) plt.plot(losses.T[0], label=&#39;Discriminator&#39;, alpha=0.5) plt.plot(losses.T[1], label=&#39;Generator&#39;, alpha=0.5) plt.title(&quot;Training Losses&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f047c093550&gt; . Generator samples from training . View samples of images from the generator, and answer a question about the strengths and weaknesses of your trained models. . # helper function for viewing a list of passed in sample images def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach().cpu().numpy() img = np.transpose(img, (1, 2, 0)) img = ((img + 1)*255 / (2)).astype(np.uint8) ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((32,32,3))) . # Load samples from generator, taken while training with open(&#39;train_samples.pkl&#39;, &#39;rb&#39;) as f: samples = pkl.load(f) . _ = view_samples(-1, samples) . Question: What do you notice about your generated samples and how might you improve this model? . When you answer this question, consider the following factors: . The dataset is biased; it is made of &quot;celebrity&quot; faces that are mostly white | Model size; larger models have the opportunity to learn more features in a data feature space | Optimization strategy; optimizers and number of epochs affect your final result | . Answer: (Write your answer in this cell) . The dataset is biased I Think More faces might help while Training GAN for better results to generate a new type of faces. . I trained the model for 10 epochs with different optimizer and network size. and i got learning rate value as 0.0002 . I Used Adam, I Think it is the best choice for GAN&#39;s . | Reducing the epochs can produce the best result. . | . The images are of very low resulution, which makes it harder to add more CNN layers . Submitting This Project . When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as &quot;dlnd_face_generation.ipynb&quot; and save it as a HTML file under &quot;File&quot; -&gt; &quot;Download as&quot;. Include the &quot;problem_unittests.py&quot; files in your submission. .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/30/dlnd_face_generation.html",
            "relUrl": "/2020/05/30/dlnd_face_generation.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Convolutional Neural Networks",
            "content": "import numpy as np from glob import glob # load filenames for human and dog images human_files = np.array(glob(&quot;/data/lfw/*/*&quot;)) dog_files = np.array(glob(&quot;/data/dog_images/*/*/*&quot;)) # print number of images in each dataset print(&#39;There are %d total human images.&#39; % len(human_files)) print(&#39;There are %d total dog images.&#39; % len(dog_files)) . There are 13233 total human images. There are 8351 total dog images. . . Step 1: Detect Humans . In this section, we use OpenCV&#39;s implementation of Haar feature-based cascade classifiers to detect human faces in images. . OpenCV provides many pre-trained face detectors, stored as XML files on github. We have downloaded one of these detectors and stored it in the haarcascades directory. In the next code cell, we demonstrate how to use this detector to find human faces in a sample image. . import cv2 import matplotlib.pyplot as plt %matplotlib inline # extract pre-trained face detector face_cascade = cv2.CascadeClassifier(&#39;haarcascades/haarcascade_frontalface_alt.xml&#39;) # load color (BGR) image img = cv2.imread(human_files[0]) # convert BGR image to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # find faces in image faces = face_cascade.detectMultiScale(gray) # print number of faces detected in the image print(&#39;Number of faces detected:&#39;, len(faces)) # get bounding box for each detected face for (x,y,w,h) in faces: # add bounding box to color image cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) # convert BGR image to RGB for plotting cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # display the image, along with bounding box plt.imshow(cv_rgb) plt.show() . Number of faces detected: 1 . Before using any of the face detectors, it is standard procedure to convert the images to grayscale. The detectMultiScale function executes the classifier stored in face_cascade and takes the grayscale image as a parameter. . In the above code, faces is a numpy array of detected faces, where each row corresponds to a detected face. Each detected face is a 1D array with four entries that specifies the bounding box of the detected face. The first two entries in the array (extracted in the above code as x and y) specify the horizontal and vertical positions of the top left corner of the bounding box. The last two entries in the array (extracted here as w and h) specify the width and height of the box. . Write a Human Face Detector . We can use this procedure to write a function that returns True if a human face is detected in an image and False otherwise. This function, aptly named face_detector, takes a string-valued file path to an image as input and appears in the code block below. . # returns &quot;True&quot; if face is detected in image stored at img_path def face_detector(img_path): img = cv2.imread(img_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray) return len(faces) &gt; 0 . (IMPLEMENTATION) Assess the Human Face Detector . Question 1: Use the code cell below to test the performance of the face_detector function. . What percentage of the first 100 images in human_files have a detected human face? | What percentage of the first 100 images in dog_files have a detected human face? | . Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face. You will see that our algorithm falls short of this goal, but still gives acceptable performance. We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays human_files_short and dog_files_short. . Answer: (You can print out your results and/or write your percentages in this cell) . Percentage of Human faces dectected in human_files: 98.0 . Percentage of Human faces dectected in dog_files: 17.0 . from tqdm import tqdm human_files_short = human_files[:100] dog_files_short = dog_files[:100] #-#-# Do NOT modify the code above this line. #-#-# ## TODO: Test the performance of the face_detector algorithm ## on the images in human_files_short and dog_files_short. human_count = 0 print(&quot;Dectecting human faces in human files&quot;) for image in tqdm(human_files_short): if face_detector(image): human_count+=1 else: # print(&quot;No human faces dectected&quot;) pass print(&quot;Dectecting human faces in dog files&quot;) dog_count = 0 for image in tqdm(dog_files_short): if face_detector(image): dog_count+=1 else: # print(&quot;No human faces dectected&quot;) pass print(&quot; Percentage of Human faces dectected in human_files: &quot;,human_count/len(human_files_short)*100) print(&quot; Percentage of Human faces dectected in human_files: &quot;,dog_count/len(dog_files_short)*100) . 4%|▍ | 4/100 [00:00&lt;00:02, 36.20it/s] . Dectecting human faces in human files . 100%|██████████| 100/100 [00:02&lt;00:00, 35.48it/s] 0%| | 0/100 [00:00&lt;?, ?it/s] . Dectecting human faces in dog files . 100%|██████████| 100/100 [00:29&lt;00:00, 3.45it/s] . Percentage of Human faces dectected in human_files: 98.0 Percentage of Human faces dectected in human_files: 17.0 . . We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :). Please use the code cell below to design and test your own face detection algorithm. If you decide to pursue this optional task, report performance on human_files_short and dog_files_short. . ### (Optional) ### TODO: Test performance of anotherface detection algorithm. ### Feel free to use as many code cells as needed.### (Optional) ### TODO: Test performance of anotherface detection algorithm. ### Feel free to use as many code cells as needed. eye_cascade = cv2.CascadeClassifier(&#39;haarcascades/haarcascade_eye.xml&#39;) def face_detector_eye_img(img_path): img = cv2.imread(img_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray) for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_gray = gray[y:y+h, x:x+w] roi_color = img[y:y+h, x:x+w] eyes = eye_cascade.detectMultiScale(roi_gray) print(len(eyes)) for (ex,ey,ew,eh) in eyes: cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) # convert BGR image to RGB for plotting cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # display the image, along with bounding box plt.imshow(cv_rgb) plt.show() return len(faces) &gt; 0 and len(eyes)&gt;=1 face_detector_eye_img(human_files[2]) . 2 . True . def face_detector_eye(img_path): img = cv2.imread(img_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray) for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_gray = gray[y:y+h, x:x+w] roi_color = img[y:y+h, x:x+w] eyes = eye_cascade.detectMultiScale(roi_gray) for (ex,ey,ew,eh) in eyes: cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) # convert BGR image to RGB for plotting cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) return len(faces) &gt; 0 and len(eyes)&gt;=1 . from tqdm import tqdm human_files_short = human_files[:100] dog_files_short = dog_files[:100] #-#-# Do NOT modify the code above this line. #-#-# ## TODO: Test the performance of the face_detector algorithm ## on the images in human_files_short and dog_files_short. human_count = 0 print(&quot;Dectecting human faces in human files&quot;) for image in tqdm(human_files_short): if face_detector(image): human_count+=1 else: # print(&quot;No human faces dectected&quot;) pass print(&quot;Dectecting human faces in dog files&quot;) dog_count = 0 for image in tqdm(dog_files_short): if face_detector_eye(image): dog_count+=1 else: # print(&quot;No human faces dectected&quot;) pass print(&quot; Percentage of Human faces dectected in human_files: &quot;,human_count/len(human_files_short)*100) print(&quot; Percentage of Human faces dectected in human_files: &quot;,dog_count/len(dog_files_short)*100) . 4%|▍ | 4/100 [00:00&lt;00:02, 36.39it/s] . Dectecting human faces in human files . 100%|██████████| 100/100 [00:02&lt;00:00, 35.43it/s] 0%| | 0/100 [00:00&lt;?, ?it/s] . Dectecting human faces in dog files . 100%|██████████| 100/100 [00:28&lt;00:00, 7.44it/s] . Percentage of Human faces dectected in human_files: 98.0 Percentage of Human faces dectected in human_files: 6.0 . . Using eye_cascade for dog images reduced the percentage of Human faces dectected in dog_files from 17.0 to 6.0 . . . Step 2: Detect Dogs . In this section, we use a pre-trained model to detect dogs in images. . Obtain Pre-trained VGG-16 Model . The code cell below downloads the VGG-16 model, along with weights that have been trained on ImageNet, a very large, very popular dataset used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object from one of 1000 categories. . import torch import torchvision.models as models # define VGG16 model VGG16 = models.vgg16(pretrained=True) # check if CUDA is available use_cuda = torch.cuda.is_available() # move model to GPU if CUDA is available if use_cuda: VGG16 = VGG16.cuda() . Downloading: &#34;https://download.pytorch.org/models/vgg16-397923af.pth&#34; to /root/.torch/models/vgg16-397923af.pth 100%|██████████| 553433881/553433881 [00:05&lt;00:00, 93746451.23it/s] . Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image. . (IMPLEMENTATION) Making Predictions with a Pre-trained Model . In the next code cell, you will write a function that accepts a path to an image (such as &#39;dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg&#39;) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model. The output should always be an integer between 0 and 999, inclusive. . Before writing the function, make sure that you take the time to learn how to appropriately pre-process tensors for pre-trained models in the PyTorch documentation. . from PIL import Image import torchvision.transforms as transforms def VGG16_predict(img_path): &#39;&#39;&#39; Use pre-trained VGG-16 model to obtain index corresponding to predicted ImageNet class for image at specified path Args: img_path: path to an image Returns: Index corresponding to VGG-16 model&#39;s prediction &#39;&#39;&#39; ## TODO: Complete the function. ## Load and pre-process an image from the given img_path ## Return the *index* of the predicted class for that image image = Image.open(img_path) #VGG16 accept the shape (244, 244), so it resize to (244, 244) transform_img = transforms.Compose([ transforms.Resize(size=(244, 244)), transforms.ToTensor()]) image = transform_img(image)[:3,:,:].unsqueeze(0) if use_cuda: image = image.cuda() ret = VGG16(image) result = torch.max(ret,1)[1].item() return result # predicted class index VGG16_predict(human_files_short[1]) . 400 . (IMPLEMENTATION) Write a Dog Detector . While looking at the dictionary, you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from &#39;Chihuahua&#39; to &#39;Mexican hairless&#39;. Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive). . Use these ideas to complete the dog_detector function below, which returns True if a dog is detected in an image (and False if not). . ### returns &quot;True&quot; if a dog is detected in the image stored at img_path def dog_detector(img_path): ## TODO: Complete the function. if VGG16_predict(img_path)&gt;=151 and VGG16_predict(img_path)&lt;=268 : return True else: return False # true/false . (IMPLEMENTATION) Assess the Dog Detector . Question 2: Use the code cell below to test the performance of your dog_detector function. . What percentage of the images in human_files_short have a detected dog? | What percentage of the images in dog_files_short have a detected dog? | . Answer: . ### TODO: Test the performance of the dog_detector function ### on the images in human_files_short and dog_files_short. human_count = 0 print(&quot;Dectecting dogs in human files&quot;) for image in tqdm(human_files_short): if dog_detector(image): human_count+=1 print(&quot;Dectecting dogs in dog files&quot;) dog_count = 0 for image in tqdm(dog_files_short): if dog_detector(image): dog_count+=1 print(&quot; Percentage of dogs dectected in human_files: &quot;,human_count/len(human_files_short)*100) print(&quot; Percentage of dogs dectected in dog_files: &quot;,dog_count/len(dog_files_short)*100) . 1%| | 1/100 [00:00&lt;00:10, 9.21it/s] . Dectecting dogs in human files . 100%|██████████| 100/100 [00:08&lt;00:00, 12.44it/s] 1%| | 1/100 [00:00&lt;00:10, 9.63it/s] . Dectecting dogs in dog files . 100%|██████████| 100/100 [00:10&lt;00:00, 11.29it/s] . Percentage of dogs dectected in human_files: 0.0 Percentage of dogs dectected in dog_files: 97.0 . . We suggest VGG-16 as a potential network to detect dog images in your algorithm, but you are free to explore other pre-trained networks (such as Inception-v3, ResNet-50, etc). Please use the code cell below to test other pre-trained PyTorch models. If you decide to pursue this optional task, report performance on human_files_short and dog_files_short. . ### (Optional) ### TODO: Report the performance of another pre-trained network. ### Feel free to use as many code cells as needed. . . . Step 3: Create a CNN to Classify Dog Breeds (from Scratch) . Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images. In this step, you will create a CNN that classifies dog breeds. You must create your CNN from scratch (so, you can&#39;t use transfer learning yet!), and you must attain a test accuracy of at least 10%. In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy. . We mention that the task of assigning breed to dogs from images is considered exceptionally challenging. To see why, consider that even a human would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel. . Brittany Welsh Springer Spaniel . | | . It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels). . Curly-Coated Retriever American Water Spaniel . | | . Likewise, recall that labradors come in yellow, chocolate, and black. Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed. . Yellow Labrador Chocolate Labrador Black Labrador . | | | . We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%. . Remember that the practice is far ahead of the theory in deep learning. Experiment with many different architectures, and trust your intuition. And, of course, have fun! . (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset . Use the code cell below to write three separate data loaders for the training, validation, and test datasets of dog images (located at dog_images/train, dog_images/valid, and dog_images/test, respectively). You may find this documentation on custom datasets to be a useful resource. If you are interested in augmenting your training and/or validation data, check out the wide variety of transforms! . ### TODO: Write data loaders for training, validation, and test sets ## Specify appropriate transforms, and batch_sizes import torch import os from torchvision import datasets,transforms from torch import utils from tqdm import tqdm from PIL import ImageFile use_cuda = torch.cuda.is_available() ImageFile.LOAD_TRUNCATED_IMAGES = True batch_size=20 data_transforms = { &#39;train&#39; : transforms.Compose([ transforms.Resize(224),transforms.CenterCrop(224), transforms.RandomHorizontalFlip(), # randomly flip and rotate transforms.RandomRotation(10), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]), &#39;valid&#39; : transforms.Compose([ transforms.Resize(224),transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]), &#39;test&#39; : transforms.Compose([ transforms.Resize(224),transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]), } train_dir = &#39;/data/dog_images/train/&#39; valid_dir = &#39;/data/dog_images/valid/&#39; test_dir = &#39;/data/dog_images/test/&#39; image_datasets = { &#39;train&#39; : datasets.ImageFolder(root=train_dir,transform=data_transforms[&#39;train&#39;]), &#39;valid&#39; : datasets.ImageFolder(root=valid_dir,transform=data_transforms[&#39;valid&#39;]), &#39;test&#39; : datasets.ImageFolder(root=test_dir,transform=data_transforms[&#39;test&#39;]) } # Loading Dataset loaders_scratch = { &#39;train&#39; : torch.utils.data.DataLoader(image_datasets[&#39;train&#39;],batch_size = batch_size,shuffle=True), &#39;valid&#39; : torch.utils.data.DataLoader(image_datasets[&#39;valid&#39;],batch_size = batch_size), &#39;test&#39; : torch.utils.data.DataLoader(image_datasets[&#39;test&#39;],batch_size = batch_size) } . Question 3: Describe your chosen procedure for preprocessing the data. . How does your code resize the images (by cropping, stretching, etc)? What size did you pick for the input tensor, and why? | Did you decide to augment the dataset? If so, how (through translations, flips, rotations, etc)? If not, why not? | . Answer: . I have resized images to 256px and then cropped to 224 x 224, Lower size could be faster processing . Yes I augment the dataset, I have applied RandomRotation, RandomResizedCrop &amp; RandomHorizontalFlip to add more training variations to the dataset . And no need of image augmentation for the validation test set . (IMPLEMENTATION) Model Architecture . Create a CNN to classify dog breed. Use the template in the code cell below. . import torch.nn as nn import torch.nn.functional as F # define the CNN architecture class Net(nn.Module): ### TODO: choose an architecture, and complete the class def __init__(self): super(Net, self).__init__() ## Define layers of a CNN self.conv1 = nn.Conv2d(3,16,3,padding=1) self.conv2 = nn.Conv2d(16,32,3,padding=1) self.conv3 = nn.Conv2d(32,64,3,padding=1) self.conv4 = nn.Conv2d(64,128,3,padding=1) self.conv5 = nn.Conv2d(128,256,3,padding=1) self.pool = nn.MaxPool2d(2,2) self.fc1 = nn.Linear(12544,512) self.fc2 = nn.Linear(512,133) self.dropout = nn.Dropout(0.2) self.conv_bn1 = nn.BatchNorm2d(3,16) self.conv_bn2 = nn.BatchNorm2d(16) self.conv_bn3 = nn.BatchNorm2d(32) self.conv_bn4 = nn.BatchNorm2d(64) self.conv_bn5 = nn.BatchNorm2d(128) self.conv_bn6 = nn.BatchNorm2d(256) def forward(self, x): ## Define forward behavior x = self.pool(self.conv_bn2(F.relu(self.conv1(x)))) x = self.pool(self.conv_bn3(F.relu(self.conv2(x)))) x = self.pool(self.conv_bn4(F.relu(self.conv3(x)))) x = self.pool(self.conv_bn5(F.relu(self.conv4(x)))) x = self.pool(self.conv_bn6(F.relu(self.conv5(x)))) x = x.view(-1,256*7*7) x = self.dropout(x) x = F.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x #-#-# You so NOT have to modify the code below this line. #-#-# # instantiate the CNN model_scratch = Net() # move tensors to GPU if CUDA is available if use_cuda: model_scratch.cuda() model_scratch . Net( (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=12544, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=133, bias=True) (dropout): Dropout(p=0.2) (conv_bn1): BatchNorm2d(3, eps=16, momentum=0.1, affine=True, track_running_stats=True) (conv_bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . Question 4: Outline the steps you took to get to your final CNN architecture and your reasoning at each step. . Answer: . Net( (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=12544, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=133, bias=True) (dropout): Dropout(p=0.2) (conv_bn1): BatchNorm2d(3, eps=16, momentum=0.1, affine=True, track_running_stats=True) (conv_bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv_bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . I have created 5 Convolutional with Batch normalization, Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift) . Max Pooling with Stride = 2 . This CNN has two fully connected layers . applied Relu activation after the first fully connected layer . and finnaly I&#39;ve applied dropout of 0.2 before each fully-connected layer to prevent overfitting . (IMPLEMENTATION) Specify Loss Function and Optimizer . Use the next code cell to specify a loss function and optimizer. Save the chosen loss function as criterion_scratch, and the optimizer as optimizer_scratch below. . import torch.optim as optim import numpy as np ### TODO: select loss function criterion_scratch = nn.CrossEntropyLoss() ### TODO: select optimizer optimizer_scratch = torch.optim.Adam(model_scratch.parameters(),lr=0.001) . (IMPLEMENTATION) Train and Validate the Model . Train and validate your model in the code cell below. Save the final model parameters at filepath &#39;model_scratch.pt&#39;. . def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path): &quot;&quot;&quot;returns trained model&quot;&quot;&quot; # initialize tracker for minimum validation loss valid_loss_min = np.Inf for epoch in tqdm(range(1, n_epochs+1)): # initialize variables to monitor training and validation loss train_loss = 0.0 valid_loss = 0.0 ################### # train the model # ################### model.train() for batch_idx, (data, target) in enumerate(loaders[&#39;train&#39;]): # move to GPU if use_cuda: data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output,target) loss.backward() optimizer.step() ## find the loss and update the model parameters accordingly ## record the average training loss, using something like train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss)) ###################### # validate the model # ###################### model.eval() for batch_idx, (data, target) in enumerate(loaders[&#39;valid&#39;]): # move to GPU if use_cuda: data, target = data.cuda(), target.cuda() ## update the average validation loss output = model(data) loss = criterion(output,target) valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss)) print(&#39;Epoch: {} tTraining Loss: {:.6f} tValidation Loss: {:.6f}&#39;.format( epoch, train_loss, valid_loss )) ## TODO: save the model if validation loss has decreased if valid_loss &lt; valid_loss_min: print(&#39;Validation loss decreased from ({:.6f} --&gt; {:.6f}). Saving model ...&#39;.format( valid_loss_min, valid_loss)) torch.save(model.state_dict(),save_path) valid_loss_min = valid_loss # return trained model return model # train the model model_scratch = train(15, loaders_scratch, model_scratch, optimizer_scratch, criterion_scratch, use_cuda, &#39;model_scratch.pt&#39;) # load the model that got the best validation accuracy model_scratch.load_state_dict(torch.load(&#39;model_scratch.pt&#39;)) . 0%| | 0/15 [00:00&lt;?, ?it/s] . Epoch: 1 Training Loss: 4.772083 Validation Loss: 4.608769 Validation loss decreased from (inf --&gt; 4.608769). Saving model ... . 13%|█▎ | 2/15 [03:26&lt;23:01, 106.29s/it] . Epoch: 2 Training Loss: 4.437562 Validation Loss: 4.344522 Validation loss decreased from (4.608769 --&gt; 4.344522). Saving model ... . 20%|██ | 3/15 [04:59&lt;20:27, 102.30s/it] . Epoch: 3 Training Loss: 4.249794 Validation Loss: 4.367167 . 27%|██▋ | 4/15 [06:31&lt;18:10, 99.13s/it] . Epoch: 4 Training Loss: 4.119568 Validation Loss: 4.234560 Validation loss decreased from (4.344522 --&gt; 4.234560). Saving model ... . 33%|███▎ | 5/15 [08:03&lt;16:10, 97.04s/it] . Epoch: 5 Training Loss: 3.998456 Validation Loss: 3.963133 Validation loss decreased from (4.234560 --&gt; 3.963133). Saving model ... . 40%|████ | 6/15 [09:35&lt;14:19, 95.48s/it] . Epoch: 6 Training Loss: 3.831763 Validation Loss: 3.876439 Validation loss decreased from (3.963133 --&gt; 3.876439). Saving model ... . 47%|████▋ | 7/15 [11:07&lt;12:35, 94.49s/it] . Epoch: 7 Training Loss: 3.698276 Validation Loss: 3.844963 Validation loss decreased from (3.876439 --&gt; 3.844963). Saving model ... . 53%|█████▎ | 8/15 [12:40&lt;10:59, 94.15s/it] . Epoch: 8 Training Loss: 3.571749 Validation Loss: 3.689959 Validation loss decreased from (3.844963 --&gt; 3.689959). Saving model ... . 60%|██████ | 9/15 [14:18&lt;09:32, 95.38s/it] . Epoch: 9 Training Loss: 3.417975 Validation Loss: 3.632160 Validation loss decreased from (3.689959 --&gt; 3.632160). Saving model ... . 67%|██████▋ | 10/15 [15:55&lt;07:58, 95.67s/it] . Epoch: 10 Training Loss: 3.296779 Validation Loss: 3.536463 Validation loss decreased from (3.632160 --&gt; 3.536463). Saving model ... . 73%|███████▎ | 11/15 [17:30&lt;06:22, 95.56s/it] . Epoch: 11 Training Loss: 3.125554 Validation Loss: 3.498873 Validation loss decreased from (3.536463 --&gt; 3.498873). Saving model ... . 80%|████████ | 12/15 [19:05&lt;04:46, 95.46s/it] . Epoch: 12 Training Loss: 2.994695 Validation Loss: 3.483631 Validation loss decreased from (3.498873 --&gt; 3.483631). Saving model ... . 87%|████████▋ | 13/15 [20:41&lt;03:10, 95.50s/it] . Epoch: 13 Training Loss: 2.844416 Validation Loss: 3.363084 Validation loss decreased from (3.483631 --&gt; 3.363084). Saving model ... . 93%|█████████▎| 14/15 [22:16&lt;01:35, 95.51s/it] . Epoch: 14 Training Loss: 2.655429 Validation Loss: 3.359952 Validation loss decreased from (3.363084 --&gt; 3.359952). Saving model ... . 100%|██████████| 15/15 [23:50&lt;00:00, 94.95s/it] . Epoch: 15 Training Loss: 2.511357 Validation Loss: 3.425326 . . (IMPLEMENTATION) Test the Model . Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy. Ensure that your test accuracy is greater than 10%. . def test(loaders, model, criterion, use_cuda): # monitor test loss and accuracy test_loss = 0. correct = 0. total = 0. model.eval() for batch_idx, (data, target) in enumerate(loaders[&#39;test&#39;]): # move to GPU if use_cuda: data, target = data.cuda(), target.cuda() # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update average test loss test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss)) # convert output probabilities to predicted class pred = output.data.max(1, keepdim=True)[1] # compare predictions to true label correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy()) total += data.size(0) print(&#39;Test Loss: {:.6f} n&#39;.format(test_loss)) print(&#39; nTest Accuracy: %2d%% (%2d/%2d)&#39; % ( 100. * correct / total, correct, total)) # call test function test(loaders_scratch, model_scratch, criterion_scratch, use_cuda) . Test Loss: 3.431892 Test Accuracy: 20% (174/836) . . . Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning) . You will now use transfer learning to create a CNN that can identify dog breed from images. Your CNN must attain at least 60% accuracy on the test set. . (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset . Use the code cell below to write three separate data loaders for the training, validation, and test datasets of dog images (located at dogImages/train, dogImages/valid, and dogImages/test, respectively). . If you like, you are welcome to use the same data loaders from the previous step, when you created a CNN from scratch. . ## TODO: Specify data loaders num_workers = 0 batch_size = 128 normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) data_transfer = datasets.ImageFolder(&#39;/data/dog_images/train/&#39;, transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(), transforms.RandomRotation(10), transforms.ToTensor(), normalize, ])) data_transfer = { &#39;train&#39; : data_transfer} train_loader = torch.utils.data.DataLoader(data_transfer[&#39;train&#39;], batch_size=batch_size, num_workers=num_workers, shuffle=True) valid_loader = torch.utils.data.DataLoader( datasets.ImageFolder(&#39;/data/dog_images/valid&#39;, transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ])), batch_size=batch_size, num_workers=num_workers, shuffle=True) test_loader = torch.utils.data.DataLoader( datasets.ImageFolder(&#39;/data/dog_images/test&#39;, transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ])), batch_size=batch_size, num_workers=num_workers, shuffle=True) loaders_transfer = {&#39;train&#39;: train_loader, &#39;test&#39;: test_loader, &#39;valid&#39; : valid_loader} . (IMPLEMENTATION) Model Architecture . Use transfer learning to create a CNN to classify dog breed. Use the code cell below, and save your initialized model as the variable model_transfer. . import torchvision.models as models import torch.nn as nn ## TODO: Specify model architecture model_transfer = models.vgg16(pretrained=True) for param in model_transfer.features.parameters(): param.requires_grad = False n_inputs = model_transfer.classifier[6].in_features last_layer = nn.Linear(n_inputs, 133) model_transfer.classifier[6] = last_layer if use_cuda: model_transfer = model_transfer.cuda() model_transfer . VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=133, bias=True) ) ) . Question 5: Outline the steps you took to get to your final CNN architecture and your reasoning at each step. Describe why you think the architecture is suitable for the current problem. . Answer: . VVG was trained on all different images which includes different types of dogs . I have made changes to classification module to match the classes in our dataset (133) . In the final Fully connected layer like doen in the class . (IMPLEMENTATION) Specify Loss Function and Optimizer . Use the next code cell to specify a loss function and optimizer. Save the chosen loss function as criterion_transfer, and the optimizer as optimizer_transfer below. . criterion_transfer = nn.CrossEntropyLoss() optimizer_transfer = optim.Adam(model_transfer.classifier.parameters(), lr=0.001) . (IMPLEMENTATION) Train and Validate the Model . Train and validate your model in the code cell below. Save the final model parameters at filepath &#39;model_transfer.pt&#39;. . # train the model model_transfer = train(15, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, &#39;model_transfer.pt&#39;) # load the model that got the best validation accuracy (uncomment the line below) model_transfer.load_state_dict(torch.load(&#39;model_transfer.pt&#39;)) . 0%| | 0/15 [00:00&lt;?, ?it/s] . Epoch: 1 Training Loss: 2.486129 Validation Loss: 1.075211 Validation loss decreased from (inf --&gt; 1.075211). Saving model ... . 7%|▋ | 1/15 [02:23&lt;33:30, 143.62s/it] . Epoch: 2 Training Loss: 1.489819 Validation Loss: 1.038838 Validation loss decreased from (1.075211 --&gt; 1.038838). Saving model ... . 13%|█▎ | 2/15 [04:47&lt;31:08, 143.71s/it] . Epoch: 3 Training Loss: 1.368035 Validation Loss: 0.880178 Validation loss decreased from (1.038838 --&gt; 0.880178). Saving model ... . 27%|██▋ | 4/15 [09:30&lt;26:06, 142.38s/it] . Epoch: 4 Training Loss: 1.343254 Validation Loss: 0.888856 . 33%|███▎ | 5/15 [11:49&lt;23:35, 141.54s/it] . Epoch: 5 Training Loss: 1.283538 Validation Loss: 0.971985 . 40%|████ | 6/15 [14:13&lt;21:19, 142.21s/it] . Epoch: 6 Training Loss: 1.330055 Validation Loss: 0.959158 . 47%|████▋ | 7/15 [16:37&lt;19:01, 142.71s/it] . Epoch: 7 Training Loss: 1.254037 Validation Loss: 1.023658 . 53%|█████▎ | 8/15 [18:59&lt;16:37, 142.46s/it] . Epoch: 8 Training Loss: 1.320102 Validation Loss: 0.917111 . 60%|██████ | 9/15 [21:22&lt;14:15, 142.62s/it] . Epoch: 9 Training Loss: 1.410277 Validation Loss: 1.020767 . 67%|██████▋ | 10/15 [23:44&lt;11:52, 142.44s/it] . Epoch: 10 Training Loss: 1.328122 Validation Loss: 0.913830 . 73%|███████▎ | 11/15 [26:08&lt;09:31, 142.91s/it] . Epoch: 11 Training Loss: 1.329343 Validation Loss: 0.905521 Epoch: 12 Training Loss: 1.332359 Validation Loss: 0.849628 Validation loss decreased from (0.880178 --&gt; 0.849628). Saving model ... . 87%|████████▋ | 13/15 [30:56&lt;04:46, 143.32s/it] . Epoch: 13 Training Loss: 1.231041 Validation Loss: 0.879702 . 93%|█████████▎| 14/15 [33:15&lt;02:22, 142.09s/it] . Epoch: 14 Training Loss: 1.269037 Validation Loss: 1.037532 . 100%|██████████| 15/15 [35:34&lt;00:00, 141.19s/it] . Epoch: 15 Training Loss: 1.359722 Validation Loss: 1.035099 . . (IMPLEMENTATION) Test the Model . Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy. Ensure that your test accuracy is greater than 60%. . test(loaders_transfer, model_transfer, criterion_transfer, use_cuda) . Test Loss: 1.046745 Test Accuracy: 74% (625/836) . (IMPLEMENTATION) Predict Dog Breed with the Model . Write a function that takes an image path as input and returns the dog breed (Affenpinscher, Afghan hound, etc) that is predicted by your model. . ### TODO: Write a function that takes a path to an image as input ### and returns the dog breed that is predicted by the model. # list of class names by index, i.e. a name can be accessed like class_names[0] class_names = [item[4:].replace(&quot;_&quot;, &quot; &quot;) for item in data_transfer[&#39;train&#39;].classes] # Load the trained model &#39;model_transfer.pt&#39; model_transfer.load_state_dict(torch.load(&#39;model_transfer.pt&#39;, map_location=&#39;cpu&#39;)) def predict_breed_transfer(img_path): # load the image and return the predicted breed image = Image.open(img_path).convert(&#39;RGB&#39;) prediction_transform = transforms.Compose([transforms.Resize(size=(224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # discard the transparent, alpha channel (that&#39;s the :3) and add the batch dimension image = prediction_transform(image)[:3,:,:].unsqueeze(0) image = image.cuda() model_transfer.eval() index = torch.argmax(model_transfer(image)) return class_names[index] . . . Step 5: Write your Algorithm . Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither. Then, . if a dog is detected in the image, return the predicted breed. | if a human is detected in the image, return the resembling dog breed. | if neither is detected in the image, provide output that indicates an error. | . You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the face_detector and human_detector functions developed above. You are required to use your CNN from Step 4 to predict dog breed. . Some sample output for our algorithm is provided below, but feel free to design your own user experience! . . (IMPLEMENTATION) Write your Algorithm . ### TODO: Write your algorithm. ### Feel free to use as many code cells as needed. def run_app(img_path): ## handle cases for a human face, dog, and neither if dog_detector(img_path) is True: breed = predict_breed_transfer(img_path) print(&quot;Hello Dog&quot;) img = Image.open(img_path) plt.imshow(img) plt.show() print(&quot; Your breed is {} n&quot;.format(breed)) elif face_detector(img_path) &gt; 0: breed = predict_breed_transfer(img_path) print(&quot;Hello Human&quot;) img = Image.open(img_path) plt.imshow(img) plt.show() print(&quot;You look like a {0} n&quot;.format(breed)) else: print(&quot;Not a Human or Dog n&quot;) . . . Step 6: Test Your Algorithm . In this section, you will take your new algorithm for a spin! What kind of dog does the algorithm think that you look like? If you have a dog, does it predict your dog&#39;s breed accurately? If you have a cat, does it mistakenly think that your cat is a dog? . (IMPLEMENTATION) Test Your Algorithm on Sample Images! . Test your algorithm at least six images on your computer. Feel free to use any images you like. Use at least two human and two dog images. . Question 6: Is the output better than you expected :) ? Or worse :( ? Provide at least three possible points of improvement for your algorithm. . Answer: My output is better then i expected : ) . (Three possible points for improvement) . Using more training data . Fine tuning the model to get a better accuracy . Using different optimizers and loss functions . More Augmentation during training(rotating and fliping images) . ## TODO: Execute your algorithm from Step 6 on ## at least 6 images on your computer. ## Feel free to use as many code cells as needed. ## suggested code, below for file in np.hstack((human_files[:3], dog_files[:3])): run_app(file) . Hello Human . You look like a Manchester terrier Hello Human . You look like a Manchester terrier Hello Human . You look like a Dachshund Hello Dog . Your breed is Bullmastiff Hello Dog . Your breed is Mastiff Hello Dog . Your breed is Bullmastiff . My Images . run_app(&#39;./my_images/dog2.jpg&#39;) . Hello Dog . Your breed is Nova scotia duck tolling retriever . run_app(&#39;./my_images/mygit.jpeg&#39;) . Hello Human . You look like a Manchester terrier . run_app(&#39;./my_images/cat.jpeg&#39;) . Not a Human or Dog . run_app(&#39;./my_images/dog.jpg&#39;) . Hello Dog . Your breed is Dachshund . run_app(&#39;./my_images/mypic.jpeg&#39;) . Hello Human . You look like a Doberman pinscher . run_app(&#39;./my_images/dog3.png&#39;) . Hello Dog . Your breed is Golden retriever .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/29/dog_app.html",
            "relUrl": "/2020/05/29/dog_app.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Transfer Learning",
            "content": "Flower power . Here we&#39;ll be using VGGNet to classify images of flowers. We&#39;ll start, as usual, by importing our usual resources. And checking if we can train our model on GPU. . Download Data . The flower data is available in a zip file in this lesson&#39;s resources, for download to your local environment. In the case of this notebook, the data is already downloaded and in the directory flower_photos/. . import os import numpy as np import torch import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt %matplotlib inline . # check if CUDA is available train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;CUDA is not available. Training on CPU ...&#39;) else: print(&#39;CUDA is available! Training on GPU ...&#39;) . CUDA is available! Training on GPU ... . Load and Transform our Data . We&#39;ll be using PyTorch&#39;s ImageFolder class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this: . root/class_1/xxx.png root/class_1/xxy.png root/class_1/xxz.png root/class_2/123.png root/class_2/nsdf3.png root/class_2/asd932_.png . Where, in this case, the root folder for training is flower_photos/train/ and the classes are the names of flower types. . # define training and test data directories data_dir = &#39;flower_photos/&#39; train_dir = os.path.join(data_dir, &#39;train/&#39;) test_dir = os.path.join(data_dir, &#39;test/&#39;) # classes are folders in each directory with these names classes = [&#39;daisy&#39;, &#39;dandelion&#39;, &#39;roses&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;] . Transforming the Data . When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects 224-dim square images as input and so, we resize each flower image to fit this mold. . # load and transform data using ImageFolder # VGG-16 Takes 224x224 images as input, so we resize all of them data_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.ToTensor()]) train_data = datasets.ImageFolder(train_dir, transform=data_transform) test_data = datasets.ImageFolder(test_dir, transform=data_transform) # print out some data stats print(&#39;Num training images: &#39;, len(train_data)) print(&#39;Num test images: &#39;, len(test_data)) . Num training images: 3130 Num test images: 540 . DataLoaders and Data Visualization . # define dataloader parameters batch_size = 20 num_workers=0 # prepare data loaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True) . # Visualize some sample data # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # convert images to numpy for display # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) plt.imshow(np.transpose(images[idx], (1, 2, 0))) ax.set_title(classes[labels[idx]]) . . Define the Model . To define a model for training we&#39;ll follow these steps: . Load in a pre-trained VGG16 model | &quot;Freeze&quot; all the parameters, so the net acts as a fixed feature extractor | Remove the last layer | Replace the last layer with a linear classifier of our own | Freezing simply means that the parameters in the pre-trained model will not change during training. . # Load the pretrained model from pytorch vgg16 = models.vgg16(pretrained=True) # print out the model structure print(vgg16) . VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) . print(vgg16.classifier[6].in_features) print(vgg16.classifier[6].out_features) . 4096 1000 . # Freeze training for all &quot;features&quot; layers for param in vgg16.features.parameters(): param.requires_grad = False . . Final Classifier Layer . Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you repace the last layer in the vgg classifier group of layers. . This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task. . You can access any layer in a pretrained network by name and (sometimes) number, i.e. vgg16.classifier[6] is the sixth layer in a group of layers named &quot;classifier&quot;. . TODO:Replace the last fully-connected layer with one that produces the appropriate number of class scores. . ## TODO: add a last linear layer that maps n_inputs -&gt; 5 flower classes ## new layers automatically have requires_grad = True import torch.nn as nn n_inputs = vgg16.classifier[6].in_features last_layer = nn.Linear(n_inputs,len(classes)) vgg16.classifier[6] = last_layer # after completing your model, if GPU is available, move the model to GPU if train_on_gpu: vgg16.cuda() vgg16.classifier[6] . Linear(in_features=4096, out_features=5, bias=True) . Specify Loss Function and Optimizer . Below we&#39;ll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input only the trainable parameters vgg.classifier.parameters(). . import torch.optim as optim # specify loss function (categorical cross-entropy) criterion = nn.CrossEntropyLoss() # specify optimizer (stochastic gradient descent) and learning rate = 0.001 optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001) . . Training . Here, we&#39;ll train the network. . Exercise: So far we&#39;ve been providing the training code for you. Here, I&#39;m going to give you a bit more of a challenge and have you write the code to train the network. Of course, you&#39;ll be able to see my solution if you need help. . # number of epochs to train the model n_epochs = 3 for epoch in range(1, n_epochs+1): # keep track of training and validation loss train_loss = 0.0 ################### # train the model # ################### # model by default is set to train for batch_i, (data, target) in enumerate(train_loader): # move tensors to GPU if CUDA is available if train_on_gpu: data, target = data.cuda(), target.cuda() # clear the gradients of all optimized variables optimizer.zero_grad() # forward pass: compute predicted outputs by passing inputs to the model output = vgg16(data) # calculate the batch loss loss = criterion(output, target) # backward pass: compute gradient of the loss with respect to model parameters loss.backward() # perform a single optimization step (parameter update) optimizer.step() # update training loss train_loss += loss.item() if batch_i % 20 == 19: # print training loss every specified number of mini-batches print(&#39;Epoch %d, Batch %d loss: %.16f&#39; % (epoch, batch_i + 1, train_loss / 20)) train_loss = 0.0 . Epoch 1, Batch 20 loss: 0.7218011558055878 Epoch 1, Batch 40 loss: 0.6632505908608437 Epoch 1, Batch 60 loss: 0.6580562725663185 Epoch 1, Batch 80 loss: 0.6966450020670891 Epoch 1, Batch 100 loss: 0.6487508699297905 Epoch 1, Batch 120 loss: 0.5942393168807030 Epoch 1, Batch 140 loss: 0.6003635406494141 Epoch 2, Batch 20 loss: 0.6820171013474464 Epoch 2, Batch 40 loss: 0.5651205480098724 Epoch 2, Batch 60 loss: 0.5982387349009514 Epoch 2, Batch 80 loss: 0.6106431797146797 Epoch 2, Batch 100 loss: 0.5735929340124131 Epoch 2, Batch 120 loss: 0.5644627243280411 Epoch 2, Batch 140 loss: 0.5108004912734032 Epoch 3, Batch 20 loss: 0.5650060355663300 Epoch 3, Batch 40 loss: 0.5508088886737823 Epoch 3, Batch 60 loss: 0.5049202263355255 Epoch 3, Batch 80 loss: 0.5447822645306587 Epoch 3, Batch 100 loss: 0.5443841889500618 Epoch 3, Batch 120 loss: 0.5433818690478802 Epoch 3, Batch 140 loss: 0.5302807942032814 . . Testing . Below you see the test accuracy for each flower class. . # track test loss # over 5 flower classes test_loss = 0.0 class_correct = list(0. for i in range(5)) class_total = list(0. for i in range(5)) vgg16.eval() # eval mode # iterate over test data for data, target in test_loader: # move tensors to GPU if CUDA is available if train_on_gpu: data, target = data.cuda(), target.cuda() # forward pass: compute predicted outputs by passing inputs to the model output = vgg16(data) # calculate the batch loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct_tensor = pred.eq(target.data.view_as(pred)) correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy()) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f} n&#39;.format(test_loss)) for i in range(5): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39; nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) . Test Loss: 0.601322 Test Accuracy of daisy: 73% (68/92) Test Accuracy of dandelion: 89% (118/132) Test Accuracy of roses: 70% (64/91) Test Accuracy of sunflowers: 83% (84/101) Test Accuracy of tulips: 72% (90/124) Test Accuracy (Overall): 78% (424/540) . Visualize Sample Test Results . # obtain one batch of test images dataiter = iter(test_loader) images, labels = dataiter.next() images.numpy() # move model inputs to cuda, if GPU available if train_on_gpu: images = images.cuda() # get sample outputs output = vgg16(images) # convert output probabilities to predicted class _, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy()) # plot the images in the batch, along with predicted and true labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) plt.imshow(np.transpose(images[idx], (1, 2, 0))) ax.set_title(&quot;{} ({})&quot;.format(classes[preds[idx]], classes[labels[idx]]), color=(&quot;green&quot; if preds[idx]==labels[idx].item() else &quot;red&quot;)) .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/25/Transfer_Learning.html",
            "relUrl": "/2020/05/25/Transfer_Learning.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Multi-Layer Perceptron, MNIST",
            "content": "# import libraries import torch import numpy as np . . Load and Visualize the Data . Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the batch_size if you want to load more data at a time. . This cell will create DataLoaders for each of our datasets. . from torchvision import datasets import torchvision.transforms as transforms # number of subprocesses to use for data loading num_workers = 0 # how many samples per batch to load batch_size = 20 # convert data to torch.FloatTensor transform = transforms.ToTensor() # choose the training and test datasets train_data = datasets.MNIST(root=&#39;data&#39;, train=True, download=True, transform=transform) test_data = datasets.MNIST(root=&#39;data&#39;, train=False, download=True, transform=transform) # prepare data loaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) . Visualize a Batch of Training Data . The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data. . import matplotlib.pyplot as plt %matplotlib inline # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[idx]), cmap=&#39;gray&#39;) # print out the correct label for each image # .item() gets the value contained in a Tensor ax.set_title(str(labels[idx].item())) . View an Image in More Detail . img = np.squeeze(images[1]) fig = plt.figure(figsize = (12,12)) ax = fig.add_subplot(111) ax.imshow(img, cmap=&#39;gray&#39;) width, height = img.shape thresh = img.max()/2.5 for x in range(width): for y in range(height): val = round(img[x][y],2) if img[x][y] !=0 else 0 ax.annotate(str(val), xy=(y,x), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, color=&#39;white&#39; if img[x][y]&lt;thresh else &#39;black&#39;) . . Define the Network Architecture . The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting. . import torch.nn as nn import torch.nn.functional as F # define the NN architecture class Net(nn.Module): def __init__(self): super(Net, self).__init__() # number of hidden nodes in each layer (512) hidden_1 = 512 hidden_2 = 512 # linear layer (784 -&gt; hidden_1) self.fc1 = nn.Linear(28 * 28, hidden_1) # linear layer (n_hidden -&gt; hidden_2) self.fc2 = nn.Linear(hidden_1, hidden_2) # linear layer (n_hidden -&gt; 10) self.fc3 = nn.Linear(hidden_2, 10) # dropout layer (p=0.2) # dropout prevents overfitting of data self.dropout = nn.Dropout(0.2) def forward(self, x): # flatten image input x = x.view(-1, 28 * 28) # add hidden layer, with relu activation function x = F.relu(self.fc1(x)) # add dropout layer x = self.dropout(x) # add hidden layer, with relu activation function x = F.relu(self.fc2(x)) # add dropout layer x = self.dropout(x) # add output layer x = self.fc3(x) return x # initialize the NN model = Net() print(model) . Net( (fc1): Linear(in_features=784, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=512, bias=True) (fc3): Linear(in_features=512, out_features=10, bias=True) (dropout): Dropout(p=0.2, inplace=False) ) . Specify Loss Function and Optimizer . It&#39;s recommended that you use cross-entropy loss for classification. If you look at the documentation (linked above), you can see that PyTorch&#39;s cross entropy function applies a softmax funtion to the output layer and then calculates the log loss. . ## TODO: Specify loss and optimization functions # specify loss function criterion = nn.CrossEntropyLoss() # specify optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01) . . Train the Network . The steps for training/learning from a batch of data are described in the comments below: . Clear the gradients of all optimized variables | Forward pass: compute predicted outputs by passing inputs to the model | Calculate the loss | Backward pass: compute gradient of the loss with respect to model parameters | Perform a single optimization step (parameter update) | Update average training loss | The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. . # number of epochs to train the model n_epochs = 30 model.train() # prep model for training for epoch in range(n_epochs): # monitor training loss train_loss = 0.0 ################### # train the model # ################### for data, target in train_loader: # clear the gradients of all optimized variables optimizer.zero_grad() # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # backward pass: compute gradient of the loss with respect to model parameters loss.backward() # perform a single optimization step (parameter update) optimizer.step() # update running training loss train_loss += loss.item()*data.size(0) # print training statistics # calculate average loss over an epoch train_loss = train_loss/len(train_loader.sampler) print(&#39;Epoch: {} tTraining Loss: {:.6f}&#39;.format( epoch+1, train_loss )) . Epoch: 1 Training Loss: 0.205456 Epoch: 2 Training Loss: 0.172935 Epoch: 3 Training Loss: 0.148984 Epoch: 4 Training Loss: 0.131135 Epoch: 5 Training Loss: 0.115686 Epoch: 6 Training Loss: 0.105010 Epoch: 7 Training Loss: 0.095415 Epoch: 8 Training Loss: 0.087881 Epoch: 9 Training Loss: 0.079777 Epoch: 10 Training Loss: 0.073888 Epoch: 11 Training Loss: 0.069917 Epoch: 12 Training Loss: 0.064001 Epoch: 13 Training Loss: 0.059430 Epoch: 14 Training Loss: 0.056488 Epoch: 15 Training Loss: 0.051418 Epoch: 16 Training Loss: 0.049550 Epoch: 17 Training Loss: 0.045766 Epoch: 18 Training Loss: 0.043260 Epoch: 19 Training Loss: 0.041252 Epoch: 20 Training Loss: 0.038824 Epoch: 21 Training Loss: 0.037077 Epoch: 22 Training Loss: 0.034818 Epoch: 23 Training Loss: 0.032878 Epoch: 24 Training Loss: 0.031076 Epoch: 25 Training Loss: 0.030160 Epoch: 26 Training Loss: 0.027836 Epoch: 27 Training Loss: 0.027004 Epoch: 28 Training Loss: 0.025021 Epoch: 29 Training Loss: 0.023869 Epoch: 30 Training Loss: 0.023172 . . Test the Trained Network . Finally, we test our best model on previously unseen test data and evaluate it&#39;s performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy. . model.eval() . model.eval() will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn &quot;off&quot; nodes during training with some probability, but should allow every node to be &quot;on&quot; for evaluation! . # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for evaluation for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(len(target)): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.sampler) print(&#39;Test Loss: {:.6f} n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39; nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) . Test Loss: 0.056348 Test Accuracy of 0: 98% (970/980) Test Accuracy of 1: 99% (1127/1135) Test Accuracy of 2: 98% (1016/1032) Test Accuracy of 3: 97% (989/1010) Test Accuracy of 4: 98% (967/982) Test Accuracy of 5: 98% (878/892) Test Accuracy of 6: 98% (940/958) Test Accuracy of 7: 97% (1002/1028) Test Accuracy of 8: 97% (947/974) Test Accuracy of 9: 98% (989/1009) Test Accuracy (Overall): 98% (9825/10000) . Visualize Sample Test Results . This cell displays test images and their labels in this format: predicted (ground-truth). The text will be green for accurately classified examples and red for incorrect predictions. . # obtain one batch of test images dataiter = iter(test_loader) images, labels = dataiter.next() # get sample outputs output = model(images) # convert output probabilities to predicted class _, preds = torch.max(output, 1) # prep images for display images = images.numpy() # plot the images in the batch, along with predicted and true labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[idx]), cmap=&#39;gray&#39;) ax.set_title(&quot;{} ({})&quot;.format(str(preds[idx].item()), str(labels[idx].item())), color=(&quot;green&quot; if preds[idx]==labels[idx] else &quot;red&quot;)) .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/mnist_mlp.html",
            "relUrl": "/2020/05/16/mnist_mlp.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Transfer Learning",
            "content": "%matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt import torch from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models . Most of the pretrained models require the input to be 224x224 images. Also, we&#39;ll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are [0.485, 0.456, 0.406] and the standard deviations are [0.229, 0.224, 0.225]. . data_dir = &#39;Cat_Dog_data&#39; # TODO: Define transforms for the training data and testing data train_transforms = transforms.Compose([transforms.RandomRotation(30), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) test_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) # Pass transforms in here, then run the next cell to see how the transforms look train_data = datasets.ImageFolder(data_dir + &#39;/train&#39;, transform=train_transforms) test_data = datasets.ImageFolder(data_dir + &#39;/test&#39;, transform=test_transforms) trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True) testloader = torch.utils.data.DataLoader(test_data, batch_size=64) . We can load in a model such as DenseNet. Let&#39;s print out the model architecture so we can see what&#39;s going on. . model = models.densenet121(pretrained=True) model . This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer (classifier): Linear(in_features=1024, out_features=1000). This layer was trained on the ImageNet dataset, so it won&#39;t work for our specific problem. That means we need to replace the classifier, but the features will work perfectly on their own. In general, I think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers. . # Freeze parameters so we don&#39;t backprop through them for param in model.parameters(): param.requires_grad = False from collections import OrderedDict classifier = nn.Sequential(OrderedDict([ (&#39;fc1&#39;, nn.Linear(1024, 500)), (&#39;relu&#39;, nn.ReLU()), (&#39;fc2&#39;, nn.Linear(500, 2)), (&#39;output&#39;, nn.LogSoftmax(dim=1)) ])) model.classifier = classifier . With our model built, we need to train the classifier. However, now we&#39;re using a really deep neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we&#39;re going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It&#39;s also possible to train on multiple GPUs, further decreasing training time. . PyTorch, along with pretty much every other deep learning framework, uses CUDA to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using model.to(&#39;cuda&#39;). You can move them back from the GPU with model.to(&#39;cpu&#39;) which you&#39;ll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, I&#39;ll compare how long it takes to perform a forward and backward pass with and without a GPU. . import time . for device in [&#39;cpu&#39;, &#39;cuda&#39;]: criterion = nn.NLLLoss() # Only train the classifier parameters, feature parameters are frozen optimizer = optim.Adam(model.classifier.parameters(), lr=0.001) model.to(device) for ii, (inputs, labels) in enumerate(trainloader): # Move input and label tensors to the GPU inputs, labels = inputs.to(device), labels.to(device) start = time.time() outputs = model.forward(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if ii==3: break print(f&quot;Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds&quot;) . You can write device agnostic code which will automatically use CUDA if it&#39;s enabled like so: . # at beginning of the script device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) ... # then whenever you get a new Tensor or Module # this won&#39;t copy if they are already on the desired device input = data.to(device) model = MyModule(...).to(device) . From here, I&#39;ll let you finish training the model. The process is the same as before except now your model is much more powerful. You should get better than 95% accuracy easily. . Exercise: Train a pretrained models to classify the cat and dog images. Continue with the DenseNet model, or try ResNet, it&#39;s also a good model to try out first. Make sure you are only training the classifier and the parameters for the features part are frozen. . # Use GPU if it&#39;s available device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = models.densenet121(pretrained=True) # Freeze parameters so we don&#39;t backprop through them for param in model.parameters(): param.requires_grad = False model.classifier = nn.Sequential(nn.Linear(1024, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256, 2), nn.LogSoftmax(dim=1)) criterion = nn.NLLLoss() # Only train the classifier parameters, feature parameters are frozen optimizer = optim.Adam(model.classifier.parameters(), lr=0.003) model.to(device); . epochs = 1 steps = 0 running_loss = 0 print_every = 5 for epoch in range(epochs): for inputs, labels in trainloader: steps += 1 # Move input and label tensors to the default device inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() logps = model.forward(inputs) loss = criterion(logps, labels) loss.backward() optimizer.step() running_loss += loss.item() if steps % print_every == 0: test_loss = 0 accuracy = 0 model.eval() with torch.no_grad(): for inputs, labels in testloader: inputs, labels = inputs.to(device), labels.to(device) logps = model.forward(inputs) batch_loss = criterion(logps, labels) test_loss += batch_loss.item() # Calculate accuracy ps = torch.exp(logps) top_p, top_class = ps.topk(1, dim=1) equals = top_class == labels.view(*top_class.shape) accuracy += torch.mean(equals.type(torch.FloatTensor)).item() print(f&quot;Epoch {epoch+1}/{epochs}.. &quot; f&quot;Train loss: {running_loss/print_every:.3f}.. &quot; f&quot;Test loss: {test_loss/len(testloader):.3f}.. &quot; f&quot;Test accuracy: {accuracy/len(testloader):.3f}&quot;) running_loss = 0 model.train() .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part8-TransferLearning.html",
            "relUrl": "/2020/05/16/Part8-TransferLearning.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Loading Image Data",
            "content": "%matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt import torch from torchvision import datasets, transforms import helper . The easiest way to load image data is with datasets.ImageFolder from torchvision (documentation). In general you&#39;ll use ImageFolder like so: . dataset = datasets.ImageFolder(&#39;path/to/data&#39;, transform=transform) . where &#39;path/to/data&#39; is the file path to the data directory and transform is a sequence of processing steps built with the transforms module from torchvision. ImageFolder expects the files and directories to be constructed like so: . root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png . where each class has it&#39;s own directory (cat and dog) for the images. The images are then labeled with the class taken from the directory name. So here, the image 123.png would be loaded with the class label cat. You can download the dataset already structured like this from here. I&#39;ve also split it into a training set and test set. . Transforms . When you load in the data with ImageFolder, you&#39;ll need to define some transforms. For example, the images are different sizes but we&#39;ll need them to all be the same size for training. You can either resize them with transforms.Resize() or crop with transforms.CenterCrop(), transforms.RandomResizedCrop(), etc. We&#39;ll also need to convert the images to PyTorch tensors with transforms.ToTensor(). Typically you&#39;ll combine these transforms into a pipeline with transforms.Compose(), which accepts a list of transforms and runs them in sequence. It looks something like this to scale, then crop, then convert to a tensor: . transform = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]) . There are plenty of transforms available, I&#39;ll cover more in a bit and you can read through the documentation. . Data Loaders . With the ImageFolder loaded, you have to pass it to a DataLoader. The DataLoader takes a dataset (such as you would get from ImageFolder) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch. . dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) . Here dataloader is a generator. To get data out of it, you need to loop through it or convert it to an iterator and call next(). . # Looping through it, get a batch on each loop for images, labels in dataloader: pass # Get one batch images, labels = next(iter(dataloader)) . Exercise: Load images from the Cat_Dog_data/train folder, define a few transforms, then build the dataloader. . data_dir = &#39;Cat_Dog_data/train&#39; transform = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]) dataset = datasets.ImageFolder(data_dir, transform=transform) dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) . # Run this to test your data loader images, labels = next(iter(dataloader)) helper.imshow(images[0], normalize=False) . If you loaded the data correctly, you should see something like this (your image will be different): . . Data Augmentation . A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it&#39;s seeing the same images but in different locations, with different sizes, in different orientations, etc. . To randomly rotate, scale and crop, then flip your images you would define your transforms like this: . train_transforms = transforms.Compose([transforms.RandomRotation(30), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]) . You&#39;ll also typically want to normalize images with transforms.Normalize. You pass in a list of means and list of standard deviations, then the color channels are normalized like so . input[channel] = (input[channel] - mean[channel]) / std[channel] . Subtracting mean centers the data around zero and dividing by std squishes the values to be between -1 and 1. Normalizing helps keep the network weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn. . You can find a list of all the available transforms here. When you&#39;re testing however, you&#39;ll want to use images that aren&#39;t altered other than normalizing. So, for validation/test images, you&#39;ll typically just resize and crop. . Exercise: Define transforms for training data and testing data below. Leave off normalization for now. . data_dir = &#39;Cat_Dog_data&#39; # TODO: Define transforms for the training data and testing data train_transforms = transforms.Compose([transforms.RandomRotation(30), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]) test_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]) # Pass transforms in here, then run the next cell to see how the transforms look train_data = datasets.ImageFolder(data_dir + &#39;/train&#39;, transform=train_transforms) test_data = datasets.ImageFolder(data_dir + &#39;/test&#39;, transform=test_transforms) trainloader = torch.utils.data.DataLoader(train_data, batch_size=32) testloader = torch.utils.data.DataLoader(test_data, batch_size=32) . # change this to the trainloader or testloader data_iter = iter(testloader) images, labels = next(data_iter) fig, axes = plt.subplots(figsize=(10,4), ncols=4) for ii in range(4): ax = axes[ii] helper.imshow(images[ii], ax=ax, normalize=False) . Your transformed images should look something like this. . Training examples: . Testing examples: . At this point you should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won&#39;t get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you&#39;ve seen 28x28 images which are tiny). . In the next part, I&#39;ll show you how to use a pre-trained network to build a model that can actually solve this problem. . # Optional TODO: Attempt to build a network to classify cats vs dogs from this dataset .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part7Loading-ImageData.html",
            "relUrl": "/2020/05/16/Part7Loading-ImageData.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Inference and Validation",
            "content": "import torch from torchvision import datasets, transforms # Define a transform to normalize the data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # Download and load the training data trainset = datasets.FashionMNIST(&#39;~/.pytorch/F_MNIST_data/&#39;, download=True, train=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Download and load the test data testset = datasets.FashionMNIST(&#39;~/.pytorch/F_MNIST_data/&#39;, download=True, train=False, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) . Here I&#39;ll create a model like normal, using the same one from my solution for part 4. . from torch import nn, optim import torch.nn.functional as F class Classifier(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) self.fc4 = nn.Linear(64, 10) def forward(self, x): # make sure input tensor is flattened x = x.view(x.shape[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) x = F.log_softmax(self.fc4(x), dim=1) return x . The goal of validation is to measure the model&#39;s performance on data that isn&#39;t part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are precision and recall) and top-5 error rate. We&#39;ll focus on accuracy here. First I&#39;ll do a forward pass with one batch from the test set. . model = Classifier() images, labels = next(iter(testloader)) # Get the class probabilities ps = torch.exp(model(images)) # Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples print(ps.shape) . With the probabilities, we can get the most likely class using the ps.topk method. This returns the $k$ highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we&#39;ll get back 4 as the index. . top_p, top_class = ps.topk(1, dim=1) # Look at the most likely classes for the first 10 examples print(top_class[:10,:]) . Now we can check if the predicted classes match the labels. This is simple to do by equating top_class and labels, but we have to be careful of the shapes. Here top_class is a 2D tensor with shape (64, 1) while labels is 1D with shape (64). To get the equality to work out the way we want, top_class and labels must have the same shape. . If we do . equals = top_class == labels . equals will have shape (64, 64), try it yourself. What it&#39;s doing is comparing the one element in each row of top_class with each element in labels which returns 64 True/False boolean values for each row. . equals = top_class == labels.view(*top_class.shape) . Now we need to calculate the percentage of correct predictions. equals has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to torch.mean. If only it was that simple. If you try torch.mean(equals), you&#39;ll get an error . RuntimeError: mean is not implemented for type torch.ByteTensor . This happens because equals has type torch.ByteTensor but torch.mean isn&#39;t implement for tensors with that type. So we&#39;ll need to convert equals to a float tensor. Note that when we take torch.mean it returns a scalar tensor, to get the actual value as a float we&#39;ll need to do accuracy.item(). . accuracy = torch.mean(equals.type(torch.FloatTensor)) print(f&#39;Accuracy: {accuracy.item()*100}%&#39;) . The network is untrained so it&#39;s making random guesses and we should see an accuracy around 10%. Now let&#39;s train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we&#39;re not updating our parameters in the validation pass, we can speed up the by turning off gradients using torch.no_grad(): . # turn off gradients with torch.no_grad(): # validation pass here for images, labels in testloader: ... . Exercise: Implement the validation loop below. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you&#39;ll always learn more by typing it rather than copy-pasting. . model = Classifier() criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.003) epochs = 30 steps = 0 train_losses, test_losses = [], [] for e in range(epochs): running_loss = 0 for images, labels in trainloader: optimizer.zero_grad() log_ps = model(images) loss = criterion(log_ps, labels) loss.backward() optimizer.step() running_loss += loss.item() else: test_loss = 0 accuracy = 0 # Turn off gradients for validation, saves memory and computations with torch.no_grad(): for images, labels in testloader: log_ps = model(images) test_loss += criterion(log_ps, labels) ps = torch.exp(log_ps) top_p, top_class = ps.topk(1, dim=1) equals = top_class == labels.view(*top_class.shape) accuracy += torch.mean(equals.type(torch.FloatTensor)) train_losses.append(running_loss/len(trainloader)) test_losses.append(test_loss/len(testloader)) print(&quot;Epoch: {}/{}.. &quot;.format(e+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/len(trainloader)), &quot;Test Loss: {:.3f}.. &quot;.format(test_loss/len(testloader)), &quot;Test Accuracy: {:.3f}&quot;.format(accuracy/len(testloader))) . %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt . plt.plot(train_losses, label=&#39;Training loss&#39;) plt.plot(test_losses, label=&#39;Validation loss&#39;) plt.legend(frameon=False) . &lt;matplotlib.legend.Legend at 0x12260fd68&gt; . Overfitting . If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting. . . The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called early-stopping. In practice, you&#39;d save the model frequently as you&#39;re training then later choose the model with the lowest validation loss. . The most common method to reduce overfitting (outside of early-stopping) is dropout, where we randomly drop input units. This forces the network to share information between weights, increasing it&#39;s ability to generalize to new data. Adding dropout in PyTorch is straightforward using the nn.Dropout module. . class Classifier(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) self.fc4 = nn.Linear(64, 10) # Dropout module with 0.2 drop probability self.dropout = nn.Dropout(p=0.2) def forward(self, x): # make sure input tensor is flattened x = x.view(x.shape[0], -1) # Now with dropout x = self.dropout(F.relu(self.fc1(x))) x = self.dropout(F.relu(self.fc2(x))) x = self.dropout(F.relu(self.fc3(x))) # output so no dropout here x = F.log_softmax(self.fc4(x), dim=1) return x . During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we&#39;re using the network to make predictions. To do this, you use model.eval(). This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with model.train(). In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode. . # turn off gradients with torch.no_grad(): # set model to evaluation mode model.eval() # validation pass here for images, labels in testloader: ... # set model back to train mode model.train() . Exercise: Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss. . class Classifier(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) self.fc4 = nn.Linear(64, 10) # Dropout module with 0.2 drop probability self.dropout = nn.Dropout(p=0.2) def forward(self, x): # make sure input tensor is flattened x = x.view(x.shape[0], -1) # Now with dropout x = self.dropout(F.relu(self.fc1(x))) x = self.dropout(F.relu(self.fc2(x))) x = self.dropout(F.relu(self.fc3(x))) # output so no dropout here x = F.log_softmax(self.fc4(x), dim=1) return x . model = Classifier() criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.003) epochs = 30 steps = 0 train_losses, test_losses = [], [] for e in range(epochs): running_loss = 0 for images, labels in trainloader: optimizer.zero_grad() log_ps = model(images) loss = criterion(log_ps, labels) loss.backward() optimizer.step() running_loss += loss.item() else: test_loss = 0 accuracy = 0 # Turn off gradients for validation, saves memory and computations with torch.no_grad(): model.eval() for images, labels in testloader: log_ps = model(images) test_loss += criterion(log_ps, labels) ps = torch.exp(log_ps) top_p, top_class = ps.topk(1, dim=1) equals = top_class == labels.view(*top_class.shape) accuracy += torch.mean(equals.type(torch.FloatTensor)) model.train() train_losses.append(running_loss/len(trainloader)) test_losses.append(test_loss/len(testloader)) print(&quot;Epoch: {}/{}.. &quot;.format(e+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(train_losses[-1]), &quot;Test Loss: {:.3f}.. &quot;.format(test_losses[-1]), &quot;Test Accuracy: {:.3f}&quot;.format(accuracy/len(testloader))) . %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt . plt.plot(train_losses, label=&#39;Training loss&#39;) plt.plot(test_losses, label=&#39;Validation loss&#39;) plt.legend(frameon=False) . &lt;matplotlib.legend.Legend at 0x1226ff908&gt; . Inference . Now that the model is trained, we can use it for inference. We&#39;ve done this before, but now we need to remember to set the model in inference mode with model.eval(). You&#39;ll also want to turn off autograd with the torch.no_grad() context. . # Import helper module (should be in the repo) import helper # Test out your network! model.eval() dataiter = iter(testloader) images, labels = dataiter.next() img = images[0] # Convert 2D image to 1D vector img = img.view(1, 784) # Calculate the class probabilities (softmax) for img with torch.no_grad(): output = model.forward(img) ps = torch.exp(output) # Plot the image and probabilities helper.view_classify(img.view(1, 28, 28), ps, version=&#39;Fashion&#39;) . Next Up! . In the next part, I&#39;ll show you how to save your trained models. In general, you won&#39;t want to train a model everytime you need it. Instead, you&#39;ll train once, save it, then load the model when you want to train more or use if for inference. .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part5InferenceandValidation.html",
            "relUrl": "/2020/05/16/Part5InferenceandValidation.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Classifying Fashion-MNIST",
            "content": "import torch from torchvision import datasets, transforms import helper # Define a transform to normalize the data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # Download and load the training data trainset = datasets.FashionMNIST(&#39;~/.pytorch/F_MNIST_data/&#39;, download=True, train=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Download and load the test data testset = datasets.FashionMNIST(&#39;~/.pytorch/F_MNIST_data/&#39;, download=True, train=False, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True) . Here we can see one of the images. . image, label = next(iter(trainloader)) helper.imshow(image[0,:]); . Building the network . Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It&#39;s up to you how many layers you add and the size of those layers. . from torch import nn, optim import torch.nn.functional as F . # TODO: Define your network architecture here class Classifier(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) self.fc4 = nn.Linear(64, 10) def forward(self, x): # make sure input tensor is flattened x = x.view(x.shape[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) x = F.log_softmax(self.fc4(x), dim=1) return x . Train the network . Now you should create your network and train it. First you&#39;ll want to define the criterion (something like nn.CrossEntropyLoss or nn.NLLLoss) and the optimizer (typically optim.SGD or optim.Adam). . Then write the training code. Remember the training pass is a fairly straightforward process: . Make a forward pass through the network to get the logits | Use the logits to calculate the loss | Perform a backward pass through the network with loss.backward() to calculate the gradients | Take a step with the optimizer to update the weights | . By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4. . # TODO: Create the network, define the criterion and optimizer model = Classifier() criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.003) . # TODO: Train the network here epochs = 5 for e in range(epochs): running_loss = 0 for images, labels in trainloader: log_ps = model(images) loss = criterion(log_ps, labels) optimizer.zero_grad() loss.backward() optimizer.step() running_loss += loss.item() else: print(f&quot;Training loss: {running_loss/len(trainloader)}&quot;) . Training loss: 283.4510831311345 Training loss: 274.7842669263482 Training loss: 267.907463490963 Training loss: 258.2156918346882 Training loss: 251.79347000271082 . %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import helper # Test out your network! dataiter = iter(testloader) images, labels = dataiter.next() img = images[1] # TODO: Calculate the class probabilities (softmax) for img ps = torch.exp(model(img)) # Plot the image and probabilities helper.view_classify(img, ps, version=&#39;Fashion&#39;) .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part4Fashion-MNIST.html",
            "relUrl": "/2020/05/16/Part4Fashion-MNIST.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Training Neural Networks",
            "content": "Backpropagation . For single layer networks, gradient descent is straightforward to implement. However, it&#39;s more complicated for deeper, multilayer neural networks like the one we&#39;ve built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks. . Training multilayer networks is done through backpropagation which is really just an application of the chain rule from calculus. It&#39;s easiest to understand if we convert a two layer network into a graph representation. . . In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $ ell$. We use the loss as a measure of how bad the network&#39;s predictions are. The goal then is to adjust the weights and biases to minimize the loss. . To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule. . $$ large frac{ partial ell}{ partial W_1} = frac{ partial L_1}{ partial W_1} frac{ partial S}{ partial L_1} frac{ partial L_2}{ partial S} frac{ partial ell}{ partial L_2} $$Note: I&#39;m glossing over a few details here that require some knowledge of vector calculus, but they aren&#39;t necessary to understand what&#39;s going on. . We update our weights using this gradient with some learning rate $ alpha$. . $$ large W^ prime_1 = W_1 - alpha frac{ partial ell}{ partial W_1} $$The learning rate $ alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum. . Losses in PyTorch . Let&#39;s start by seeing how we calculate the loss with PyTorch. Through the nn module, PyTorch provides losses such as the cross-entropy loss (nn.CrossEntropyLoss). You&#39;ll usually see the loss assigned to criterion. As noted in the last part, with a classification problem such as MNIST, we&#39;re using the softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss. To actually calculate the loss, you first define the criterion then pass in the output of your network and the correct labels. . Something really important to note here. Looking at the documentation for nn.CrossEntropyLoss, . This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. . The input is expected to contain scores for each class. . This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the logits or scores. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can&#39;t accurately represent values near zero or one (read more here). It&#39;s usually best to avoid doing calculations with probabilities, typically we use log-probabilities. . import torch from torch import nn import torch.nn.functional as F from torchvision import datasets, transforms # Define a transform to normalize the data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), ]) # Download and load the training data trainset = datasets.MNIST(&#39;~/.pytorch/MNIST_data/&#39;, download=True, train=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) . # Build a feed-forward network model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 10)) # Define the loss criterion = nn.CrossEntropyLoss() # Get our data images, labels = next(iter(trainloader)) # Flatten images images = images.view(images.shape[0], -1) # Forward pass, get our logits logits = model(images) # Calculate the loss with the logits and the labels loss = criterion(logits, labels) print(loss) . tensor(2.2810) . In my experience it&#39;s more convenient to build the model with a log-softmax output using nn.LogSoftmax or F.log_softmax (documentation). Then you can get the actual probabilites by taking the exponential torch.exp(output). With a log-softmax output, you want to use the negative log likelihood loss, nn.NLLLoss (documentation). . Exercise: Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. . ## Solution # Build a feed-forward network model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 10), nn.LogSoftmax(dim=1)) # Define the loss criterion = nn.NLLLoss() # Get our data images, labels = next(iter(trainloader)) # Flatten images images = images.view(images.shape[0], -1) # Forward pass, get our log-probabilities logps = model(images) # Calculate the loss with the logps and the labels loss = criterion(logps, labels) print(loss) . tensor(2.3118) . Autograd . Now that we know how to calculate a loss, how do we use it to perform backpropagation? Torch provides a module, autograd, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set requires_grad = True on a tensor. You can do this at creation with the requires_grad keyword, or at any time with x.requires_grad_(True). . You can turn off gradients for a block of code with the torch.no_grad() content: . x = torch.zeros(1, requires_grad=True) &gt; &gt;&gt; with torch.no_grad():... y = x * 2&gt;&gt;&gt; y.requires_grad False . Also, you can turn on or off gradients altogether with torch.set_grad_enabled(True|False). . The gradients are computed with respect to some variable z with z.backward(). This does a backward pass through the operations that created z. . x = torch.randn(2,2, requires_grad=True) print(x) . tensor([[ 0.7652, -1.4550], [-1.2232, 0.1810]]) . y = x**2 print(y) . tensor([[ 0.5856, 2.1170], [ 1.4962, 0.0328]]) . Below we can see the operation that created y, a power operation PowBackward0. . ## grad_fn shows the function that generated this variable print(y.grad_fn) . &lt;PowBackward0 object at 0x10b508b70&gt; . The autograd module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it&#39;s able to calculate the gradients for a chain of operations, with respect to any one tensor. Let&#39;s reduce the tensor y to a scalar value, the mean. . z = y.mean() print(z) . tensor(1.0579) . You can check the gradients for x and y but they are empty currently. . print(x.grad) . None . To calculate the gradients, you need to run the .backward method on a Variable, z for example. This will calculate the gradient for z with respect to x . $$ frac{ partial z}{ partial x} = frac{ partial}{ partial x} left[ frac{1}{n} sum_i^n x_i^2 right] = frac{x}{2} $$ z.backward() print(x.grad) print(x/2) . tensor([[ 0.3826, -0.7275], [-0.6116, 0.0905]]) tensor([[ 0.3826, -0.7275], [-0.6116, 0.0905]]) . These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step. . Loss and Autograd together . When we create a network with PyTorch, all of the parameters are initialized with requires_grad = True. This means that when we calculate the loss and call loss.backward(), the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass. . # Build a feed-forward network model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 10), nn.LogSoftmax(dim=1)) criterion = nn.NLLLoss() images, labels = next(iter(trainloader)) images = images.view(images.shape[0], -1) logps = model(images) loss = criterion(logps, labels) . print(&#39;Before backward pass: n&#39;, model[0].weight.grad) loss.backward() print(&#39;After backward pass: n&#39;, model[0].weight.grad) . Before backward pass: None After backward pass: tensor(1.00000e-02 * [[-0.0296, -0.0296, -0.0296, ..., -0.0296, -0.0296, -0.0296], [-0.0441, -0.0441, -0.0441, ..., -0.0441, -0.0441, -0.0441], [ 0.0177, 0.0177, 0.0177, ..., 0.0177, 0.0177, 0.0177], ..., [ 0.4021, 0.4021, 0.4021, ..., 0.4021, 0.4021, 0.4021], [-0.1361, -0.1361, -0.1361, ..., -0.1361, -0.1361, -0.1361], [-0.0155, -0.0155, -0.0155, ..., -0.0155, -0.0155, -0.0155]]) . Training the network! . There&#39;s one last piece we need to start training, an optimizer that we&#39;ll use to update the weights with the gradients. We get these from PyTorch&#39;s optim package. For example we can use stochastic gradient descent with optim.SGD. You can see how to define an optimizer below. . from torch import optim # Optimizers require the parameters to optimize and a learning rate optimizer = optim.SGD(model.parameters(), lr=0.01) . Now we know how to use all the individual parts so it&#39;s time to see how they work together. Let&#39;s consider just one learning step before looping through all the data. The general process with PyTorch: . Make a forward pass through the network | Use the network output to calculate the loss | Perform a backward pass through the network with loss.backward() to calculate the gradients | Take a step with the optimizer to update the weights | . Below I&#39;ll go through one training step and print out the weights and gradients so you can see how it changes. Note that I have a line of code optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you&#39;ll retain gradients from previous training batches. . print(&#39;Initial weights - &#39;, model[0].weight) images, labels = next(iter(trainloader)) images.resize_(64, 784) # Clear the gradients, do this because gradients are accumulated optimizer.zero_grad() # Forward pass, then backward pass, then update weights output = model(images) loss = criterion(output, labels) loss.backward() print(&#39;Gradient -&#39;, model[0].weight.grad) . Initial weights - Parameter containing: tensor([[ 3.5691e-02, 2.1438e-02, 2.2862e-02, ..., -1.3882e-02, -2.3719e-02, -4.6573e-03], [-3.2397e-03, 3.5117e-03, -1.5220e-03, ..., 1.4400e-02, 2.8463e-03, 2.5381e-03], [ 5.6122e-03, 4.8693e-03, -3.4507e-02, ..., -2.8224e-02, -1.2907e-02, -1.5818e-02], ..., [-1.4372e-02, 2.3948e-02, 2.8374e-02, ..., -1.5817e-02, 3.2719e-02, 8.5537e-03], [-1.1999e-02, 1.9462e-02, 1.3998e-02, ..., -2.0170e-03, 1.4254e-02, 2.2238e-02], [ 3.9955e-04, 4.8263e-03, -2.1819e-02, ..., 1.2959e-02, -4.4880e-03, 1.4609e-02]]) Gradient - tensor(1.00000e-02 * [[-0.2609, -0.2609, -0.2609, ..., -0.2609, -0.2609, -0.2609], [-0.0695, -0.0695, -0.0695, ..., -0.0695, -0.0695, -0.0695], [ 0.0514, 0.0514, 0.0514, ..., 0.0514, 0.0514, 0.0514], ..., [ 0.0967, 0.0967, 0.0967, ..., 0.0967, 0.0967, 0.0967], [-0.1878, -0.1878, -0.1878, ..., -0.1878, -0.1878, -0.1878], [ 0.0281, 0.0281, 0.0281, ..., 0.0281, 0.0281, 0.0281]]) . # Take an update step and few the new weights optimizer.step() print(&#39;Updated weights - &#39;, model[0].weight) . Updated weights - Parameter containing: tensor([[ 3.5717e-02, 2.1464e-02, 2.2888e-02, ..., -1.3856e-02, -2.3693e-02, -4.6312e-03], [-3.2327e-03, 3.5187e-03, -1.5150e-03, ..., 1.4407e-02, 2.8533e-03, 2.5450e-03], [ 5.6071e-03, 4.8642e-03, -3.4513e-02, ..., -2.8230e-02, -1.2912e-02, -1.5823e-02], ..., [-1.4381e-02, 2.3938e-02, 2.8365e-02, ..., -1.5827e-02, 3.2709e-02, 8.5441e-03], [-1.1981e-02, 1.9481e-02, 1.4016e-02, ..., -1.9983e-03, 1.4272e-02, 2.2257e-02], [ 3.9674e-04, 4.8235e-03, -2.1821e-02, ..., 1.2956e-02, -4.4908e-03, 1.4606e-02]]) . Training for real . Now we&#39;ll put this algorithm into a loop so we can go through all the images. Some nomenclature, one pass through the entire dataset is called an epoch. So here we&#39;re going to loop through trainloader to get our training batches. For each batch, we&#39;ll doing a training pass where we calculate the loss, do a backwards pass, and update the weights. . Exercise: Implement the training pass for our network. If you implemented it correctly, you should see the training loss drop with each epoch. . model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 10), nn.LogSoftmax(dim=1)) criterion = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.003) epochs = 5 for e in range(epochs): running_loss = 0 for images, labels in trainloader: # Flatten MNIST images into a 784 long vector images = images.view(images.shape[0], -1) # TODO: Training pass optimizer.zero_grad() output = model(images) loss = criterion(output, labels) loss.backward() optimizer.step() running_loss += loss.item() else: print(f&quot;Training loss: {running_loss/len(trainloader)}&quot;) . Training loss: 1.8959971736234897 Training loss: 0.8684300759644397 Training loss: 0.537974218426864 Training loss: 0.43723612014990626 Training loss: 0.39094475933165945 . With the network trained, we can check out it&#39;s predictions. . %matplotlib inline import helper images, labels = next(iter(trainloader)) img = images[0].view(1, 784) # Turn off gradients to speed up this part with torch.no_grad(): logps = model(img) # Output of the network are log-probabilities, need to take exponential for probabilities ps = torch.exp(logps) helper.view_classify(img.view(1, 28, 28), ps) . Now our network is brilliant. It can accurately predict the digits in our images. Next up you&#39;ll write the code for training a neural network on a more complex dataset. .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part3TrainingNeuralNetworks.html",
            "relUrl": "/2020/05/16/Part3TrainingNeuralNetworks.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Neural networks with PyTorch",
            "content": "# Import necessary packages %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import numpy as np import torch import helper import matplotlib.pyplot as plt . Now we&#39;re going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we&#39;ll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below . . Our goal is to build a neural network that can take one of these images and predict the digit in the image. . First up, we need to get our dataset. This is provided through the torchvision package. The code below will download the MNIST dataset, then create training and test datasets for us. Don&#39;t worry too much about the details here, you&#39;ll learn more about this later. . ### Run this cell from torchvision import datasets, transforms # Define a transform to normalize the data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), ]) # Download and load the training data trainset = datasets.MNIST(&#39;~/.pytorch/MNIST_data/&#39;, download=True, train=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) . We have the training data loaded into trainloader and we make that an iterator with iter(trainloader). Later, we&#39;ll use this to loop through the dataset for training, like . for image, label in trainloader: ## do things with images and labels . You&#39;ll notice I created the trainloader with a batch size of 64, and shuffle=True. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch. And shuffle=True tells it to shuffle the dataset every time we start going through the data loader again. But here I&#39;m just grabbing the first batch so we can check out the data. We can see below that images is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images. . dataiter = iter(trainloader) images, labels = dataiter.next() print(type(images)) print(images.shape) print(labels.shape) . &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([64, 1, 28, 28]) torch.Size([64]) . This is what one of the images looks like. . plt.imshow(images[1].numpy().squeeze(), cmap=&#39;Greys_r&#39;); . First, let&#39;s try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we&#39;ll see how to do it using PyTorch&#39;s nn module which provides a much more convenient and powerful method for defining network architectures. . The networks you&#39;ve seen so far are called fully-connected or dense networks. Each unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples). However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape (64, 1, 28, 28) to a have a shape of (64, 784), 784 is 28 times 28. This is typically called flattening, we flattened the 2D images into 1D vectors. . Previously you built a network with one output unit. Here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we&#39;ll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). We&#39;ll see how to convert the network output into a probability distribution next. . Exercise: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we&#39;ll add one that gives us a probability distribution next. . ## Solution def activation(x): return 1/(1+torch.exp(-x)) # Flatten the input images inputs = images.view(images.shape[0], -1) # Create parameters w1 = torch.randn(784, 256) b1 = torch.randn(256) w2 = torch.randn(256, 10) b2 = torch.randn(10) h = activation(torch.mm(inputs, w1) + b1) out = torch.mm(h, w2) + b2 . Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to. Something that looks like this: . Here we see that the probability for each class is roughly the same. This is representing an untrained network, it hasn&#39;t seen any data yet so it just returns a uniform distribution with equal probabilities for each class. . To calculate this probability distribution, we often use the softmax function. Mathematically this looks like . $$ Large sigma(x_i) = cfrac{e^{x_i}}{ sum_k^K{e^{x_k}}} $$What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one. . Exercise: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you&#39;ll need to pay attention to the shapes when doing this. If you have a tensor a with shape (64, 10) and a tensor b with shape (64,), doing a/b will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you&#39;ll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need b to have a shape of (64, 1). This way PyTorch will divide the 10 values in each row of a by the one value in each row of b. Pay attention to how you take the sum as well. You&#39;ll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum across the rows while dim=1 takes the sum across the columns. . ## Solution def softmax(x): return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1) probabilities = softmax(out) # Does it have the right shape? Should be (64, 10) print(probabilities.shape) # Does it sum to 1? print(probabilities.sum(dim=1)) . torch.Size([64, 10]) tensor([ 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) . Building networks with PyTorch . PyTorch provides a module nn that makes building networks much simpler. Here I&#39;ll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output. . from torch import nn . class Network(nn.Module): def __init__(self): super().__init__() # Inputs to hidden layer linear transformation self.hidden = nn.Linear(784, 256) # Output layer, 10 units - one for each digit self.output = nn.Linear(256, 10) # Define sigmoid activation and softmax output self.sigmoid = nn.Sigmoid() self.softmax = nn.Softmax(dim=1) def forward(self, x): # Pass the input tensor through each of our operations x = self.hidden(x) x = self.sigmoid(x) x = self.output(x) x = self.softmax(x) return x . Let&#39;s go through this bit by bit. . class Network(nn.Module): . Here we&#39;re inheriting from nn.Module. Combined with super().__init__() this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from nn.Module when you&#39;re creating a class for your network. The name of the class itself can be anything. . self.hidden = nn.Linear(784, 256) . This line creates a module for a linear transformation, $x mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to self.hidden. The module automatically creates the weight and bias tensors which we&#39;ll use in the forward method. You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias. . self.output = nn.Linear(256, 10) . Similarly, this creates another linear transformation with 256 inputs and 10 outputs. . self.sigmoid = nn.Sigmoid() self.softmax = nn.Softmax(dim=1) . Here I defined operations for the sigmoid activation and softmax output. Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns. . def forward(self, x): . PyTorch networks created with nn.Module must have a forward method defined. It takes in a tensor x and passes it through the operations you defined in the __init__ method. . x = self.hidden(x) x = self.sigmoid(x) x = self.output(x) x = self.softmax(x) . Here the input tensor x is passed through each operation a reassigned to x. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn&#39;t matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the __init__ method doesn&#39;t matter, but you&#39;ll need to sequence the operations correctly in the forward method. . Now we can create a Network object. . # Create the network and look at it&#39;s text representation model = Network() model . Network( (hidden): Linear(in_features=784, out_features=256, bias=True) (output): Linear(in_features=256, out_features=10, bias=True) (sigmoid): Sigmoid() (softmax): Softmax() ) . You can define the network somewhat more concisely and clearly using the torch.nn.functional module. This is the most common way you&#39;ll see networks defined as many operations are simple element-wise functions. We normally import this module as F, import torch.nn.functional as F. . import torch.nn.functional as F class Network(nn.Module): def __init__(self): super().__init__() # Inputs to hidden layer linear transformation self.hidden = nn.Linear(784, 256) # Output layer, 10 units - one for each digit self.output = nn.Linear(256, 10) def forward(self, x): # Hidden layer with sigmoid activation x = F.sigmoid(self.hidden(x)) # Output layer with softmax activation x = F.softmax(self.output(x), dim=1) return x . Activation functions . So far we&#39;ve only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, the activation functions must be non-linear. Here are a few more examples of common activation functions: Tanh (hyperbolic tangent), and ReLU (rectified linear unit). . . In practice, the ReLU function is used almost exclusively as the activation function for hidden layers. . Your Turn to Build a Network . . Exercise: Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the nn.ReLU module or F.relu function. It&#39;s good practice to name your layers by their type of network, for instance &#39;fc&#39; to represent a fully-connected layer. As you code your solution, use fc1, fc2, and fc3 as your layer names. . ## Solution class Network(nn.Module): def __init__(self): super().__init__() # Defining the layers, 128, 64, 10 units each self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) # Output layer, 10 units - one for each digit self.fc3 = nn.Linear(64, 10) def forward(self, x): &#39;&#39;&#39; Forward pass through the network, returns the output logits &#39;&#39;&#39; x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) x = F.softmax(x, dim=1) return x model = Network() model . Network( (fc1): Linear(in_features=784, out_features=128, bias=True) (fc2): Linear(in_features=128, out_features=64, bias=True) (fc3): Linear(in_features=64, out_features=10, bias=True) ) . Initializing weights and biases . The weights and such are automatically initialized for you, but it&#39;s possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with model.fc1.weight for instance. . print(model.fc1.weight) print(model.fc1.bias) . Parameter containing: tensor([[-2.3278e-02, -1.2170e-03, -1.1882e-02, ..., 3.3567e-02, 4.4827e-03, 1.4840e-02], [ 4.8464e-03, 1.9844e-02, 3.9791e-03, ..., -2.6048e-02, -3.5558e-02, -2.2386e-02], [-1.9664e-02, 8.1722e-03, 2.6729e-02, ..., -1.5122e-02, 2.7632e-02, -1.9567e-02], ..., [-3.3571e-02, -2.9686e-02, -2.1387e-02, ..., 3.0770e-02, 1.0800e-02, -6.5941e-03], [ 2.9749e-02, 1.2849e-02, 2.7320e-02, ..., -1.9899e-02, 2.7131e-02, 2.2082e-02], [ 1.3992e-02, -2.1520e-02, 3.1907e-02, ..., 2.2435e-02, 1.1370e-02, 2.1568e-02]]) Parameter containing: tensor(1.00000e-02 * [-1.3222, 2.4094, -2.1571, 3.2237, 2.5302, -1.1515, 2.6382, -2.3426, -3.5689, -1.0724, -2.8842, -2.9667, -0.5022, 1.1381, 1.2849, 3.0731, -2.0207, -2.3282, 0.3168, -2.8098, -1.0740, -1.8273, 1.8692, 2.9404, 0.1783, 0.9391, -0.7085, -1.2522, -2.7769, 0.0916, -1.4283, -0.3267, -1.6876, -1.8580, -2.8724, -3.5512, 3.2155, 1.5532, 0.8836, -1.2911, 1.5735, -3.0478, -1.3089, -2.2117, 1.5162, -0.8055, -1.3307, -2.4267, -1.2665, 0.8666, -2.2325, -0.4797, -0.5448, -0.6612, -0.6022, 2.6399, 1.4673, -1.5417, -2.9492, -2.7507, 0.6157, -0.0681, -0.8171, -0.3554, -0.8225, 3.3906, 3.3509, -1.4484, 3.5124, -2.6519, 0.9721, -2.5068, -3.4962, 3.4743, 1.1525, -2.7555, -3.1673, 2.2906, 2.5914, 1.5992, -1.2859, -0.5682, 2.1488, -2.0631, 2.6281, -2.4639, 2.2622, 2.3632, -0.1979, 0.7160, 1.7594, 0.0761, -2.8886, -3.5467, 2.7691, 0.8280, -2.2398, -1.4602, -1.3475, -1.4738, 0.6338, 3.2811, -3.0628, 2.7044, 1.2775, 2.8856, -3.3938, 2.7056, 0.5826, -0.6286, 1.2381, 0.7316, -2.4725, -1.2958, -3.1543, -0.8584, 0.5517, 2.8176, 0.0947, -1.6849, -1.4968, 3.1039, 1.7680, 1.1803, -1.4402, 2.5710, -3.3057, 1.9027]) . For custom initialization, we want to modify these tensors in place. These are actually autograd Variables, so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors, we can fill them with zeros (for biases) or random normal values. . # Set biases to all zeros model.fc1.bias.data.fill_(0) . tensor([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . # sample from random normal with standard dev = 0.01 model.fc1.weight.data.normal_(std=0.01) . tensor([[ 5.9294e-03, 9.6176e-04, 9.5034e-03, ..., -1.2588e-03, 3.4878e-03, 1.4495e-02], [-9.3127e-04, -1.0287e-02, 3.5423e-03, ..., 1.1155e-02, 2.5012e-03, 1.3503e-02], [-1.5078e-02, 8.1119e-03, 5.0678e-03, ..., 2.8613e-03, 9.7443e-03, -6.5530e-03], ..., [-2.2938e-03, -1.8630e-02, -4.2459e-03, ..., 1.4718e-02, 2.7655e-03, 3.2661e-03], [-3.0470e-03, 1.2617e-02, -7.9772e-03, ..., -1.4699e-02, -9.1425e-03, 5.3492e-03], [ 2.5911e-03, 2.4797e-03, -6.1172e-03, ..., 1.0584e-02, -1.0307e-02, -3.1901e-03]]) . Forward pass . Now that we have a network, let&#39;s see what happens when we pass in an image. . # Grab some data dataiter = iter(trainloader) images, labels = dataiter.next() # Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) images.resize_(64, 1, 784) # or images.resize_(images.shape[0], 1, 784) to automatically get batch size # Forward pass through the network img_idx = 0 ps = model.forward(images[img_idx,:]) img = images[img_idx] helper.view_classify(img.view(1, 28, 28), ps) . As you can see above, our network has basically no idea what this digit is. It&#39;s because we haven&#39;t trained it yet, all the weights are random! . Using nn.Sequential . PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, nn.Sequential (documentation). Using this to build the equivalent network: . # Hyperparameters for our network input_size = 784 hidden_sizes = [128, 64] output_size = 10 # Build a feed-forward network model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]), nn.ReLU(), nn.Linear(hidden_sizes[0], hidden_sizes[1]), nn.ReLU(), nn.Linear(hidden_sizes[1], output_size), nn.Softmax(dim=1)) print(model) # Forward pass through the network and display output images, labels = next(iter(trainloader)) images.resize_(images.shape[0], 1, 784) ps = model.forward(images[0,:]) helper.view_classify(images[0].view(1, 28, 28), ps) . Sequential( (0): Linear(in_features=784, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=64, bias=True) (3): ReLU() (4): Linear(in_features=64, out_features=10, bias=True) (5): Softmax() ) . The operations are availble by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you&#39;d use model[0]. . print(model[0]) model[0].weight . Linear(in_features=784, out_features=128, bias=True) . Parameter containing: tensor([[-7.0372e-03, 2.9689e-02, 2.6028e-02, ..., 1.1196e-02, -2.1928e-02, -3.4886e-02], [ 1.0624e-04, -1.6610e-02, -2.2891e-02, ..., -6.4412e-03, -2.5026e-02, 1.0674e-02], [-2.4707e-02, 1.4146e-02, 2.0084e-02, ..., 1.2227e-02, 2.3441e-02, -9.5175e-03], ..., [-2.0119e-02, 7.2614e-03, -1.3481e-02, ..., 8.1745e-03, -1.3348e-02, -1.1955e-02], [ 3.2282e-02, 1.8674e-02, -3.0826e-02, ..., 1.7296e-02, -2.6710e-02, -3.0684e-02], [ 1.7686e-02, 3.1376e-02, -2.2645e-02, ..., 2.4630e-02, 3.1129e-02, 7.0939e-03]]) . You can also pass in an OrderedDict to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so each operation must have a different name. . from collections import OrderedDict model = nn.Sequential(OrderedDict([ (&#39;fc1&#39;, nn.Linear(input_size, hidden_sizes[0])), (&#39;relu1&#39;, nn.ReLU()), (&#39;fc2&#39;, nn.Linear(hidden_sizes[0], hidden_sizes[1])), (&#39;relu2&#39;, nn.ReLU()), (&#39;output&#39;, nn.Linear(hidden_sizes[1], output_size)), (&#39;softmax&#39;, nn.Softmax(dim=1))])) model . Sequential( (fc1): Linear(in_features=784, out_features=128, bias=True) (relu1): ReLU() (fc2): Linear(in_features=128, out_features=64, bias=True) (relu2): ReLU() (output): Linear(in_features=64, out_features=10, bias=True) (softmax): Softmax() ) . Now you can access layers either by integer or the name . print(model[0]) print(model.fc1) . Linear(in_features=784, out_features=128, bias=True) Linear(in_features=784, out_features=128, bias=True) . In the next notebook, we&#39;ll see how we can train a neural network to accuractly predict the numbers appearing in the MNIST images. .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part2NeuralNetworksinPyTorch.html",
            "relUrl": "/2020/05/16/Part2NeuralNetworksinPyTorch.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Introduction to Deep Learning with PyTorch",
            "content": "Neural Networks . Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply &quot;neurons.&quot; Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit&#39;s output. . . Mathematically this looks like: . $$ begin{align} y &amp;= f(w_1 x_1 + w_2 x_2 + b) y &amp;= f left( sum_i w_i x_i +b right) end{align} $$With vectors this is the dot/inner product of two vectors: . $$ h = begin{bmatrix} x_1 , x_2 cdots x_n end{bmatrix} cdot begin{bmatrix} w_1 w_2 vdots w_n end{bmatrix} $$ Tensors . It turns out neural network computations are just a bunch of linear algebra operations on tensors, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors. . . With the basics covered, it&#39;s time to explore how we can use PyTorch to build a simple neural network. . # First, import PyTorch import torch . def activation(x): &quot;&quot;&quot; Sigmoid activation function Arguments x: torch.Tensor &quot;&quot;&quot; return 1/(1+torch.exp(-x)) . ### Generate some data torch.manual_seed(7) # Set the random seed so things are predictable # Features are 5 random normal variables features = torch.randn((1, 5)) # True weights for our data, random normal variables again weights = torch.randn_like(features) # and a true bias term bias = torch.randn((1, 1)) . Above I generated data we can use to get the output of our simple network. This is all just random for now, going forward we&#39;ll start using normal data. Going through each relevant line: . features = torch.randn((1, 5)) creates a tensor with shape (1, 5), one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one. . weights = torch.randn_like(features) creates another tensor with the same shape as features, again containing values from a normal distribution. . Finally, bias = torch.randn((1, 1)) creates a single value from a normal distribution. . PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you&#39;ll use PyTorch tensors pretty much the same way you&#39;d use Numpy arrays. They come with some nice benefits though such as GPU acceleration which we&#39;ll get to later. For now, use the generated data to calculate the output of this simple single layer network. . Exercise:Calculate the output of the network with input features features, weights weights, and bias bias. Similar to Numpy, PyTorch has a torch.sum() function, as well as a .sum() method on tensors, for taking sums. Use the function activation defined above as the activation function. . ### Solution # Now, make our labels from our data and true weights y = activation(torch.sum(features * weights) + bias) y = activation((features * weights).sum() + bias) . You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you&#39;ll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs. . Here, we want to do a matrix multiplication of the features and the weights. For this we can use torch.mm() or torch.matmul() which is somewhat more complicated and supports broadcasting. If we try to do it with features and weights as they are, we&#39;ll get an error . &gt; &gt; torch.mm(features, weights) RuntimeError Traceback (most recent call last) &lt;ipython-input-13-15d592eb5279&gt; in &lt;module&gt;() -&gt; 1 torch.mm(features, weights) RuntimeError:size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033 . As you&#39;re building neural networks in any framework, you&#39;ll see this often. Really often. What&#39;s happening here is our tensors aren&#39;t the correct shapes to perform a matrix multiplication. Remember that for matrix multiplications, the number of columns in the first tensor must equal to the number of rows in the second column. Both features and weights have the same shape, (1, 5). This means we need to change the shape of weights to get the matrix multiplication to work. . Note: To see the shape of a tensor called tensor, use tensor.shape. If you&#39;re building neural networks, you&#39;ll be using this method often. . There are a few options here: weights.reshape(), weights.resize_(), and weights.view(). . weights.reshape(a, b) will return a new tensor with the same data as weights with size (a, b) sometimes, and sometimes a clone, as in it copies the data to another part of memory. | weights.resize_(a, b) returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed in-place. Here is a great forum thread to read more about in-place operations in PyTorch. | weights.view(a, b) will return a new tensor with the same data as weights with size (a, b). | . I usually use .view(), but any of the three methods will work for this. So, now we can reshape weights to have five rows and one column with something like weights.view(5, 1). . Exercise:Calculate the output of our little network using matrix multiplication. . ## Solution y = activation(torch.mm(features, weights.view(5,1)) + bias) . Stack them up! . That&#39;s how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix. . . The first layer shown on the bottom here are the inputs, understandably called the input layer. The middle layer is called the hidden layer, and the final layer (on the right) is the output layer. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated . $$ vec{h} = [h_1 , h_2] = begin{bmatrix} x_1 , x_2 cdots , x_n end{bmatrix} cdot begin{bmatrix} w_{11} &amp; w_{12} w_{21} &amp;w_{22} vdots &amp; vdots w_{n1} &amp;w_{n2} end{bmatrix} $$The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply . $$ y = f_2 ! left( , f_1 ! left( vec{x} , mathbf{W_1} right) mathbf{W_2} right) $$ ### Generate some data torch.manual_seed(7) # Set the random seed so things are predictable # Features are 3 random normal variables features = torch.randn((1, 3)) # Define the size of each layer in our network n_input = features.shape[1] # Number of input units, must match number of input features n_hidden = 2 # Number of hidden units n_output = 1 # Number of output units # Weights for inputs to hidden layer W1 = torch.randn(n_input, n_hidden) # Weights for hidden layer to output layer W2 = torch.randn(n_hidden, n_output) # and bias terms for hidden and output layers B1 = torch.randn((1, n_hidden)) B2 = torch.randn((1, n_output)) . Exercise: Calculate the output for this multi-layer network using the weights W1 &amp; W2, and the biases, B1 &amp; B2. . ### Solution h = activation(torch.mm(features, W1) + B1) output = activation(torch.mm(h, W2) + B2) print(output) . tensor([[ 0.3171]]) . If you did this correctly, you should see the output tensor([[ 0.3171]]). . The number of hidden units a parameter of the network, often called a hyperparameter to differentiate it from the weights and biases parameters. As you&#39;ll see later when we discuss training a neural network, the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions. . Numpy to Torch and back . Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use torch.from_numpy(). To convert a tensor to a Numpy array, use the .numpy() method. . import numpy as np a = np.random.rand(4,3) a . array([[ 0.33669496, 0.59531562, 0.65433944], [ 0.86531224, 0.59945364, 0.28043973], [ 0.48409303, 0.98357622, 0.33884284], [ 0.25591391, 0.51081783, 0.39986403]]) . b = torch.from_numpy(a) b . 0.3367 0.5953 0.6543 0.8653 0.5995 0.2804 0.4841 0.9836 0.3388 0.2559 0.5108 0.3999 [torch.DoubleTensor of size 4x3] . b.numpy() . array([[ 0.33669496, 0.59531562, 0.65433944], [ 0.86531224, 0.59945364, 0.28043973], [ 0.48409303, 0.98357622, 0.33884284], [ 0.25591391, 0.51081783, 0.39986403]]) . The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well. . # Multiply PyTorch Tensor by 2, in place b.mul_(2) . 0.6734 1.1906 1.3087 1.7306 1.1989 0.5609 0.9682 1.9672 0.6777 0.5118 1.0216 0.7997 [torch.DoubleTensor of size 4x3] . # Numpy array matches new values from Tensor a . array([[ 0.67338991, 1.19063124, 1.30867888], [ 1.73062448, 1.19890728, 0.56087946], [ 0.96818606, 1.96715243, 0.67768568], [ 0.51182782, 1.02163565, 0.79972807]]) .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/16/Part1TensorsinPyTorch.html",
            "relUrl": "/2020/05/16/Part1TensorsinPyTorch.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Predicting Student Admissions with Neural Networks",
            "content": "# Importing pandas and numpy import pandas as pd import numpy as np # Reading the csv file into a pandas DataFrame data = pd.read_csv(&#39;student_data.csv&#39;) # Printing out the first 10 rows of our data data[:10] . admit gre gpa rank . 0 0 | 380 | 3.61 | 3 | . 1 1 | 660 | 3.67 | 3 | . 2 1 | 800 | 4.00 | 1 | . 3 1 | 640 | 3.19 | 4 | . 4 0 | 520 | 2.93 | 4 | . 5 1 | 760 | 3.00 | 2 | . 6 1 | 560 | 2.98 | 1 | . 7 0 | 400 | 3.08 | 2 | . 8 1 | 540 | 3.39 | 3 | . 9 0 | 700 | 3.92 | 2 | . Plotting the data . First let&#39;s make a plot of our data to see how it looks. In order to have a 2D plot, let&#39;s ingore the rank. . # Importing matplotlib import matplotlib.pyplot as plt %matplotlib inline # Function to help us plot def plot_points(data): X = np.array(data[[&quot;gre&quot;,&quot;gpa&quot;]]) y = np.array(data[&quot;admit&quot;]) admitted = X[np.argwhere(y==1)] rejected = X[np.argwhere(y==0)] plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = &#39;red&#39;, edgecolor = &#39;k&#39;) plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = &#39;cyan&#39;, edgecolor = &#39;k&#39;) plt.xlabel(&#39;Test (GRE)&#39;) plt.ylabel(&#39;Grades (GPA)&#39;) # Plotting the points plot_points(data) plt.show() . Roughly, it looks like the students with high scores in the grades and test passed, while the ones with low scores didn&#39;t, but the data is not as nicely separable as we hoped it would. Maybe it would help to take the rank into account? Let&#39;s make 4 plots, each one for each rank. . # Separating the ranks data_rank1 = data[data[&quot;rank&quot;]==1] data_rank2 = data[data[&quot;rank&quot;]==2] data_rank3 = data[data[&quot;rank&quot;]==3] data_rank4 = data[data[&quot;rank&quot;]==4] # Plotting the graphs plot_points(data_rank1) plt.title(&quot;Rank 1&quot;) plt.show() plot_points(data_rank2) plt.title(&quot;Rank 2&quot;) plt.show() plot_points(data_rank3) plt.title(&quot;Rank 3&quot;) plt.show() plot_points(data_rank4) plt.title(&quot;Rank 4&quot;) plt.show() . This looks more promising, as it seems that the lower the rank, the higher the acceptance rate. Let&#39;s use the rank as one of our inputs. In order to do this, we should one-hot encode it. . TODO: One-hot encoding the rank . Use the get_dummies function in pandas in order to one-hot encode the data. . Hint: To drop a column, it&#39;s suggested that you use one_hot_data.drop( ). . # TODO: Make dummy variables for rank and concat existing columns one_hot_data = pd.concat([data, pd.get_dummies(data[&#39;rank&#39;], prefix=&#39;rank&#39;)], axis=1) # TODO: Drop the previous rank column one_hot_data = one_hot_data.drop(&#39;rank&#39;, axis=1) # Print the first 10 rows of our data one_hot_data[:10] . admit gre gpa rank_1 rank_2 rank_3 rank_4 . 0 0 | 380 | 3.61 | 0 | 0 | 1 | 0 | . 1 1 | 660 | 3.67 | 0 | 0 | 1 | 0 | . 2 1 | 800 | 4.00 | 1 | 0 | 0 | 0 | . 3 1 | 640 | 3.19 | 0 | 0 | 0 | 1 | . 4 0 | 520 | 2.93 | 0 | 0 | 0 | 1 | . 5 1 | 760 | 3.00 | 0 | 1 | 0 | 0 | . 6 1 | 560 | 2.98 | 1 | 0 | 0 | 0 | . 7 0 | 400 | 3.08 | 0 | 1 | 0 | 0 | . 8 1 | 540 | 3.39 | 0 | 0 | 1 | 0 | . 9 0 | 700 | 3.92 | 0 | 1 | 0 | 0 | . TODO: Scaling the data . The next step is to scale the data. We notice that the range for grades is 1.0-4.0, whereas the range for test scores is roughly 200-800, which is much larger. This means our data is skewed, and that makes it hard for a neural network to handle. Let&#39;s fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800. . # Making a copy of our data processed_data = one_hot_data[:] # TODO: Scale the columns processed_data[&#39;gre&#39;] = processed_data[&#39;gre&#39;]/800 processed_data[&#39;gpa&#39;] = processed_data[&#39;gpa&#39;]/4.0 # Printing the first 10 rows of our procesed data processed_data[:10] . admit gre gpa rank_1 rank_2 rank_3 rank_4 . 0 0 | 0.475 | 0.9025 | 0 | 0 | 1 | 0 | . 1 1 | 0.825 | 0.9175 | 0 | 0 | 1 | 0 | . 2 1 | 1.000 | 1.0000 | 1 | 0 | 0 | 0 | . 3 1 | 0.800 | 0.7975 | 0 | 0 | 0 | 1 | . 4 0 | 0.650 | 0.7325 | 0 | 0 | 0 | 1 | . 5 1 | 0.950 | 0.7500 | 0 | 1 | 0 | 0 | . 6 1 | 0.700 | 0.7450 | 1 | 0 | 0 | 0 | . 7 0 | 0.500 | 0.7700 | 0 | 1 | 0 | 0 | . 8 1 | 0.675 | 0.8475 | 0 | 0 | 1 | 0 | . 9 0 | 0.875 | 0.9800 | 0 | 1 | 0 | 0 | . Splitting the data into Training and Testing . In order to test our algorithm, we&#39;ll split the data into a Training and a Testing set. The size of the testing set will be 10% of the total data. . sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False) train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample) print(&quot;Number of training samples is&quot;, len(train_data)) print(&quot;Number of testing samples is&quot;, len(test_data)) print(train_data[:10]) print(test_data[:10]) . Number of training samples is 360 Number of testing samples is 40 admit gre gpa rank_1 rank_2 rank_3 rank_4 100 0 0.425 0.7875 0 0 1 0 284 1 0.550 0.8475 0 1 0 0 376 0 0.775 0.9075 0 1 0 0 69 0 1.000 0.9325 1 0 0 0 147 0 0.700 0.6775 0 0 1 0 298 0 0.675 0.8750 0 1 0 0 127 0 0.925 0.9350 0 0 0 1 135 0 0.625 0.8925 0 0 1 0 260 0 0.850 0.7775 0 1 0 0 46 1 0.725 0.8650 0 1 0 0 admit gre gpa rank_1 rank_2 rank_3 rank_4 8 1 0.675 0.8475 0 0 1 0 20 0 0.625 0.7925 0 0 1 0 22 0 0.750 0.7050 0 0 0 1 23 0 0.850 0.7975 0 0 0 1 34 0 0.450 0.7850 1 0 0 0 35 0 0.500 0.7625 0 1 0 0 40 0 0.700 0.6050 0 1 0 0 41 1 0.725 0.8300 0 1 0 0 43 0 0.625 0.8275 0 0 1 0 55 1 0.925 1.0000 0 0 1 0 . Splitting the data into features and targets (labels) . Now, as a final step before the training, we&#39;ll split the data into features (X) and targets (y). . features = train_data.drop(&#39;admit&#39;, axis=1) targets = train_data[&#39;admit&#39;] features_test = test_data.drop(&#39;admit&#39;, axis=1) targets_test = test_data[&#39;admit&#39;] print(features[:10]) print(targets[:10]) . gre gpa rank_1 rank_2 rank_3 rank_4 100 0.425 0.7875 0 0 1 0 284 0.550 0.8475 0 1 0 0 376 0.775 0.9075 0 1 0 0 69 1.000 0.9325 1 0 0 0 147 0.700 0.6775 0 0 1 0 298 0.675 0.8750 0 1 0 0 127 0.925 0.9350 0 0 0 1 135 0.625 0.8925 0 0 1 0 260 0.850 0.7775 0 1 0 0 46 0.725 0.8650 0 1 0 0 100 0 284 1 376 0 69 0 147 0 298 0 127 0 135 0 260 0 46 1 Name: admit, dtype: int64 . Training the 2-layer Neural Network . The following function trains the 2-layer neural network. First, we&#39;ll write some helper functions. . # Activation (sigmoid) function def sigmoid(x): return 1 / (1 + np.exp(-x)) def sigmoid_prime(x): return sigmoid(x) * (1-sigmoid(x)) def error_formula(y, output): return - y*np.log(output) - (1 - y) * np.log(1-output) . TODO: Backpropagate the error . Now it&#39;s your turn to shine. Write the error term. Remember that this is given by the equation $$ (y- hat{y}) sigma&#39;(x) $$ . # TODO: Write the error term formula def error_term_formula(x, y, output): return (y- output)*sigmoid_prime(x) . # Neural Network hyperparameters epochs = 10000 learnrate = 0.4 # Training function def train_nn(features, targets, epochs, learnrate): # Use to same seed to make debugging easier np.random.seed(42) n_records, n_features = features.shape last_loss = None # Initialize weights weights = np.random.normal(scale=1 / n_features**.5, size=n_features) for e in range(epochs): del_w = np.zeros(weights.shape) for x, y in zip(features.values, targets): # Loop through all records, x is the input, y is the target # Activation of the output unit # Notice we multiply the inputs and the weights here # rather than storing h as a separate variable output = sigmoid(np.dot(x, weights)) # The error, the target minus the network output error = error_formula(y, output) # The error term error_term = error_term_formula(x, y, output) # The gradient descent step, the error times the gradient times the inputs del_w += error_term * x # Update the weights here. The learning rate times the # change in weights, divided by the number of records to average weights += learnrate * del_w / n_records # Printing out the mean square error on the training set if e % (epochs / 10) == 0: out = sigmoid(np.dot(features, weights)) loss = np.mean((out - targets) ** 2) print(&quot;Epoch:&quot;, e) if last_loss and last_loss &lt; loss: print(&quot;Train loss: &quot;, loss, &quot; WARNING - Loss Increasing&quot;) else: print(&quot;Train loss: &quot;, loss) last_loss = loss print(&quot;=========&quot;) print(&quot;Finished training!&quot;) return weights weights = train_nn(features, targets, epochs, learnrate) . Epoch: 0 Train loss: 0.27647450760355763 ========= Epoch: 1000 Train loss: 0.20444948086497636 ========= Epoch: 2000 Train loss: 0.2037769486737855 ========= Epoch: 3000 Train loss: 0.20320870213231285 ========= Epoch: 4000 Train loss: 0.20270360082455266 ========= Epoch: 5000 Train loss: 0.20224924706030448 ========= Epoch: 6000 Train loss: 0.20183609708227276 ========= Epoch: 7000 Train loss: 0.20145689935833555 ========= Epoch: 8000 Train loss: 0.20110614373775093 ========= Epoch: 9000 Train loss: 0.2007796176698535 ========= Finished training! . Calculating the Accuracy on the Test Data . # Calculate accuracy on test data test_out = sigmoid(np.dot(features_test, weights)) predictions = test_out &gt; 0.5 accuracy = np.mean(predictions == targets_test) print(&quot;Prediction accuracy: {:.3f}&quot;.format(accuracy)) . Prediction accuracy: 0.650 .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/10/StudentAdmissions.html",
            "relUrl": "/2020/05/10/StudentAdmissions.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Data in NumPy",
            "content": "Python is convenient, but it can also be slow. However, it does allow you to access libraries that execute faster code written in languages like C. NumPy is one such library: it provides fast alternatives to math operations in Python and is designed to work efficiently with groups of numbers - like matrices. . NumPy is a large library and we are only going to scratch the surface of it here. If you plan on doing much math with Python, you should definitely spend some time exploring its documentation to learn more. . Importing NumPy . When importing the NumPy library, the convention you&#39;ll see used most often – including here – is to name it np, like so: . import numpy as np . Now you can use the library by prefixing the names of functions and types with np., which you&#39;ll see in the following examples . Data Types and Shapes . The most common way to work with numbers in NumPy is through ndarray objects. They are similar to Python lists, but can have any number of dimensions. Also, ndarray supports fast math operations, which is just what we want. . Since it can store any number of dimensions, you can use ndarrays to represent any of the data types we covered before: scalars, vectors, matrices, or tensors. . Scalars . Scalars in NumPy are a bit more involved than in Python. Instead of Python’s basic types like int, float, etc., NumPy lets you specify signed and unsigned types, as well as different sizes. So instead of Python’s int, you have access to types like uint8, int8, uint16, int16, and so on. . These types are important because every object you make (vectors, matrices, tensors) eventually stores scalars. And when you create a NumPy array, you can specify the type - but every item in the array must have the same type. In this regard, NumPy arrays are more like C arrays than Python lists. . If you want to create a NumPy array that holds a scalar, you do so by passing the value to NumPy&#39;s array function, like so: . s = np.array(5) . s.shape . () . () means it has zero dimensions. . x = s + 3 x . 8 . Vectors . To create a vector, you&#39;d pass a Python list to the array function, like this: . v = np.array([1,2,3]) v.shape . (3,) . x = v[1] x . 2 . x = v[1:] x . array([2, 3]) . Matrices . You create matrices using NumPy&#39;s array function, just you did for vectors. However, instead of just passing in a list, you need to supply a list of lists, where each list represents a row. So to create a 3x3 matrix containing the numbers one through nine, you could do this: . m = np.array([[1,2,3], [4,5,6], [7,8,9]]) m . array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . m.shape . (3, 3) . m[2][2] . 9 . Tensors . Tensors are just like vectors and matrices, but they can have more dimensions. For example, to create a 3x3x2x1 tensor, you could do the following: . t = np.array([[[[1],[2]],[[3],[4]],[[5],[6]]],[[[7],[8]], [[9],[10]],[[11],[12]]],[[[13],[14]],[[15],[16]],[[17],[17]]]]) t . array([[[[ 1], [ 2]], [[ 3], [ 4]], [[ 5], [ 6]]], [[[ 7], [ 8]], [[ 9], [10]], [[11], [12]]], [[[13], [14]], [[15], [16]], [[17], [17]]]]) . t.shape . (3, 3, 2, 1) . Changing Shapes . Sometimes you&#39;ll need to change the shape of your data without actually changing its contents. For example, you may have a vector, which is one-dimensional, but need a matrix, which is two-dimensional. There are two ways you can do that. . Let&#39;s say you have the following vector: . v = np.array([1,2,3,4]) v.shape . (4,) . x = v.reshape(1,4) x.shape . (1, 4) . x = v.reshape(4,1) x.shape . (4, 1) . One more thing about reshaping NumPy arrays: if you see code from experienced NumPy users, you will often see them use a special slicing syntax instead of calling reshape. Using this syntax, the previous two examples would look like this: . x = v[None, :] x . array([[1, 2, 3, 4]]) . x = v[0:, None] x . array([[1], [2], [3], [4]]) . Those lines create a slice that looks at all of the items of v but asks NumPy to add a new dimension of size 1 for the associated axis. It may look strange to you now, but it&#39;s a common technique so it&#39;s good to be aware of it. . Element-wise operations . The Python way . Suppose you had a list of numbers, and you wanted to add 5 to every item in the list. Without NumPy, you might do something like this: . values = [1,2,3,4,5] for i in range(len(values)): values[i] += 5 values . [6, 7, 8, 9, 10] . The NumPy way . In NumPy, we could do the following: . values = [1,2,3,4,5] values = np.array(values) + 5 values . array([ 6, 7, 8, 9, 10]) . Creating that array may seem odd, but normally you&#39;ll be storing your data in ndarrays anyway. So if you already had an ndarray named values, you could have just done: . values += 5 values . array([11, 12, 13, 14, 15]) . We should point out, NumPy actually has functions for things like adding, multiplying, etc. But it also supports using the standard math operators. So the following two lines are equivalent: . x = np.multiply(values, 5) x = values * 5 x . array([55, 60, 65, 70, 75]) . a = np.array([[1,3],[5,7]]) a . array([[1, 3], [5, 7]]) . b = np.array([[2,4],[6,8]]) b . array([[2, 4], [6, 8]]) . a+b . array([[ 3, 7], [11, 15]]) . Important Reminders About Matrix Multiplication . The number of columns in the left matrix must equal the number of rows in the right matrix. | The answer matrix always has the same number of rows as the left matrix and the same number of columns as the right matrix. | Order matters. Multiplying A•B is not the same as multiplying B•A. | Data in the left matrix should be arranged as rows., while data in the right matrix should be arranged as columns. | . NumPy Matrix Multiplication . You&#39;ve heard a lot about matrix multiplication in the last few videos – now you&#39;ll get to see how to do it with NumPy. However, it&#39;s important to know that NumPy supports several types of matrix multiplication. . Element-wise Multiplication You saw some element-wise multiplication already. You accomplish that with the multiply function or the * operator. Just to revisit, it would look like this: . m = np.array([[1,2,3],[4,5,6]]) m . array([[1, 2, 3], [4, 5, 6]]) . n = m * 0.25 n . array([[0.25, 0.5 , 0.75], [1. , 1.25, 1.5 ]]) . m * n . array([[0.25, 1. , 2.25], [4. , 6.25, 9. ]]) . np.multiply(m, n) . array([[0.25, 1. , 2.25], [4. , 6.25, 9. ]]) . Matrix Product . To find the matrix product, you use NumPy&#39;s matmul function. . If you have compatible shapes, then it&#39;s as simple as this: . a = np.array([[1,2,3,4],[5,6,7,8]]) a . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . a.shape . (2, 4) . b = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]) b . array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) . b.shape . (4, 3) . c = np.matmul(a, b) c . array([[ 70, 80, 90], [158, 184, 210]]) . c.shape . (2, 3) . If your matrices have incompatible shapes, you&#39;ll get an error, like the following: . np.matmul(b, a) . ValueError Traceback (most recent call last) &lt;ipython-input-67-af3b88aa2232&gt; in &lt;module&gt; -&gt; 1 np.matmul(b, a) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . NumPy&#39;s dot function . You may sometimes see NumPy&#39;s dot function in places where you would expect a matmul. It turns out that the results of dot and matmul are the same if the matrices are two dimensional. . So these two results are equivalent: . a = np.array([[1,2],[3,4]]) a . array([[ 7, 10], [15, 22]]) . np.dot(a,a) . array([[ 7, 10], [15, 22]]) . a.dot(a) # you can call `dot` directly on the `ndarray` . array([[ 7, 10], [15, 22]]) . np.matmul(a,a) . array([[ 7, 10], [15, 22]]) . While these functions return the same results for two dimensional data, you should be careful about which you choose when working with other data shapes. You can read more about the differences, and find links to other NumPy functions, in the matmul and dot documentation. . Transpose . Getting the transpose of a matrix is really easy in NumPy. Simply access its T attribute. There is also a transpose() function which returns the same thing, but you’ll rarely see that used anywhere because typing T is so much easier. :) . For example: . m = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) m . array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) . m.T . array([[ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11], [ 4, 8, 12]]) . m.transpose() . array([[ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11], [ 4, 8, 12]]) . m . array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) . NumPy does this without actually moving any data in memory - it simply changes the way it indexes the original matrix - so it’s quite efficient. . However, that also means you need to be careful with how you modify objects, because they are sharing the same data. For example, with the same matrix m from above, let&#39;s make a new variable m_t that stores m&#39;s transpose. Then look what happens if we modify a value in m_t: . m_t = m.T m_t[3][1] = 200 m_t . array([[ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11], [ 4, 200, 12]]) . m . array([[ 1, 2, 3, 4], [ 5, 6, 7, 200], [ 9, 10, 11, 12]]) . Notice how it modified both the transpose and the original matrix, too! That&#39;s because they are sharing the same copy of data. So remember to consider the transpose just as a different view of your matrix, rather than a different matrix entirely. . A real use case . I don&#39;t want to get into too many details about neural networks because you haven&#39;t covered them yet, but there is one place you will almost certainly end up using a transpose, or at least thinking about it. . Let&#39;s say you have the following two matrices, called inputs and weights, . inputs = np.array([[-0.27, 0.45, 0.64, 0.31]]) inputs . array([[-0.27, 0.45, 0.64, 0.31]]) . inputs.shape . (1, 4) . weights = np.array([[0.02, 0.001, -0.03, 0.036], [0.04, -0.003, 0.025, 0.009], [0.012, -0.045, 0.28, -0.067]]) . weights.shape . (3, 4) . weights . array([[ 0.02 , 0.001, -0.03 , 0.036], [ 0.04 , -0.003, 0.025, 0.009], [ 0.012, -0.045, 0.28 , -0.067]]) . I won&#39;t go into what they&#39;re for because you&#39;ll learn about them later, but you&#39;re going to end up wanting to find the matrix product of these two matrices. . If you try it like they are now, you get an error: . np.matmul(inputs, weights) . ValueError Traceback (most recent call last) &lt;ipython-input-88-6e050fb6601d&gt; in &lt;module&gt; -&gt; 1 np.matmul(inputs, weights) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 3 is different from 4) . If you did the matrix multiplication lesson, then you&#39;ve seen this error before. It&#39;s complaining of incompatible shapes because the number of columns in the left matrix, 4, does not equal the number of rows in the right matrix, 3. . So that doesn&#39;t work, but notice if you take the transpose of the weights matrix, it will: . np.matmul(inputs, weights.T).shape . (1, 3) . np.matmul(weights, inputs.T) . array([[-0.01299], [ 0.00664], [ 0.13494]]) . The two answers are transposes of each other, so which multiplication you use really just depends on the shape you want for the output. .",
            "url": "https://manisaiprasad.github.io/notes/2020/05/10/DatainNumPy.html",
            "relUrl": "/2020/05/10/DatainNumPy.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "US Flights of 2020 Jan",
            "content": "# import all packages and set plots to be embedded inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sb %matplotlib inline . Load in your dataset and describe its properties through the questions below. Try and motivate your exploration goals through this section. . #load dataset df = pd.read_csv(&#39;./62598021_T_ONTIME_REPORTING.csv&#39;) df.head() . YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM OP_CARRIER_FL_NUM ORIGIN_AIRPORT_ID ORIGIN_AIRPORT_SEQ_ID ORIGIN_CITY_MARKET_ID ... ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE CARRIER_DELAY WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY Unnamed: 44 . 0 2020 | 1 | 1 | 3 | WN | N951WN | 5888 | 13891 | 1389101 | 32575 | ... | 122.0 | 74.0 | 1.0 | 363.0 | 8.0 | 0.0 | 27.0 | 0.0 | 33.0 | NaN | . 1 2020 | 1 | 1 | 3 | WN | N467WN | 6276 | 13891 | 1389101 | 32575 | ... | 92.0 | 71.0 | 1.0 | 363.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 2 2020 | 1 | 1 | 3 | WN | N7885A | 4598 | 13891 | 1389101 | 32575 | ... | 68.0 | 57.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 3 2020 | 1 | 1 | 3 | WN | N551WN | 4761 | 13891 | 1389101 | 32575 | ... | 75.0 | 63.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 2020 | 1 | 1 | 3 | WN | N968WN | 5162 | 13891 | 1389101 | 32575 | ... | 67.0 | 57.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 45 columns . df.shape . (607346, 45) . df.dtypes . YEAR int64 MONTH int64 DAY_OF_MONTH int64 DAY_OF_WEEK int64 OP_UNIQUE_CARRIER object TAIL_NUM object OP_CARRIER_FL_NUM int64 ORIGIN_AIRPORT_ID int64 ORIGIN_AIRPORT_SEQ_ID int64 ORIGIN_CITY_MARKET_ID int64 ORIGIN object ORIGIN_STATE_ABR object ORIGIN_STATE_FIPS int64 ORIGIN_STATE_NM object DEST_AIRPORT_ID int64 DEST_AIRPORT_SEQ_ID int64 DEST_CITY_MARKET_ID int64 DEST object DEST_STATE_ABR object DEST_STATE_FIPS int64 DEST_STATE_NM object CRS_DEP_TIME int64 DEP_TIME float64 DEP_DELAY float64 DEP_DELAY_NEW float64 TAXI_OUT float64 TAXI_IN float64 CRS_ARR_TIME int64 ARR_TIME float64 ARR_DELAY float64 ARR_DELAY_NEW float64 CANCELLED float64 CANCELLATION_CODE object DIVERTED float64 CRS_ELAPSED_TIME float64 ACTUAL_ELAPSED_TIME float64 AIR_TIME float64 FLIGHTS float64 DISTANCE float64 CARRIER_DELAY float64 WEATHER_DELAY float64 NAS_DELAY float64 SECURITY_DELAY float64 LATE_AIRCRAFT_DELAY float64 Unnamed: 44 float64 dtype: object . df.head(10) . YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM OP_CARRIER_FL_NUM ORIGIN_AIRPORT_ID ORIGIN_AIRPORT_SEQ_ID ORIGIN_CITY_MARKET_ID ... ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE CARRIER_DELAY WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY Unnamed: 44 . 0 2020 | 1 | 1 | 3 | WN | N951WN | 5888 | 13891 | 1389101 | 32575 | ... | 122.0 | 74.0 | 1.0 | 363.0 | 8.0 | 0.0 | 27.0 | 0.0 | 33.0 | NaN | . 1 2020 | 1 | 1 | 3 | WN | N467WN | 6276 | 13891 | 1389101 | 32575 | ... | 92.0 | 71.0 | 1.0 | 363.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 2 2020 | 1 | 1 | 3 | WN | N7885A | 4598 | 13891 | 1389101 | 32575 | ... | 68.0 | 57.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 3 2020 | 1 | 1 | 3 | WN | N551WN | 4761 | 13891 | 1389101 | 32575 | ... | 75.0 | 63.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 2020 | 1 | 1 | 3 | WN | N968WN | 5162 | 13891 | 1389101 | 32575 | ... | 67.0 | 57.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 5 2020 | 1 | 1 | 3 | WN | N7856A | 5684 | 13891 | 1389101 | 32575 | ... | 80.0 | 57.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 6 2020 | 1 | 1 | 3 | WN | N7735A | 6152 | 13891 | 1389101 | 32575 | ... | 72.0 | 62.0 | 1.0 | 333.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 7 2020 | 1 | 1 | 3 | WN | N405WN | 1679 | 13891 | 1389101 | 32575 | ... | 73.0 | 66.0 | 1.0 | 390.0 | 0.0 | 0.0 | 0.0 | 7.0 | 40.0 | NaN | . 8 2020 | 1 | 1 | 3 | WN | N489WN | 3479 | 13891 | 1389101 | 32575 | ... | 100.0 | 73.0 | 1.0 | 390.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 9 2020 | 1 | 1 | 3 | WN | N7708E | 4069 | 13891 | 1389101 | 32575 | ... | 84.0 | 71.0 | 1.0 | 390.0 | NaN | NaN | NaN | NaN | NaN | NaN | . 10 rows × 45 columns . # correcting dates formate df[&#39;DEP_TIME&#39;] = df.DEP_TIME.apply(lambda x: str(int(x)).zfill(4) if pd.notnull(x) else x) df[&#39;CRS_DEP_TIME&#39;] = df.CRS_DEP_TIME.apply(lambda x: str(int(x)).zfill(4) if pd.notnull(x) else x) df[&#39;ARR_TIME&#39;] = df.ARR_TIME.apply(lambda x: str(int(x)).zfill(4) if pd.notnull(x) else x) df[&#39;CRS_ARR_TIME&#39;] = df.CRS_ARR_TIME.apply(lambda x: str(int(x)).zfill(4) if pd.notnull(x) else x) # creating AM/PM in df def setPM(x): if((x&gt;=12) &amp; (x&lt;= 23)): return &#39;PM&#39; elif((x&gt;=0) &amp; (x&lt;= 11) | (x==24)): return &#39;AM&#39; else: return x df.DEP_TIME.dropna(inplace=True) df[&#39;DepTimePM&#39;] = df.DEP_TIME.str[:2].astype(&#39;int64&#39;) df[&#39;DepTimePM&#39;] = df.DepTimePM.apply(setPM) df.DepTimePM.dropna(inplace=True) . df.describe() . YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK OP_CARRIER_FL_NUM ORIGIN_AIRPORT_ID ORIGIN_AIRPORT_SEQ_ID ORIGIN_CITY_MARKET_ID ORIGIN_STATE_FIPS DEST_AIRPORT_ID ... ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE CARRIER_DELAY WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY Unnamed: 44 . count 607346.0 | 607346.0 | 607346.000000 | 607346.000000 | 607346.000000 | 607346.000000 | 6.073460e+05 | 607346.000000 | 607346.000000 | 607346.000000 | ... | 599268.000000 | 599268.000000 | 607346.0 | 607346.000000 | 82285.000000 | 82285.000000 | 82285.000000 | 82285.000000 | 82285.000000 | 0.0 | . mean 2020.0 | 1.0 | 16.014354 | 3.955735 | 2622.365261 | 12657.389167 | 1.265743e+06 | 31761.273269 | 26.876029 | 12657.196320 | ... | 137.039345 | 112.187437 | 1.0 | 798.022341 | 24.696324 | 4.594944 | 14.262733 | 0.091062 | 20.561658 | NaN | . std 0.0 | 0.0 | 8.990719 | 1.910205 | 1822.545302 | 1524.407203 | 1.524405e+05 | 1308.052641 | 16.560267 | 1524.279269 | ... | 72.293510 | 70.629553 | 0.0 | 587.282639 | 72.972359 | 39.180258 | 33.736783 | 2.308003 | 50.370818 | NaN | . min 2020.0 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 10135.000000 | 1.013506e+06 | 30070.000000 | 1.000000 | 10135.000000 | ... | 18.000000 | 8.000000 | 1.0 | 31.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | NaN | . 25% 2020.0 | 1.0 | 8.000000 | 2.000000 | 1070.000000 | 11292.000000 | 1.129202e+06 | 30713.000000 | 12.000000 | 11292.000000 | ... | 84.000000 | 61.000000 | 1.0 | 369.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | NaN | . 50% 2020.0 | 1.0 | 16.000000 | 4.000000 | 2177.000000 | 12889.000000 | 1.288903e+06 | 31453.000000 | 26.000000 | 12889.000000 | ... | 120.000000 | 94.000000 | 1.0 | 641.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | 0.000000 | NaN | . 75% 2020.0 | 1.0 | 24.000000 | 5.000000 | 4108.000000 | 14027.000000 | 1.402702e+06 | 32467.000000 | 42.000000 | 14027.000000 | ... | 168.000000 | 142.000000 | 1.0 | 1037.000000 | 22.000000 | 0.000000 | 19.000000 | 0.000000 | 22.000000 | NaN | . max 2020.0 | 1.0 | 31.000000 | 7.000000 | 6860.000000 | 16869.000000 | 1.686901e+06 | 35991.000000 | 78.000000 | 16869.000000 | ... | 744.000000 | 698.000000 | 1.0 | 5095.000000 | 2489.000000 | 1525.000000 | 1408.000000 | 188.000000 | 2228.000000 | NaN | . 8 rows × 32 columns . What is the structure of your dataset? . There are 607346 flight observations with 45 features in 2020 jan. . What is/are the main feature(s) of interest in your dataset? . delayed and canceled flights in terms fo carriers, locations &amp; time. . What features in the dataset do you think will help support your investigation into your feature(s) of interest? . Arr_Delay, DepDelay, Cancelled, Month, Day_Of_Week, Dep_Time, Arr_Time, OP_UNIQUE_CARRIER, Origin &amp; Dest. . Univariate Exploration . I&#39;ll start by looking at the airlines sizes . #finding airline sizes airlines = df.OP_UNIQUE_CARRIER.value_counts() plt.title(&#39;Airline size&#39;) plt.xlabel(&#39;Airline&#39;) plt.ylabel(&#39;Flights&#39;) plt.bar(airlines.index, airlines); . Flight quantity is different among airlines. . Flights size in each state . #finding flight size in every state plt.figure(figsize=[20,4]) states = df.ORIGIN_STATE_ABR.value_counts() # Add title and axis names plt.title(&#39;Flights size in each state&#39;) plt.xlabel(&#39;States&#39;) plt.ylabel(&#39;Flights&#39;) plt.bar(states.index, states); . Flight quantity is different among airlines. . Flights distributed during days of week . #flights in week width = 1 bins = np.arange(1, 9, width) plt.hist(df.DAY_OF_WEEK, rwidth=.8 , bins=bins) name_days = [&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;] # Add title and axis names plt.title(&#39;Flights distributed during days of week&#39;) plt.xlabel(&#39;Days&#39;) plt.ylabel(&#39;Flights&#39;) plt.xticks(np.arange(1.5,max(bins),width), name_days); . There are little drop of flights in weekend. . Rate of cancelled flights . plt.title(&#39;Rate of cancelled flights&#39;) plt.pie(df.CANCELLED.value_counts(), autopct=&#39;%.2f%%&#39;); . There are 1.14% flights were cancelled. . Distribution of departure delay . plt.figure(figsize=[15,4]) plt.subplot(2,1,1) step = 5 x_min, x_max = -30, 110 bin_edges = np.arange(df.DEP_DELAY.min()-step, df.DEP_DELAY.max()+step, step) plt.hist(df.DEP_DELAY, bins=bin_edges) plt.xlim(x_min,x_max) plt.xticks(np.arange(x_min, x_max, step)) # Add title and axis names plt.title(&#39;Distribution of departure delay&#39;) plt.ylabel(&#39;Flights&#39;) plt.subplot(2,1,2) plt.boxplot(x=df[pd.notnull(df.DEP_DELAY)].DEP_DELAY, vert=False, showfliers=False) plt.xlim(x_min,x_max) plt.xticks(np.arange(x_min, x_max, step)); . There are delayed departures as well as earlier departures and the most of them are between -25 to 25 min differ of the scheduled time. However, there is a flight has been delayed more than 1 day and 17 hours, and also there is a flight had be earlier around 9 hours of the schedule time. . Distribution of arrival delay . plt.figure(figsize=[15,4]) plt.subplot(2,1,1) step = 5 x_min, x_max = -50, 160 bin_edges = np.arange(df.ARR_DELAY.min()-step, df.ARR_DELAY.max()+step, step) plt.hist(df.ARR_DELAY, bins=bin_edges) plt.xlim(x_min, x_max) plt.xticks(np.arange(x_min, x_max, step)) # Add title and axis names plt.title(&#39;Distribution of arrival delay&#39;) plt.ylabel(&#39;Flights&#39;) plt.subplot(2,1,2) plt.boxplot(x=df[pd.notnull(df.ARR_DELAY)].ARR_DELAY, vert=False, showfliers=False) plt.xlim(x_min, x_max) plt.xticks(np.arange(x_min, x_max, step)); . There are delayed arrivals as well as earlier arrivals and the most of them are between -45 to 45 min differ of the scheduled time. . Bivariate Exploration . To start off with, I want to look at relationship among Cannelled, DepDelay, ArrDelay &amp; flight quantity of airlines in terms of airlines . sample = df.groupby(&#39;OP_UNIQUE_CARRIER&#39;) sample = pd.merge( pd.DataFrame(sample.CANCELLED.count()), pd.DataFrame(sample.DEP_DELAY.sum()), on=&#39;OP_UNIQUE_CARRIER&#39; ).merge( pd.DataFrame(sample.ARR_DELAY.sum()), on=&#39;OP_UNIQUE_CARRIER&#39; ) sb.pairplot(sample); . There are positive correlation coefficients among Cannelled, DepDelay, ArrDelay &amp; flight quantity of airlines. . Relationship among Cannelled, DepDelay, ArrDelay &amp; flight quantity of airlines in terms of states . sample = df.groupby(&#39;ORIGIN_STATE_ABR&#39;) sample = pd.merge( pd.DataFrame(sample.CANCELLED.count()), pd.DataFrame(sample.DEP_DELAY.sum()), on=&#39;ORIGIN_STATE_ABR&#39; ).merge( pd.DataFrame(sample.ARR_DELAY.sum()), on=&#39;ORIGIN_STATE_ABR&#39; ) sb.pairplot(sample); . There are positive correlation coefficients among Cannelled, DepDelay, ArrDelay &amp; flight quantity of states. . Relationship between states &amp; airlines . state_airlines = df.groupby([&#39;ORIGIN_STATE_ABR&#39;, &#39;OP_UNIQUE_CARRIER&#39;], as_index=False).count() state_airlines = state_airlines[[&#39;ORIGIN_STATE_ABR&#39;, &#39;OP_UNIQUE_CARRIER&#39;]] plt.figure(figsize=[17,5]) # Add title and axis names plt.title(&#39;Relationship between states &amp; airlines&#39;) plt.xlabel(&#39;States&#39;) plt.ylabel(&#39;Airlines&#39;) plt.scatter(state_airlines.ORIGIN_STATE_ABR, state_airlines.OP_UNIQUE_CARRIER); . The most of airlines reach the most of states. . Cancelled flights for each airline . carriers = df.groupby([&#39;OP_UNIQUE_CARRIER&#39;,&#39;CANCELLED&#39;], as_index=False).count() carriers = carriers.pivot_table(&#39;YEAR&#39;, &#39;OP_UNIQUE_CARRIER&#39;, &#39;CANCELLED&#39;, fill_value=0) carriers.rename(columns={0:&#39;cxl_0&#39;, 1:&#39;cxl_1&#39;}, inplace=True) plt.figure(figsize=[12,10]) carriers plt.subplot(1,4,2) plt.barh(carriers.index, carriers.cxl_0, color=&#39;C0&#39;) plt.subplot(1,4,1) plt.barh(carriers.index, carriers.cxl_1*-1, color=&#39;C1&#39;) plt.yticks(&#39; &#39;) carriers.cxl_0 = np.log10(carriers.cxl_0) carriers.cxl_1 = np.log10(carriers.cxl_1) plt.subplot(1,4,4) plt.barh(carriers.index, carriers.cxl_0, color=&#39;C0&#39;) plt.xlim(0, 7) plt.subplot(1,4,3) plt.barh(carriers.index, carriers.cxl_1*-1, color=&#39;C1&#39;) plt.xlim(-7, 0) plt.yticks(&#39; &#39;); . Normalizing data by log10 make it more informative. . Multivariate Exploration . Create plots of three or more variables to investigate your data even further. Make sure that your investigations are justified, and follow from your work in the previous sections. . Delay and cancelation factors with airlines and their size. . airlines_mean = df.groupby(&#39;OP_UNIQUE_CARRIER&#39;, as_index=False).mean() airlines_count = df.groupby(&#39;OP_UNIQUE_CARRIER&#39;, as_index=False).count() airlines = airlines_mean[[&#39;OP_UNIQUE_CARRIER&#39;,&#39;ARR_DELAY&#39;,&#39;CANCELLED&#39;]].merge( airlines_count[[&#39;OP_UNIQUE_CARRIER&#39;,&#39;FLIGHTS&#39;]], on=&#39;OP_UNIQUE_CARRIER&#39; ) airlines.rename(columns={&#39;FLIGHTS&#39;:&#39;Count&#39;}, inplace=True) airlines[&#39;Size&#39;] = airlines.Count.map(lambda x: round(x/120000)+1) airlines[&#39;Size&#39;] = airlines.Size.map(lambda x: 7 if x&gt;7 else x) plt.scatter(airlines.ARR_DELAY, airlines.CANCELLED, s=airlines.Size**3) x_max = max(airlines.ARR_DELAY) x_min = min(airlines.ARR_DELAY) y_max = max(airlines.CANCELLED) y_min = min(airlines.CANCELLED) x_len = x_max - x_min y_len = y_max - y_min x = x_len*.05 y = y_len*.05 plt.xlim(x_min-x, x_max+x) plt.ylim(y_min-y, y_max+y) plt.axhline(y=y_max-(y_len/2)) plt.axvline(x_max - (x_len/2)) for i, txt in enumerate(airlines.OP_UNIQUE_CARRIER): plt.annotate(txt, (airlines.ARR_DELAY[i], airlines.CANCELLED[i])); . delay factor is impact on the most airlines while cancellation is impact on around all of them. . Delay and cancelation factors with states and flights size . state_mean = df.groupby(&#39;ORIGIN_STATE_ABR&#39;, as_index=False).mean() state_count = df.groupby(&#39;ORIGIN_STATE_ABR&#39;, as_index=False).count() states = state_mean[[&#39;ORIGIN_STATE_ABR&#39;,&#39;ARR_DELAY&#39;,&#39;CANCELLED&#39;]].merge( state_count[[&#39;ORIGIN_STATE_ABR&#39;,&#39;FLIGHTS&#39;]], on=&#39;ORIGIN_STATE_ABR&#39; ) states.rename(columns={&#39;FLIGHTS&#39;:&#39;Count&#39;}, inplace=True) states[&#39;Size&#39;] = states.Count.map(lambda x: round(x/120000)+1) states[&#39;Size&#39;] = states.Size.map(lambda x: 7 if x&gt;7 else x) plt.scatter(states.ARR_DELAY, states.CANCELLED, s=states.Size**3) x_max = max(states.ARR_DELAY) x_min = min(states.ARR_DELAY) y_max = max(states.CANCELLED) y_min = min(states.CANCELLED) x_len = x_max - x_min y_len = y_max - y_min x = x_len*.05 y = y_len*.05 plt.xlim(x_min-x, x_max+x) plt.ylim(y_min-y, y_max+y) plt.axhline(y=y_max-(y_len/2)) plt.axvline(x_max - (x_len/2)) for i, txt in enumerate(states.ORIGIN_STATE_ABR): plt.annotate(txt, (states.ARR_DELAY[i], states.CANCELLED[i])); . There is variation among states in terms of delayed and cancelled flightes and also the quantity of flights. . Delay and cancellation factors with flights in terms of time (AM/PM) . flights_mean = df.groupby([&#39;OP_CARRIER_FL_NUM&#39;,&#39;DepTimePM&#39;], as_index=False).mean() flights_count = df.groupby(&#39;OP_CARRIER_FL_NUM&#39;, as_index=False).count() flights = flights_mean[[&#39;OP_CARRIER_FL_NUM&#39;,&#39;DepTimePM&#39;, &#39;DEP_DELAY&#39;,&#39;CANCELLED&#39;]].merge( flights_count[[&#39;OP_CARRIER_FL_NUM&#39;,&#39;FLIGHTS&#39;]], on=&#39;OP_CARRIER_FL_NUM&#39; ) flights.rename(columns={&#39;FLIGHTS&#39;:&#39;Count&#39;}, inplace=True) Q1 = flights.quantile(.25) Q3 = flights.quantile(.75) IQ = Q3-Q1 Q_max = Q3+IQ*1.5 Q_min = Q1-IQ*1.5 flights_filtered = flights[(flights.DEP_DELAY &gt; Q_min.DEP_DELAY) &amp; (flights.DEP_DELAY &lt; Q_max.DEP_DELAY)] flights_filtered = flights_filtered[ (flights_filtered.Count &gt; Q_min.Count) &amp; (flights_filtered.Count &lt; Q_max.Count) ] flights_no_cxl = flights_filtered[flights_filtered.CANCELLED == 0] flights_filtered = flights_filtered[flights_filtered.CANCELLED &gt; 0] flights_filtered.reset_index(inplace=True) flights_filtered = flights[flights.CANCELLED &gt; 0] sb.scatterplot(flights_filtered.DEP_DELAY, flights_filtered.CANCELLED, alpha=.5, hue=flights_filtered.DepTimePM) # x_max = max(flights_filtered.DepDelay) x_max = 60 x_min = min(flights_filtered.DEP_DELAY) # y_max = max(flights_filtered.Cancelled) y_max = .04 y_min = min(flights_filtered.CANCELLED) x_len = x_max - x_min y_len = y_max - y_min x = x_len*.05 y = y_len*.05 plt.xlim(x_min-x, x_max+x) plt.ylim(y_min-y, y_max+y) plt.axhline(y=y_max-(y_len/2)) plt.axvline(x=x_max - (x_len/2)); . Cancelled flights are in AM flights, while PM flights have more delayed flights. . At the end of your report, make sure that you export the notebook as an html file from the File &gt; Download as... &gt; HTML menu. Make sure you keep track of where the exported file goes, so you can put it in the same folder as this notebook for project submission. Also, make sure you remove all of the quote-formatted guide notes like this one before you finish your report! .",
            "url": "https://manisaiprasad.github.io/notes/2020/04/13/us-flights-data-exploration.html",
            "relUrl": "/2020/04/13/us-flights-data-exploration.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Finding Donors for CharityML",
            "content": "Getting Started . In this project, you will employ several supervised algorithms of your choice to accurately model individuals&#39; income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual&#39;s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual&#39;s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features. . The dataset for this project originates from the UCI Machine Learning Repository. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article &quot;Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid&quot;. You can find the article by Ron Kohavi online. The data we investigate here consists of small changes to the original dataset, such as removing the &#39;fnlwgt&#39; feature and records with missing or ill-formatted entries. . . Exploring the Data . Run the code cell below to load necessary Python libraries and load the census data. Note that the last column from this dataset, &#39;income&#39;, will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database. . # Import libraries necessary for this project import numpy as np import pandas as pd from time import time from IPython.display import display # Allows the use of display() for DataFrames # Import supplementary visualization code visuals.py import visuals as vs # Pretty display for notebooks %matplotlib inline # Load the Census dataset data = pd.read_csv(&quot;census.csv&quot;) # Success - Display the first record display(data.head(n=1)) . age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income . 0 39 | State-gov | Bachelors | 13.0 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174.0 | 0.0 | 40.0 | United-States | &lt;=50K | . Implementation: Data Exploration . A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than $50,000. In the code cell below, you will need to compute the following: . The total number of records, &#39;n_records&#39; | The number of individuals making more than $50,000 annually, &#39;n_greater_50k&#39;. | The number of individuals making at most $50,000 annually, &#39;n_at_most_50k&#39;. | The percentage of individuals making more than $50,000 annually, &#39;greater_percent&#39;. | . HINT: You may need to look at the table above to understand how the &#39;income&#39; entries are formatted. . # TODO: Total number of records n_records = data.shape[0] # TODO: Number of records where individual&#39;s income is more than $50,000 n_greater_50k = data[data.income==&#39;&gt;50K&#39;].shape[0] # TODO: Number of records where individual&#39;s income is at most $50,000 n_at_most_50k = data[data.income==&#39;&lt;=50K&#39;].shape[0] # TODO: Percentage of individuals whose income is more than $50,000 greater_percent = n_greater_50k * 100. / n_records # Print the results print(&quot;Total number of records: {}&quot;.format(n_records)) print(&quot;Individuals making more than $50,000: {}&quot;.format(n_greater_50k)) print(&quot;Individuals making at most $50,000: {}&quot;.format(n_at_most_50k)) print(&quot;Percentage of individuals making more than $50,000: {:.2f}%&quot;.format(greater_percent)) . Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% . Featureset Exploration . age: continuous. | workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. | education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. | education-num: continuous. | marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. | occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. | relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. | race: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. | sex: Female, Male. | capital-gain: continuous. | capital-loss: continuous. | hours-per-week: continuous. | native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands. | . . Preparing the Data . Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as preprocessing. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms. . Transforming Skewed Continuous Features . A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: &#39;capital-gain&#39; and &#39;capital-loss&#39;. . Run the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed. . # Split the data into features and target label income_raw = data[&#39;income&#39;] features_raw = data.drop(&#39;income&#39;, axis = 1) # Visualize skewed continuous features of original data vs.distribution(data) . For highly-skewed feature distributions such as &#39;capital-gain&#39; and &#39;capital-loss&#39;, it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully. . Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. . # Log-transform the skewed features skewed = [&#39;capital-gain&#39;, &#39;capital-loss&#39;] features_log_transformed = pd.DataFrame(data = features_raw) features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1)) # Visualize the new log distributions vs.distribution(features_log_transformed, transformed = True) . Normalizing Numerical Features . In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature&#39;s distribution (such as &#39;capital-gain&#39; or &#39;capital-loss&#39; above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below. . Run the code cell below to normalize each numerical feature. We will use sklearn.preprocessing.MinMaxScaler for this. . # Import sklearn.preprocessing.StandardScaler from sklearn.preprocessing import MinMaxScaler # Initialize a scaler, then apply it to the features scaler = MinMaxScaler() # default=(0, 1) numerical = [&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;] features_log_minmax_transform = pd.DataFrame(data = features_log_transformed) features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical]) # Show an example of a record with scaling applied display(features_log_minmax_transform.head(n = 5)) . age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country . 0 0.301370 | State-gov | Bachelors | 0.800000 | Never-married | Adm-clerical | Not-in-family | White | Male | 0.667492 | 0.0 | 0.397959 | United-States | . 1 0.452055 | Self-emp-not-inc | Bachelors | 0.800000 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0.000000 | 0.0 | 0.122449 | United-States | . 2 0.287671 | Private | HS-grad | 0.533333 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0.000000 | 0.0 | 0.397959 | United-States | . 3 0.493151 | Private | 11th | 0.400000 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0.000000 | 0.0 | 0.397959 | United-States | . 4 0.150685 | Private | Bachelors | 0.800000 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0.000000 | 0.0 | 0.397959 | Cuba | . Implementation: Data Preprocessing . From the table in Exploring the Data above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables) be converted. One popular way to convert categorical variables is by using the one-hot encoding scheme. One-hot encoding creates a &quot;dummy&quot; variable for each possible category of each non-numeric feature. For example, assume someFeature has three possible entries: A, B, or C. We then encode this feature into someFeature_A, someFeature_B and someFeature_C. . someFeature someFeature_A someFeature_B someFeature_C . 0 | B | | 0 | 1 | 0 | . 1 | C | -&gt; one-hot encode -&gt; | 0 | 0 | 1 | . 2 | A | | 1 | 0 | 0 | . Additionally, as with the non-numeric features, we need to convert the non-numeric target label, &#39;income&#39; to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (&quot;&lt;=50K&quot; and &quot;&gt;50K&quot;), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1, respectively. In code cell below, you will need to implement the following: . Use pandas.get_dummies() to perform one-hot encoding on the &#39;features_log_minmax_transform&#39; data. | Convert the target label &#39;income_raw&#39; to numerical entries. Set records with &quot;&lt;=50K&quot; to 0 and records with &quot;&gt;50K&quot; to 1. | . | . # TODO: One-hot encode the &#39;features_log_minmax_transform&#39; data using pandas.get_dummies() features_final = pd.get_dummies(features_raw) # TODO: Encode the &#39;income_raw&#39; data to numerical values income = income_raw.apply(lambda x: 1 if x == &#39;&gt;50K&#39; else 0) # Print the number of features after one-hot encoding encoded = list(features_final.columns) print(&quot;{} total features after one-hot encoding.&quot;.format(len(encoded))) # Uncomment the following line to see the encoded feature names print(encoded) . 103 total features after one-hot encoding. [&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_level_ 10th&#39;, &#39;education_level_ 11th&#39;, &#39;education_level_ 12th&#39;, &#39;education_level_ 1st-4th&#39;, &#39;education_level_ 5th-6th&#39;, &#39;education_level_ 7th-8th&#39;, &#39;education_level_ 9th&#39;, &#39;education_level_ Assoc-acdm&#39;, &#39;education_level_ Assoc-voc&#39;, &#39;education_level_ Bachelors&#39;, &#39;education_level_ Doctorate&#39;, &#39;education_level_ HS-grad&#39;, &#39;education_level_ Masters&#39;, &#39;education_level_ Preschool&#39;, &#39;education_level_ Prof-school&#39;, &#39;education_level_ Some-college&#39;, &#39;marital-status_ Divorced&#39;, &#39;marital-status_ Married-AF-spouse&#39;, &#39;marital-status_ Married-civ-spouse&#39;, &#39;marital-status_ Married-spouse-absent&#39;, &#39;marital-status_ Never-married&#39;, &#39;marital-status_ Separated&#39;, &#39;marital-status_ Widowed&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;relationship_ Husband&#39;, &#39;relationship_ Not-in-family&#39;, &#39;relationship_ Other-relative&#39;, &#39;relationship_ Own-child&#39;, &#39;relationship_ Unmarried&#39;, &#39;relationship_ Wife&#39;, &#39;race_ Amer-Indian-Eskimo&#39;, &#39;race_ Asian-Pac-Islander&#39;, &#39;race_ Black&#39;, &#39;race_ Other&#39;, &#39;race_ White&#39;, &#39;sex_ Female&#39;, &#39;sex_ Male&#39;, &#39;native-country_ Cambodia&#39;, &#39;native-country_ Canada&#39;, &#39;native-country_ China&#39;, &#39;native-country_ Columbia&#39;, &#39;native-country_ Cuba&#39;, &#39;native-country_ Dominican-Republic&#39;, &#39;native-country_ Ecuador&#39;, &#39;native-country_ El-Salvador&#39;, &#39;native-country_ England&#39;, &#39;native-country_ France&#39;, &#39;native-country_ Germany&#39;, &#39;native-country_ Greece&#39;, &#39;native-country_ Guatemala&#39;, &#39;native-country_ Haiti&#39;, &#39;native-country_ Holand-Netherlands&#39;, &#39;native-country_ Honduras&#39;, &#39;native-country_ Hong&#39;, &#39;native-country_ Hungary&#39;, &#39;native-country_ India&#39;, &#39;native-country_ Iran&#39;, &#39;native-country_ Ireland&#39;, &#39;native-country_ Italy&#39;, &#39;native-country_ Jamaica&#39;, &#39;native-country_ Japan&#39;, &#39;native-country_ Laos&#39;, &#39;native-country_ Mexico&#39;, &#39;native-country_ Nicaragua&#39;, &#39;native-country_ Outlying-US(Guam-USVI-etc)&#39;, &#39;native-country_ Peru&#39;, &#39;native-country_ Philippines&#39;, &#39;native-country_ Poland&#39;, &#39;native-country_ Portugal&#39;, &#39;native-country_ Puerto-Rico&#39;, &#39;native-country_ Scotland&#39;, &#39;native-country_ South&#39;, &#39;native-country_ Taiwan&#39;, &#39;native-country_ Thailand&#39;, &#39;native-country_ Trinadad&amp;Tobago&#39;, &#39;native-country_ United-States&#39;, &#39;native-country_ Vietnam&#39;, &#39;native-country_ Yugoslavia&#39;] . Shuffle and Split Data . Now all categorical variables have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing. . Run the code cell below to perform this split. . # Import train_test_split from sklearn.cross_validation import train_test_split # Split the &#39;features&#39; and &#39;income&#39; data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features_final, income, test_size = 0.2, random_state = 0) # Show the results of the split print(&quot;Training set has {} samples.&quot;.format(X_train.shape[0])) print(&quot;Testing set has {} samples.&quot;.format(X_test.shape[0])) . Training set has 36177 samples. Testing set has 9045 samples. . . Evaluating Model Performance . In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor. . Metrics and the Naive Predictor . CharityML, equipped with their research, knows individuals that make more than $50,000 are most likely to donate to their charity. Because of this, *CharityML* is particularly interested in predicting who makes more than $50,000 accurately. It would seem that using accuracy as a metric for evaluating a particular model&#39;s performace would be appropriate. Additionally, identifying someone that does not make more than $50,000 as someone who does would be detrimental to *CharityML*, since they are looking to find individuals willing to donate. Therefore, a model&#39;s ability to precisely predict those that make more than $50,000 is more important than the model&#39;s ability to recall those individuals. We can use F-beta score as a metric that considers both precision and recall: . $$ F_{ beta} = (1 + beta^2) cdot frac{precision cdot recall}{ left( beta^2 cdot precision right) + recall} $$ . In particular, when $ beta = 0.5$, more emphasis is placed on precision. This is called the F$_{0.5}$ score (or F-score for simplicity). . Looking at the distribution of classes (those who make at most $50,000, and those who make more), it&#39;s clear most individuals do not make more than $50,000. This can greatly affect accuracy, since we could simply say &quot;this person does not make more than $50,000&quot; and generally be right, without ever looking at the data! Making such a statement would be called naive, since we have not considered any information to substantiate the claim. It is always important to consider the naive prediction for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than $50,000, CharityML would identify no one as donors. . Note: Recap of accuracy, precision, recall . Accuracy measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points). . Precision tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of . [True Positives/(True Positives + False Positives)] . Recall(sensitivity) tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of . [True Positives/(True Positives + False Negatives)] . For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren&#39;t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios). . Question 1 - Naive Predictor Performace . If we chose a model that always predicted an individual made more than $50,000, what would that model&#39;s accuracy and F-score be on this dataset? You must use the code cell below and assign your results to &#39;accuracy&#39; and &#39;fscore&#39; to be used later. | . Please note that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place you could start from. . HINT: . When we have a model that always predicts &#39;1&#39; (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative(&#39;0&#39; value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives/(True Positives + False Positives)) as every prediction that we have made with value &#39;1&#39; that should have &#39;0&#39; becomes a False Positive; therefore our denominator in this case is the total number of records we have in total. | Our Recall score(True Positives/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives. | . &#39;&#39;&#39; TP = np.sum(income) # Counting the ones as this is the naive case. Note that &#39;income&#39; is the &#39;income_raw&#39; data encoded to numerical values done in the data preprocessing step. FP = income.count() - TP # Specific to the naive case TN = 0 # No predicted negatives in the naive case FN = 0 # No predicted negatives in the naive case &#39;&#39;&#39; # TODO: Calculate F-score using the formula above for beta = 0.5 accuracy = greater_percent TP = n_greater_50k FP = n_at_most_50k FN = 0 TN = 0 beta = 0.5 precision = TP*1. / (TP + FP) recall = TP*1. / (TP + FN) fscore = (1 + beta**2)*precision*recall/(beta**2 * precision+recall) # Print the results print(&quot;Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]&quot;.format(accuracy, fscore)) . Naive Predictor: [Accuracy score: 24.7844, F-score: 0.2917] . Supervised Learning Models . The following are some of the supervised learning models that are currently available in scikit-learn that you may choose from: . Gaussian Naive Bayes (GaussianNB) | Decision Trees | Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting) | K-Nearest Neighbors (KNeighbors) | Stochastic Gradient Descent Classifier (SGDC) | Support Vector Machines (SVM) | Logistic Regression | . Question 2 - Model Application . List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen . Describe one real-world application in industry where the model can be applied. | What are the strengths of the model; when does it perform well? | What are the weaknesses of the model; when does it perform poorly? | What makes this model a good candidate for the problem, given what you know about the data? | . HINT: . Structure your answer in the same format as above^, with 4 parts for each of the three models you pick. Please include references with your answer. . Answer: . Support Vector Machines (SVM): . Real World Example: . https://papers.nips.cc/paper/2179-mismatch-string-kernels-for-svm-protein-classification.pdf . It is a pretty common model in the biological science, for example used in protein classification. Here is the paper: . What are the strengths of the model Thanks to kernel feature it is very flexible, we can fit linear and non-linear data. When there are many features SVM is able to separate classes quickly and with less overfitting. . What are the weaknesses of the model The training time is slower than in other models. . What makes this model a good candidate for the problem, given what you know about the data? We have a small data set, just 40000 rows. Therefore, training time will be slower but not to the point of being a problem. SVM will also have less overfitting problems this high dimensional data. The possibility of choosing different kernels will allow us to train linear and non-linear models during hyper parameters tune, so we will have both possibilities covered. . Gaussian Naive Bayes (GaussianNB): . Real World Example: https://wiki.apache.org/spamassassin/BayesInSpamAssassin This model is used in one of the most famous AntiSpam products available, SpamAssassin. A part from a large list of fixed rules to classify spam essages, it used bayes classification. . What are the strengths of the model No overfitting problems. It is fast and performs well in small datasets. It has less parameters to tune, so the hyper parameter tune is faster. . What are the weaknesses of the model It assumes independence between every pair of features. . What makes this model a good candidate for the problem, given what you know about the data? It is one of the models that performs well when we don&#39;t have many training points, like in this particular case. We avoid overfitting, hyper parameter tune will be fast. Its weakness, independence assumption, is also rarely true in real world data. . Random Forest: . Real World Example: http://www.nature.com/modpathol/journal/v18/n4/full/3800322a.html A part from being one of most used models in Kaggle, one real life example we can find is in Tumor Classification: . What are the strengths of the model Very simple and easy to understand, we are not dealing with a black box. It runs multiple decission trees, that eliminates the overfitting problem. It will allow us to check feature imporance. They can represent non linear decision boundaries. It is not affected by noisy features. Lower variance. . What are the weaknesses of the model The bias of the model increases a bit in comparison with a single decision tree. Usually compensated by the lower variance. Depending on the number of trees it runs, the training time could increase a lot. . What makes this model a good candidate for the problem, given what you know about the data? It fits well linear and non linear models. We don&#39;t have many data points, so it will run fast and we can use a large number of trees. . Implementation - Creating a Training and Predicting Pipeline . To properly evaluate the performance of each model you&#39;ve chosen, it&#39;s important that you create a training and predicting pipeline that allows you to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. Your implementation here will be used in the following section. In the code block below, you will need to implement the following: . Import fbeta_score and accuracy_score from sklearn.metrics. | Fit the learner to the sampled training data and record the training time. | Perform predictions on the test data X_test, and also on the first 300 training points X_train[:300]. Record the total prediction time. | . | Calculate the accuracy score for both the training subset and testing set. | Calculate the F-score for both the training subset and testing set. Make sure that you set the beta parameter! | . | . # TODO: Import two metrics from sklearn - fbeta_score and accuracy_score from sklearn.metrics import fbeta_score from sklearn.metrics import accuracy_score def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): &#39;&#39;&#39; inputs: - learner: the learning algorithm to be trained and predicted on - sample_size: the size of samples (number) to be drawn from training set - X_train: features training set - y_train: income training set - X_test: features testing set - y_test: income testing set &#39;&#39;&#39; results = {} # TODO: Fit the learner to the training data using slicing with &#39;sample_size&#39; start = time() # Get start time learner = learner.fit(X_train[:sample_size],y_train[:sample_size]) end = time() # Get end time # TODO: Calculate the training time results[&#39;train_time&#39;] = end - start # TODO: Get the predictions on the test set, # then get predictions on the first 300 training samples start = time() # Get start time predictions_test = learner.predict(X_test) predictions_train = learner.predict(X_train[:300]) end = time() # Get end time # TODO: Calculate the total prediction time results[&#39;pred_time&#39;] = end - start # TODO: Compute accuracy on the first 300 training samples results[&#39;acc_train&#39;] = accuracy_score(y_train[:300],predictions_train) # TODO: Compute accuracy on test set results[&#39;acc_test&#39;] = accuracy_score(y_test,predictions_test) # TODO: Compute F-score on the the first 300 training samples results[&#39;f_train&#39;] = fbeta_score(y_train[:300],predictions_train,beta=0.5) # TODO: Compute F-score on the test set results[&#39;f_test&#39;] = fbeta_score(y_test,predictions_test,beta=0.5) # Success print (&quot;{} trained on {} samples.&quot;.format(learner.__class__.__name__, sample_size)) # Return the results return results . Implementation: Initial Model Evaluation . In the code cell, you will need to implement the following: . Import the three supervised learning models you&#39;ve discussed in the previous section. | Initialize the three models and store them in &#39;clf_A&#39;, &#39;clf_B&#39;, and &#39;clf_C&#39;. Use a &#39;random_state&#39; for each model you use, if provided. | Note: Use the default settings for each model — you will tune one specific model in a later section. | . | Calculate the number of records equal to 1%, 10%, and 100% of the training data. Store those values in &#39;samples_1&#39;, &#39;samples_10&#39;, and &#39;samples_100&#39; respectively. | . | . Note: Depending on which algorithms you chose, the following implementation may take some time to run! . # TODO: Import the three supervised learning models from sklearn from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn import svm # TODO: Initialize the three models clf_A = svm.SVC(random_state=50) clf_B = GaussianNB() clf_C = RandomForestClassifier(random_state=50) # TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data samples_1 = int(len(X_train)*0.01) samples_10 = int(len(X_train)*0.1) samples_100 = int(len(X_train)) # Collect results on the learners results = {} for clf in [clf_A, clf_B, clf_C]: clf_name = clf.__class__.__name__ results[clf_name] = {} for i, samples in enumerate([samples_1, samples_10, samples_100]): results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test) # Run metrics visualization for the three supervised learning models chosen vs.evaluate(results, accuracy, fscore) . /opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples. &#39;precision&#39;, &#39;predicted&#39;, average, warn_for) . SVC trained on 361 samples. SVC trained on 3617 samples. SVC trained on 36177 samples. GaussianNB trained on 361 samples. GaussianNB trained on 3617 samples. GaussianNB trained on 36177 samples. RandomForestClassifier trained on 361 samples. RandomForestClassifier trained on 3617 samples. RandomForestClassifier trained on 36177 samples. . . Improving Results . In this final section, you will choose from the three supervised learning models the best model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (X_train and y_train) by tuning at least one parameter to improve upon the untuned model&#39;s F-score. . Question 3 - Choosing the Best Model . Based on the evaluation you performed earlier, in one to two paragraphs, explain to CharityML which of the three models you believe to be most appropriate for the task of identifying individuals that make more than $50,000. | . HINT: Look at the graph at the bottom left from the cell above(the visualization created by vs.evaluate(results, accuracy, fscore)) and check the F score for the testing set when 100% of the training set is used. Which model has the highest score? Your answer should include discussion of the: . metrics - F score on the testing when 100% of the training data is used, | prediction/training time | the algorithm&#39;s suitability for the data. | . Answer: From the three models I have evaluated, we can discard Naives Bayes. It has a pretty log F-score and precision. It is actually fast doing training and prediction, but those low scores shows that it is clearly not an option. . RandomForest shows a really high f-score in the training set, nearly 1. While on testing set it gets down to 0.6. This model seems to be overfitting. . SVM is slower doing the training, but we don&#39;t have many rows anyway. The score and f-score on testing set is similar to RandomForest, but without overfitting problem. Therefore, SVM is the choosen model. . Question 4 - Describing the Model in Layman&#39;s Terms . In one to two paragraphs, explain to CharityML, in layman&#39;s terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical jargon, such as describing equations. | . HINT: . When explaining your model, if using external resources please include all citations. . Answer: . SVM (Support Vector Machine) is a representation of the linear model: . The method works by first training the model, then using the trained model to predict. . Training is done by reading census data from each individual. For example, marital status, ocuppation, age, sex and so on. That information is used to generate the function that separates the data between those who earn more then 50k and those who earn less. The idea is to find a line that best separates both groups, with the largest margin possible. It is not always possible to separate the data with a straight line. In those cases more complex functions are needed. After we train the model, the best possible function that separates the data is created. Now, the model can use that function to find what would be the salary of a person never seen before, based on his/her census data. The model will place a point in the graph. Then, to predict the salary it just needs to check the position of that point. Taking the first graph as reference, depending if the new point is on the left or the right, it will be a &quot;star&quot; or a &quot;circle&quot;. In our case, it will be &quot;&gt;50k&quot; or &quot;&lt;50k&quot;. . Implementation: Model Tuning . Fine tune the chosen model. Use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following: . Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer. | Initialize the classifier you&#39;ve chosen and store it in clf. Set a random_state if one is available to the same state you set before. | . | Create a dictionary of parameters you wish to tune for the chosen model. Example: parameters = {&#39;parameter&#39; : [list of values]}. | Note: Avoid tuning the max_features parameter of your learner if that parameter is available! | . | Use make_scorer to create an fbeta_score scoring object (with $ beta = 0.5$). | Perform grid search on the classifier clf using the &#39;scorer&#39;, and store it in grid_obj. | Fit the grid search object to the training data (X_train, y_train), and store it in grid_fit. | . Note: Depending on the algorithm chosen and the parameter list, the following implementation may take some time to run! . # TODO: Import &#39;GridSearchCV&#39;, &#39;make_scorer&#39;, and any other necessary libraries from sklearn import svm from sklearn import grid_search from sklearn.metrics import make_scorer # TODO: Initialize the classifier clf = svm.SVC() # TODO: Create the parameters list you wish to tune parameters = {&#39;C&#39;: [1,2,3]} # TODO: Make an fbeta_score scoring object scorer = make_scorer(fbeta_score, beta=0.5) # TODO: Perform grid search on the classifier using &#39;scorer&#39; as the scoring method grid_obj = grid_search.GridSearchCV(clf, parameters, scoring=scorer) # TODO: Fit the grid search object to the training data and find the optimal parameters grid_fit = grid_obj.fit(X_train, y_train) # Get the estimator best_clf = grid_fit.best_estimator_ # Make predictions using the unoptimized and model predictions = (clf.fit(X_train, y_train)).predict(X_test) best_predictions = best_clf.predict(X_test) # Report the before-and-afterscores print(&quot;Unoptimized model n&quot;) print(&quot;Accuracy score on testing data: {:.4f}&quot;.format(accuracy_score(y_test, predictions))) print(&quot;F-score on testing data: {:.4f}&quot;.format(fbeta_score(y_test, predictions, beta = 0.5))) print(&quot; nOptimized Model n&quot;) print(&quot;Final accuracy score on the testing data: {:.4f}&quot;.format(accuracy_score(y_test, best_predictions))) print(&quot;Final F-score on the testing data: {:.4f}&quot;.format(fbeta_score(y_test, best_predictions, beta = 0.5))) . /opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20. DeprecationWarning) . Unoptimized model Accuracy score on testing data: 0.8371 F-score on testing data: 0.6745 Optimized Model Final accuracy score on the testing data: 0.8371 Final F-score on the testing data: 0.6745 . Question 5 - Final Model Evaluation . What is your optimized model&#39;s accuracy and F-score on the testing data? | Are these scores better or worse than the unoptimized model? | How do the results from your optimized model compare to the naive predictor benchmarks you found earlier in Question 1?_ | . Note: Fill in the table below with your results, and then provide discussion in the Answer box. . Results: . Metric Unoptimized Model Optimized Model . Accuracy Score | 0.8371 | 0.837 | . F-score | 0.6745 | 0.6745 | . Answer: . The scores we get are a bit better. We have higher accuracy and F-Score. That means that our model will be able do better predicting the correct labels and reducing the changes of false positives at the same time. If we compare them with the naive predictor the difference is much larger, making our Benchmark Predictor almost useless even if we compare it with the Random Forest Unoptimized Model. . . Feature Importance . An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than $50,000. . Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a feature_importance_ attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset. . Question 6 - Feature Relevance Observation . When Exploring the Data, it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why? . Answer: age occupation education_level workclass hours-per-week Age is one of the most importants. Age means experience, better positions in the company, getting promotions, so more age could mean more salary too. . The salary also depends on the occupation, so that makes it the second place. For two people on same occupation, the older one will probably get higher salary because of the previous experience and role . In third place there should be education_level, because the education you get doesn&#39;t always predict your future job or the salary you could get. Having a particular education level could help you to get to a good occupation, so that is why education_level goes to third place. . In fourth place a category similar to occupation, workclass. Depending on which group the employee is the salary could vary. For example, a Federal-gov could make more than a Self-emp-inc. But it doesn&#39;t always have to the the case. Since the separation of class and salary (mostly because of the classes in the data set) is not as well defined as occupation, this one goes to 4th and occupation stays in 2nd. . Finally, the number of hours-per-week can increase the salary received if those hours are paid as an extra, something that doesn&#39;t happen in all situations but could make a difference. . Implementation - Extracting Feature Importance . Choose a scikit-learn supervised learning algorithm that has a feature_importance_ attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm. . In the code cell below, you will need to implement the following: . Import a supervised learning model from sklearn if it is different from the three used earlier. | Train the supervised model on the entire training set. | Extract the feature importances using &#39;.feature_importances_&#39;. | . # TODO: Import a supervised learning model that has &#39;feature_importances_&#39; from sklearn.ensemble import AdaBoostClassifier # TODO: Train the supervised model on the training set model = AdaBoostClassifier() model.fit(X_train, y_train) # TODO: Extract the feature importances importances = model.feature_importances_ # Plot vs.feature_plot(importances, X_train, y_train) . Question 7 - Extracting Feature Importance . Observe the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above $50,000. . How do these five features compare to the five features you discussed in Question 6? | If you were close to the same answer, how does this visualization confirm your thoughts? | If you were not close, why do you think these features are more relevant? | . Answer: . If we count education-num as a variation of education-level, I got 3 of 5. Those that I didn&#39;t take in account, capital-loss and capital-gain were good ones that I forgot. The capital of a person, meassured by what are the incomes and expenses, could probably describe the salary that person gets. So, those two are pretty good features to take in account. . It is important to mention that our choosen model has been SVM, but it doesn&#39;t have the possibility to extract the feature importance. Therefore, this data may not be the best one for our model. We need to test it to see if it does better or not. . Feature Selection . How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower — at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set with only the top five important features. . # Import functionality for cloning a model from sklearn.base import clone # Reduce the feature space X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]] X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]] # Train on the &quot;best&quot; model found from grid search earlier clf = (clone(best_clf)).fit(X_train_reduced, y_train) # Make new predictions reduced_predictions = clf.predict(X_test_reduced) # Report scores from the final model using both versions of data print(&quot;Final Model trained on full data n&quot;) print(&quot;Accuracy on testing data: {:.4f}&quot;.format(accuracy_score(y_test, best_predictions))) print(&quot;F-score on testing data: {:.4f}&quot;.format(fbeta_score(y_test, best_predictions, beta = 0.5))) print(&quot; nFinal Model trained on reduced data n&quot;) print(&quot;Accuracy on testing data: {:.4f}&quot;.format(accuracy_score(y_test, reduced_predictions))) print(&quot;F-score on testing data: {:.4f}&quot;.format(fbeta_score(y_test, reduced_predictions, beta = 0.5))) . Final Model trained on full data Accuracy on testing data: 0.8371 F-score on testing data: 0.6745 Final Model trained on reduced data Accuracy on testing data: 0.7992 F-score on testing data: 0.5493 . Question 8 - Effects of Feature Selection . How does the final model&#39;s F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used? | If training time was a factor, would you consider using the reduced data as your training set? | . Answer: . Training time is a factor. SVM is much slower than the other models and that&#39;s make hyperparameter optimization slower than expected. Would be good to reduce the data in the training set, for example doing a .6 split instead of .2. . Also, the dataset with reduced number of features performs worse than using the full data. Accuracy and f-score is lower. Something normal taking in account that we used Adaboost with DecissionTree as base predictor to extract feature importances and then SVM to do the actual training. It could be possible to use SVM as base predictor, but taking in account the time it takes, could be too slow. . Note:Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -&gt; Download as -&gt; HTML (.html). Include the finished document along with this notebook as your submission. . Before You Submit . You will also need run the following in order to convert the Jupyter notebook into HTML, so that your submission will include both files. . !!jupyter nbconvert *.ipynb . [&#39;[NbConvertApp] Converting notebook finding_donors.ipynb to html&#39;, &#39;[NbConvertApp] Writing 496082 bytes to finding_donors.html&#39;] .",
            "url": "https://manisaiprasad.github.io/notes/2020/04/09/finding_donors.html",
            "relUrl": "/2020/04/09/finding_donors.html",
            "date": " • Apr 9, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Implementing the Gradient Descent Algorithm",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd #Some helper functions for plotting and drawing lines def plot_points(X, y): admitted = X[np.argwhere(y==1)] rejected = X[np.argwhere(y==0)] plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = &#39;blue&#39;, edgecolor = &#39;k&#39;) plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = &#39;red&#39;, edgecolor = &#39;k&#39;) def display(m, b, color=&#39;g--&#39;): plt.xlim(-0.05,1.05) plt.ylim(-0.05,1.05) x = np.arange(-10, 10, 0.1) plt.plot(x, m*x+b, color) . Reading and plotting the data . data = pd.read_csv(&#39;data.csv&#39;, header=None) X = np.array(data[[0,1]]) y = np.array(data[2]) plot_points(X,y) plt.show() . TODO: Implementing the basic functions . Here is your turn to shine. Implement the following formulas, as explained in the text. . Sigmoid activation function | . $$ sigma(x) = frac{1}{1+e^{-x}}$$ . Output (prediction) formula | . $$ hat{y} = sigma(w_1 x_1 + w_2 x_2 + b)$$ . Error function | . $$Error(y, hat{y}) = - y log( hat{y}) - (1-y) log(1- hat{y})$$ . The function that updates the weights | . $$ w_i longrightarrow w_i + alpha (y - hat{y}) x_i$$ . $$ b longrightarrow b + alpha (y - hat{y})$$ . # Activation (sigmoid) function def sigmoid(x): return 1 / (1 + np.exp(-x)) def output_formula(features, weights, bias): return sigmoid(np.dot(features, weights) + bias) def error_formula(y, output): return - y*np.log(output) - (1 - y) * np.log(1-output) def update_weights(x, y, weights, bias, learnrate): output = output_formula(x, weights, bias) d_error = y - output weights += learnrate * d_error * x bias += learnrate * d_error return weights, bias . Training function . This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm. . np.random.seed(44) epochs = 100 learnrate = 0.01 def train(features, targets, epochs, learnrate, graph_lines=False): errors = [] n_records, n_features = features.shape last_loss = None weights = np.random.normal(scale=1 / n_features**.5, size=n_features) bias = 0 for e in range(epochs): del_w = np.zeros(weights.shape) for x, y in zip(features, targets): output = output_formula(x, weights, bias) error = error_formula(y, output) weights, bias = update_weights(x, y, weights, bias, learnrate) # Printing out the log-loss error on the training set out = output_formula(features, weights, bias) loss = np.mean(error_formula(targets, out)) errors.append(loss) if e % (epochs / 10) == 0: print(&quot; n========== Epoch&quot;, e,&quot;==========&quot;) if last_loss and last_loss &lt; loss: print(&quot;Train loss: &quot;, loss, &quot; WARNING - Loss Increasing&quot;) else: print(&quot;Train loss: &quot;, loss) last_loss = loss predictions = out &gt; 0.5 accuracy = np.mean(predictions == targets) print(&quot;Accuracy: &quot;, accuracy) if graph_lines and e % (epochs / 100) == 0: display(-weights[0]/weights[1], -bias/weights[1]) # Plotting the solution boundary plt.title(&quot;Solution boundary&quot;) display(-weights[0]/weights[1], -bias/weights[1], &#39;black&#39;) # Plotting the data plot_points(features, targets) plt.show() # Plotting the error plt.title(&quot;Error Plot&quot;) plt.xlabel(&#39;Number of epochs&#39;) plt.ylabel(&#39;Error&#39;) plt.plot(errors) plt.show() . Time to train the algorithm! . When we run the function, we&#39;ll obtain the following: . 10 updates with the current training loss and accuracy | A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs. | A plot of the error function. Notice how it decreases as we go through more epochs. | . train(X, y, epochs, learnrate, True) . ========== Epoch 0 ========== Train loss: 0.713584519538 Accuracy: 0.4 ========== Epoch 10 ========== Train loss: 0.622583521045 Accuracy: 0.59 ========== Epoch 20 ========== Train loss: 0.554874408367 Accuracy: 0.74 ========== Epoch 30 ========== Train loss: 0.501606141872 Accuracy: 0.84 ========== Epoch 40 ========== Train loss: 0.459333464186 Accuracy: 0.86 ========== Epoch 50 ========== Train loss: 0.425255434335 Accuracy: 0.93 ========== Epoch 60 ========== Train loss: 0.397346157167 Accuracy: 0.93 ========== Epoch 70 ========== Train loss: 0.374146976524 Accuracy: 0.93 ========== Epoch 80 ========== Train loss: 0.354599733682 Accuracy: 0.94 ========== Epoch 90 ========== Train loss: 0.337927365888 Accuracy: 0.94 .",
            "url": "https://manisaiprasad.github.io/notes/2020/02/20/GradientDescent.html",
            "relUrl": "/2020/02/20/GradientDescent.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://manisaiprasad.github.io/notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Build a Traffic Sign Recognition Classifier",
            "content": ". Step 0: Load The Data . # Load pickled data import pickle # TODO: Fill this in based on where you saved the training and testing data training_file = &quot;train.p&quot; validation_file= &quot;valid.p&quot; testing_file = &quot;test.p&quot; with open(training_file, mode=&#39;rb&#39;) as f: train = pickle.load(f) with open(validation_file, mode=&#39;rb&#39;) as f: valid = pickle.load(f) with open(testing_file, mode=&#39;rb&#39;) as f: test = pickle.load(f) X_train, y_train = train[&#39;features&#39;], train[&#39;labels&#39;] X_valid, y_valid = valid[&#39;features&#39;], valid[&#39;labels&#39;] X_test, y_test = test[&#39;features&#39;], test[&#39;labels&#39;] . . Step 1: Dataset Summary &amp; Exploration . The pickled data is a dictionary with 4 key/value pairs: . &#39;features&#39; is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels). | &#39;labels&#39; is a 1D array containing the label/class id of the traffic sign. The file signnames.csv contains id -&gt; name mappings for each id. | &#39;sizes&#39; is a list containing tuples, (width, height) representing the the original width and height the image. | &#39;coords&#39; is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES | . Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the pandas shape method might be useful for calculating some of the summary results. . Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas . ### Replace each question mark with the appropriate value. ### Use python, pandas or numpy methods rather than hard coding the results import numpy as np # Number of training examples n_train = X_train.shape[0] # Number of testing examples. n_test = X_test.shape[0] # What&#39;s the shape of an traffic sign image? image_shape = X_train.shape[1:] # How many unique classes/labels there are in the dataset. n_classes = len(np.unique(y_train)) print(&quot;Number of training examples =&quot;, n_train) print(&quot;Number of testing examples =&quot;, n_test) print(&quot;Image data shape =&quot;, image_shape) print(&quot;Number of classes =&quot;, n_classes) . Number of training examples = 34799 Number of testing examples = 12630 Image data shape = (32, 32, 3) Number of classes = 43 . Include an exploratory visualization of the dataset . Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. . The Matplotlib examples and gallery pages are a great resource for doing visualizations in Python. . NOTE: It&#39;s recommended you start with something simple first. If you wish to do more, come back to it after you&#39;ve completed the rest of the sections. . ### Data exploration visualization code goes here. ### Feel free to use as many code cells as needed. import matplotlib.pyplot as plt # Visualizations will be shown in the notebook. %matplotlib inline import random import csv def plot_figures(figures, nrows = 1, ncols=1, labels=None): fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(12, 14)) axs = axs.ravel() for index, title in zip(range(len(figures)), figures): axs[index].imshow(figures[title], plt.gray()) if(labels != None): axs[index].set_title(labels[index]) else: axs[index].set_title(title) axs[index].set_axis_off() plt.tight_layout() name_values = np.genfromtxt(&#39;signnames.csv&#39;, skip_header=1, dtype=[(&#39;myint&#39;,&#39;i8&#39;), (&#39;mysring&#39;,&#39;S55&#39;)], delimiter=&#39;,&#39;) number_to_stop = 8 figures = {} labels = {} for i in range(number_to_stop): index = random.randint(0, n_train-1) labels[i] = name_values[y_train[index]][1].decode(&#39;ascii&#39;) # print(name_values[y_train[index]][1].decode(&#39;ascii&#39;)) figures[i] = X_train[index] plot_figures(figures, 4, 2, labels) . Personal Note . Data appears good although occasionally for some reason the image cannot be displayed properly. Maybe bad images in the dataset? . unique_train, counts_train = np.unique(y_train, return_counts=True) plt.bar(unique_train, counts_train) plt.grid() plt.title(&quot;Train Dataset Sign Counts&quot;) plt.show() unique_test, counts_test = np.unique(y_test, return_counts=True) plt.bar(unique_test, counts_test) plt.grid() plt.title(&quot;Test Dataset Sign Counts&quot;) plt.show() unique_valid, counts_valid = np.unique(y_valid, return_counts=True) plt.bar(unique_valid, counts_valid) plt.grid() plt.title(&quot;Valid Dataset Sign Counts&quot;) plt.show() . Personal Note . Data appears uniform although each part of the dataset doesn&#39;t have equal sizes. This should be ok though, but something to keep in mind if I run into problems detecting specific signs that should be classified out of the 43. . # yuv = np.array([[1, 0, 1.13983], [1, -0.39465, -0.58060], [1, 2.03211, 0]]) # X_train_yuv = X_train*yuv # from skimage import color # X_train_yuv = color.convert_colorspace(X_train, &#39;RGB&#39;, &#39;YUV&#39;) # X_train_yuv = color.rgb2yuv(X_train) # X_train_yuv = color.rgb2yuv(X_train) # X_train_y = X_train_yuv[0:,:,] # number_to_stop = 8 # figures = {} # for i in range(number_to_stop): # index = random.randint(0, n_train-1) # print(name_values[y_train[index]]) # figures[y_train[index]] = X_train_yuv[index] # plot_figures(figures, 2, 4) # print(X_train_y) # X_train = X_train_yuv . Personal Note . I tried to use YUV, but kind of ran out of time. In the paper recommended by the class, from Pierre Sermanet and Yann LeCun, they said they used it. I am still curious exactly how they did it. . . Step 2: Design and Test a Model Architecture . Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the German Traffic Sign Dataset. . There are various aspects to consider when thinking about this problem: . Neural network architecture | Play around preprocessing techniques (normalization, rgb to grayscale, etc) | Number of examples per label (some have more than others). | Generate fake data. | . Here is an example of a published baseline model on this problem. It&#39;s not required to be familiar with the approach used in the paper but, it&#39;s good practice to try to read papers like these. . NOTE: The LeNet-5 implementation shown in the classroom at the end of the CNN lesson is a solid starting point. You&#39;ll have to change the number of classes and possibly the preprocessing, but aside from that it&#39;s plug and play! . Pre-process the Data Set (normalization, grayscale, etc.) . Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. . ### Preprocess the data here. Preprocessing steps could include normalization, converting to grayscale, etc. ### Feel free to use as many code cells as needed. import tensorflow as tf from tensorflow.contrib.layers import flatten from math import ceil from sklearn.utils import shuffle # Convert to grayscale X_train_rgb = X_train X_train_gray = np.sum(X_train/3, axis=3, keepdims=True) X_test_rgb = X_test X_test_gray = np.sum(X_test/3, axis=3, keepdims=True) X_valid_rgb = X_valid X_valid_gray = np.sum(X_valid/3, axis=3, keepdims=True) print(X_train_rgb.shape) print(X_train_gray.shape) print(X_test_rgb.shape) print(X_test_gray.shape) . (34799, 32, 32, 3) (34799, 32, 32, 1) (12630, 32, 32, 3) (12630, 32, 32, 1) . # X_train = tf.image.rgb_to_grayscale(X_train, name=None) X_train = X_train_gray X_test = X_test_gray X_valid = X_valid_gray . image_depth_channels = X_train.shape[3] # print(image_depth_channels) number_to_stop = 8 figures = {} random_signs = [] for i in range(number_to_stop): index = random.randint(0, n_train-1) labels[i] = name_values[y_train[index]][1].decode(&#39;ascii&#39;) figures[i] = X_train[index].squeeze() random_signs.append(index) # print(random_signs) plot_figures(figures, 4, 2, labels) . import cv2 more_X_train = [] more_y_train = [] more2_X_train = [] more2_y_train = [] new_counts_train = counts_train for i in range(n_train): if(new_counts_train[y_train[i]] &lt; 3000): for j in range(3): dx, dy = np.random.randint(-1.7, 1.8, 2) M = np.float32([[1, 0, dx], [0, 1, dy]]) dst = cv2.warpAffine(X_train[i], M, (X_train[i].shape[0], X_train[i].shape[1])) dst = dst[:,:,None] more_X_train.append(dst) more_y_train.append(y_train[i]) random_higher_bound = random.randint(27, 32) random_lower_bound = random.randint(0, 5) points_one = np.float32([[0,0],[32,0],[0,32],[32,32]]) points_two = np.float32([[0, 0], [random_higher_bound, random_lower_bound], [random_lower_bound, 32],[32, random_higher_bound]]) M = cv2.getPerspectiveTransform(points_one, points_two) dst = cv2.warpPerspective(X_train[i], M, (32,32)) more2_X_train.append(dst) more2_y_train.append(y_train[i]) tilt = random.randint(-12, 12) M = cv2.getRotationMatrix2D((X_train[i].shape[0]/2, X_train[i].shape[1]/2), tilt, 1) dst = cv2.warpAffine(X_train[i], M, (X_train[i].shape[0], X_train[i].shape[1])) more2_X_train.append(dst) more2_y_train.append(y_train[i]) new_counts_train[y_train[i]] += 2 more_X_train = np.array(more_X_train) more_y_train = np.array(more_y_train) X_train = np.concatenate((X_train, more_X_train), axis=0) y_train = np.concatenate((y_train, more_y_train), axis=0) more2_X_train = np.array(more_X_train) more2_y_train = np.array(more_y_train) more2_X_train = np.reshape(more2_X_train, (np.shape(more2_X_train)[0], 32, 32, 1)) X_train = np.concatenate((X_train, more2_X_train), axis=0) y_train = np.concatenate((y_train, more2_y_train), axis=0) X_train = np.concatenate((X_train, X_valid), axis=0) y_train = np.concatenate((y_train, y_valid), axis=0) . figures1 = {} labels = {} figures1[0] = X_train[n_train+1].squeeze() labels[0] = y_train[n_train+1] figures1[1] = X_train[0].squeeze() labels[1] = y_train[0] plot_figures(figures1, 1, 2, labels) . from sklearn.model_selection import train_test_split X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) print(&quot;New Dataset Size : {}&quot;.format(X_train.shape[0])) unique, counts = np.unique(y_train, return_counts=True) plt.bar(unique, counts) plt.grid() plt.title(&quot;Train Dataset Sign Counts&quot;) plt.show() unique, counts = np.unique(y_test, return_counts=True) plt.bar(unique, counts) plt.grid() plt.title(&quot;Test Dataset Sign Counts&quot;) plt.show() unique, counts = np.unique(y_valid, return_counts=True) plt.bar(unique, counts) plt.grid() plt.title(&quot;Valid Dataset Sign Counts&quot;) plt.show() . New Dataset Size : 89860 . def normalize(im): return -np.log(1/((1 + im)/257) - 1) # X_train_normalized = normalize(X_train) # X_test_normalized = normalize(X_test) X_train_normalized = X_train/127.5-1 X_test_normalized = X_test/127.5-1 number_to_stop = 8 figures = {} count = 0 for i in random_signs: labels[count] = name_values[y_train[i]][1].decode(&#39;ascii&#39;) figures[count] = X_train_normalized[i].squeeze() count += 1; plot_figures(figures, 4, 2, labels) . X_train = X_train_normalized X_test = X_test_normalized . Model Architecture . def conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=&#39;VALID&#39;) x = tf.nn.bias_add(x, b) print(x.shape) return tf.nn.relu(x) def LeNet(x): mu = 0 sigma = 0.1 W_one = tf.Variable(tf.truncated_normal(shape=(5, 5, image_depth_channels, 6), mean = mu, stddev = sigma)) b_one = tf.Variable(tf.zeros(6)) layer_one = conv2d(x, W_one, b_one, 1) layer_one = tf.nn.max_pool(layer_one, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;VALID&#39;) print(layer_one.shape) print() W_two = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)) b_two = tf.Variable(tf.zeros(16)) layer_two = conv2d(layer_one, W_two, b_two, 1) layer_two = tf.nn.max_pool(layer_two, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;VALID&#39;) print(layer_two.shape) print() W_two_a = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 412), mean = mu, stddev = sigma)) b_two_a = tf.Variable(tf.zeros(412)) layer_two_a = conv2d(layer_two, W_two_a, b_two_a, 1) # If a well known architecture was chosen: * What architecture was chosen? * Why did you believe it would be relevant to the traffic sign application? * How does the final model&#39;s accuracy on the training, validation and test set provide evidence that the model is working well?layer_two_a = tf.nn.max_pool(layer_two_a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;VALID&#39;) print(layer_two_a.shape) print() flat = flatten(layer_two_a) W_three = tf.Variable(tf.truncated_normal(shape=(412, 122), mean = mu, stddev = sigma)) b_three = tf.Variable(tf.zeros(122)) layer_three = tf.nn.relu(tf.nn.bias_add(tf.matmul(flat, W_three), b_three)) layer_three = tf.nn.dropout(layer_three, keep_prob) W_four = tf.Variable(tf.truncated_normal(shape=(122, 84), mean = mu, stddev = sigma)) b_four = tf.Variable(tf.zeros(84)) layer_four = tf.nn.relu(tf.nn.bias_add(tf.matmul(layer_three, W_four), b_four)) layer_four = tf.nn.dropout(layer_four, keep_prob) W_five = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma)) b_five = tf.Variable(tf.zeros(43)) layer_five = tf.nn.bias_add(tf.matmul(layer_four, W_five), b_five) return layer_five x = tf.placeholder(tf.float32, (None, 32, 32, image_depth_channels)) y = tf.placeholder(tf.int32, (None)) one_hot_y = tf.one_hot(y, 43) keep_prob = tf.placeholder(tf.float32) . Train, Validate and Test the Model . A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation sets imply underfitting. A high accuracy on the test set but low accuracy on the validation set implies overfitting. . ### Train your model here. ### Calculate and report the accuracy on the training and validation set. ### Once a final model architecture is selected, ### the accuracy on the test set should be calculated and reported as well. ### Feel free to use as many code cells as needed. EPOCHS = 27 BATCH_SIZE = 156 rate = 0.00097 logits = LeNet(x) cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y) loss_operation = tf.reduce_mean(cross_entropy) optimizer = tf.train.AdamOptimizer(learning_rate = rate) training_operation = optimizer.minimize(loss_operation) correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1)) accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) saver = tf.train.Saver() def evaluate(X_data, y_data): num_examples = len(X_data) total_accuracy = 0 sess = tf.get_default_session() for offset in range(0, num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0}) total_accuracy += (accuracy * len(batch_x)) return total_accuracy / num_examples . (?, 28, 28, 6) (?, 14, 14, 6) (?, 10, 10, 16) (?, 5, 5, 16) (?, 1, 1, 412) (?, 1, 1, 412) . with tf.Session() as sess: sess.run(tf.global_variables_initializer()) num_examples = len(X_train) print(&quot;Training...&quot;) print() validation_accuracy_figure = [] test_accuracy_figure = [] for i in range(EPOCHS): X_train, y_train = shuffle(X_train, y_train) for offset in range(0, num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5}) validation_accuracy = evaluate(X_valid, y_valid) validation_accuracy_figure.append(validation_accuracy) test_accuracy = evaluate(X_train, y_train) test_accuracy_figure.append(test_accuracy) print(&quot;EPOCH {} ...&quot;.format(i+1)) print(&quot;Test Accuracy = {:.3f}&quot;.format(test_accuracy)) print(&quot;Validation Accuracy = {:.3f}&quot;.format(validation_accuracy)) print() saver.save(sess, &#39;./lenet&#39;) print(&quot;Model saved&quot;) . Training... EPOCH 1 ... Test Accuracy = 0.872 Validation Accuracy = 0.776 EPOCH 2 ... Test Accuracy = 0.958 Validation Accuracy = 0.911 EPOCH 3 ... Test Accuracy = 0.980 Validation Accuracy = 0.947 EPOCH 4 ... Test Accuracy = 0.985 Validation Accuracy = 0.957 EPOCH 5 ... Test Accuracy = 0.991 Validation Accuracy = 0.964 EPOCH 6 ... Test Accuracy = 0.993 Validation Accuracy = 0.972 EPOCH 7 ... Test Accuracy = 0.996 Validation Accuracy = 0.974 EPOCH 8 ... Test Accuracy = 0.995 Validation Accuracy = 0.979 EPOCH 9 ... Test Accuracy = 0.998 Validation Accuracy = 0.981 EPOCH 10 ... Test Accuracy = 0.997 Validation Accuracy = 0.976 EPOCH 11 ... Test Accuracy = 0.998 Validation Accuracy = 0.984 EPOCH 12 ... Test Accuracy = 0.999 Validation Accuracy = 0.984 EPOCH 13 ... Test Accuracy = 0.997 Validation Accuracy = 0.977 EPOCH 14 ... Test Accuracy = 0.999 Validation Accuracy = 0.988 EPOCH 15 ... Test Accuracy = 0.998 Validation Accuracy = 0.988 EPOCH 16 ... Test Accuracy = 0.999 Validation Accuracy = 0.991 EPOCH 17 ... Test Accuracy = 0.999 Validation Accuracy = 0.989 EPOCH 18 ... Test Accuracy = 0.999 Validation Accuracy = 0.990 EPOCH 19 ... Test Accuracy = 0.999 Validation Accuracy = 0.989 EPOCH 20 ... Test Accuracy = 0.999 Validation Accuracy = 0.991 EPOCH 21 ... Test Accuracy = 0.999 Validation Accuracy = 0.988 EPOCH 22 ... Test Accuracy = 0.999 Validation Accuracy = 0.991 EPOCH 23 ... Test Accuracy = 1.000 Validation Accuracy = 0.992 EPOCH 24 ... Test Accuracy = 1.000 Validation Accuracy = 0.990 EPOCH 25 ... Test Accuracy = 1.000 Validation Accuracy = 0.992 EPOCH 26 ... Test Accuracy = 1.000 Validation Accuracy = 0.993 EPOCH 27 ... Test Accuracy = 1.000 Validation Accuracy = 0.993 Model saved . plt.plot(validation_accuracy_figure) plt.title(&quot;Test Accuracy&quot;) plt.show() plt.plot(validation_accuracy_figure) plt.title(&quot;Validation Accuracy&quot;) plt.show() . Display Accuracy on test set . with tf.Session() as sess: saver.restore(sess, tf.train.latest_checkpoint(&#39;.&#39;)) train_accuracy = evaluate(X_train, y_train) print(&quot;Train Accuracy = {:.3f}&quot;.format(train_accuracy)) valid_accuracy = evaluate(X_valid, y_valid) print(&quot;Valid Accuracy = {:.3f}&quot;.format(valid_accuracy)) test_accuracy = evaluate(X_test, y_test) print(&quot;Test Accuracy = {:.3f}&quot;.format(test_accuracy)) . Train Accuracy = 1.000 Valid Accuracy = 0.993 Test Accuracy = 0.942 . . Step 3: Test a Model on New Images . To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type. . You may find signnames.csv useful as it contains mappings from the class id (integer) to the actual sign name. . Load and Output the Images . import glob import cv2 my_images = sorted(glob.glob(&#39;./mysigns/*.png&#39;)) my_labels = np.array([1, 22, 35, 15, 37, 18]) figures = {} labels = {} my_signs = [] index = 0 for my_image in my_images: img = cv2.cvtColor(cv2.imread(my_image), cv2.COLOR_BGR2RGB) my_signs.append(img) figures[index] = img labels[index] = name_values[my_labels[index]][1].decode(&#39;ascii&#39;) index += 1 plot_figures(figures, 3, 2, labels) . my_signs = np.array(my_signs) my_signs_gray = np.sum(my_signs/3, axis=3, keepdims=True) my_signs_normalized = my_signs_gray/127.5-1 number_to_stop = 6 figures = {} labels = {} for i in range(number_to_stop): labels[i] = name_values[my_labels[i]][1].decode(&#39;ascii&#39;) figures[i] = my_signs_gray[i].squeeze() plot_figures(figures, 3, 2, labels) . Predict the Sign Type for Each Image . ### Run the predictions here and use the model to output the prediction for each image. ### Make sure to pre-process the images with the same pre-processing pipeline used earlier. ### Feel free to use as many code cells as needed. with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # saver = tf.train.import_meta_graph(&#39;./lenet.meta&#39;) saver.restore(sess, &quot;./lenet&quot;) my_accuracy = evaluate(my_signs_normalized, my_labels) print(&quot;My Data Set Accuracy = {:.3f}&quot;.format(my_accuracy)) . My Data Set Accuracy = 1.000 . Analyze Performance . ### Calculate the accuracy for these 5 new images. ### For example, if the model predicted 1 out of 5 signs correctly, it&#39;s 20% accurate on these new images. my_single_item_array = [] my_single_item_label_array = [] for i in range(6): my_single_item_array.append(my_signs_normalized[i]) my_single_item_label_array.append(my_labels[i]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # saver = tf.train.import_meta_graph(&#39;./lenet.meta&#39;) saver.restore(sess, &quot;./lenet&quot;) my_accuracy = evaluate(my_single_item_array, my_single_item_label_array) print(&#39;Image {}&#39;.format(i+1)) print(&quot;Image Accuracy = {:.3f}&quot;.format(my_accuracy)) print() . Image 1 Image Accuracy = 1.000 Image 2 Image Accuracy = 1.000 Image 3 Image Accuracy = 1.000 Image 4 Image Accuracy = 1.000 Image 5 Image Accuracy = 1.000 Image 6 Image Accuracy = 1.000 . Output Top 5 Softmax Probabilities For Each Image Found on the Web . For each of the new images, print out the model&#39;s softmax probabilities to show the certainty of the model&#39;s predictions (limit the output to the top 5 probabilities for each image). tf.nn.top_k could prove helpful here. . The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image. . tf.nn.top_k will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it&#39;ll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids. . Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. tk.nn.top_k is used to choose the three classes with the highest probability: . # (5, 6) array a = np.array([[ 0.24879643, 0.07032244, 0.12641572, 0.34763842, 0.07893497, 0.12789202], [ 0.28086119, 0.27569815, 0.08594638, 0.0178669 , 0.18063401, 0.15899337], [ 0.26076848, 0.23664738, 0.08020603, 0.07001922, 0.1134371 , 0.23892179], [ 0.11943333, 0.29198961, 0.02605103, 0.26234032, 0.1351348 , 0.16505091], [ 0.09561176, 0.34396535, 0.0643941 , 0.16240774, 0.24206137, 0.09155967]]) . Running it through sess.run(tf.nn.top_k(tf.constant(a), k=3)) produces: . TopKV2(values=array([[ 0.34763842, 0.24879643, 0.12789202], [ 0.28086119, 0.27569815, 0.18063401], [ 0.26076848, 0.23892179, 0.23664738], [ 0.29198961, 0.26234032, 0.16505091], [ 0.34396535, 0.24206137, 0.16240774]]), indices=array([[3, 0, 5], [0, 1, 4], [0, 5, 1], [1, 3, 5], [1, 4, 3]], dtype=int32)) . Looking just at the first row we get [ 0.34763842, 0.24879643, 0.12789202], you can confirm these are the 3 largest probabilities in a. You&#39;ll also notice [3, 0, 5] are the corresponding indices. . ### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. ### Feel free to use as many code cells as needed. k_size = 5 softmax_logits = tf.nn.softmax(logits) top_k = tf.nn.top_k(softmax_logits, k=k_size) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # my_saver = tf.train.import_meta_graph(&#39;./lenet.meta&#39;) saver.restore(sess, &quot;./lenet&quot;) my_softmax_logits = sess.run(softmax_logits, feed_dict={x: my_signs_normalized, keep_prob: 1.0}) my_top_k = sess.run(top_k, feed_dict={x: my_signs_normalized, keep_prob: 1.0}) # print(my_top_k) for i in range(6): figures = {} labels = {} figures[0] = my_signs[i] labels[0] = &quot;Original&quot; for j in range(k_size): # print(&#39;Guess {} : ({:.0f}%)&#39;.format(j+1, 100*my_top_k[0][i][j])) labels[j+1] = &#39;Guess {} : ({:.0f}%)&#39;.format(j+1, 100*my_top_k[0][i][j]) figures[j+1] = X_valid[np.argwhere(y_valid == my_top_k[1][i][j])[0]].squeeze() # print() plot_figures(figures, 1, 6, labels) .",
            "url": "https://manisaiprasad.github.io/notes/2019/03/20/Traffic_Sign_Classifier.html",
            "relUrl": "/2019/03/20/Traffic_Sign_Classifier.html",
            "date": " • Mar 20, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Contact",
          "content": "You can find me in multiple social media platforms. . GitHub | LinkedIn | Instagram | Email | Twitter | . A few of my links at Site | .",
          "url": "https://manisaiprasad.github.io/notes/contact/",
          "relUrl": "/contact/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://manisaiprasad.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}